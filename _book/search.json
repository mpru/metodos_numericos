[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Métodos Numéricos con Python",
    "section": "",
    "text": "Prefacio\nEl presente documento es la guía de estudio para la asignatura Métodos Numéricos de la Licenciatura en Estadística (Universidad Nacional de Rosario). Se ha utilizado como fuente para la creación de este material a la bibliografía mencionada en el programa de la asignatura. La asignatura se complementa con variados materiales (prácticas, ejemplos, proyectos) disponibles en el aula virtual del curso de acceso privado.\nEstos apuntes no están libres de contener errores. Sugerencias para corregirlos o para expresar de manera más adecuada las ideas volcadas son siempre bienvenidas1.\nPrimera publicación: enero 2024.\n\n\n\n\n\n\nEn general, no se cuenta con derechos para las imágenes empleadas (a menos que sean de creación propia). Ante cualquier problema, contactar al autor.↩︎",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "05_autovalores.html#introducción",
    "href": "05_autovalores.html#introducción",
    "title": "1  Valores y vectores propios",
    "section": "1.1 Introducción",
    "text": "1.1 Introducción\n\n\nUna matriz \\(m \\times n\\) se puede considerar como una función que utiliza multiplicación de matrices para transformar vectores columna \\(n\\)-dimensionales en vectores columna \\(m\\)-dimensional.\nPor eso, toda matriz \\(\\mathbf A\\) de dimensión \\(m \\times n\\) puede ser pensada como una transformación lineal de \\(\\mathbb R^n\\) a \\(\\mathbb R^m\\):\n\n\\[\nT: \\mathbb R^n \\rightarrow \\mathbb R^m \\quad | \\quad T(\\mathbf x) = \\mathbf{Ax}\n\\]\n\nNos va a interesar de manera particular los casos donde esta función está definida por una matriz \\(\\mathbf A\\) cuadrada (de dimensión \\(n \\times n\\)), con lo cual la transformación \\(\\mathbf{Ax}\\) toma un vector en \\(\\mathbb R^n\\) y devuelve otro en \\(\\mathbb R^n\\).\nAhora bien, en general no es muy intuitivo saber qué tipo de cambios va a sufrir un vector \\(\\mathbf x\\) si lo premultiplicamos por \\(\\mathbf A\\).\nPero hay ciertos vectores que se modifican de una manera muy sencilla: lo único que hace la matriz \\(\\mathbf A\\) es “estirarlos” o “comprimirlos”. Es decir, puede cambiar su módulo o sentido, pero no su dirección.\nExpresado matemáticamente, para algunos vectores, la transformación \\(\\mathbf{Ax}\\) da por resultado el mismo vector \\(\\mathbf{x}\\), multiplicado por una constante no nula \\(\\lambda\\): \\(\\mathbf{Ax} = \\lambda \\mathbf{x}\\).\nEstos vectores que “cambian poco” cuando se los transforma mediante la matriz \\(\\mathbf A\\) reciben el nombre de autovectores, vectores propios o eigenvectores de matriz \\(\\mathbf A\\).\n\n\nDefinición: Un autovector de una matriz \\(\\mathbf{A}\\) es cualquier vector \\(\\mathbf{x}\\) para el que sólo cambia su escala cuando se lo multiplica con \\(\\mathbf{A}\\), es decir: \\(\\mathbf{Ax} = \\lambda \\mathbf{x}\\), para algún número \\(\\lambda\\) real o complejo, que recibe el nombre de autovalor. En otras palabras:\n\\[\\mathbf{x} \\text{ es un autovector y } \\lambda \\text{ es un autovalor de }\\mathbf{A} \\iff \\mathbf{Ax} = \\lambda \\mathbf{x}, \\quad \\mathbf{x} \\neq \\mathbf{0}, \\quad \\lambda \\in \\mathbb{C}\\]\n\n\nLos autovalores y autovectores son muy importantes en muchas disciplinas, ya que los objetos que se estudian suelen ser representados con vectores y las operaciones que se hacen sobre ellos, con matrices.\nEntonces si una matriz \\(\\mathbf{A}\\) describe algún tipo de sistema u operación, los autovectores son aquellos vectores que, cuando pasan por el sistema, se modifican en una forma muy sencilla.\nPor ejemplo, si la matriz \\(\\mathbf{A}\\) representa transformaciones en \\(\\mathbb R^2\\), en principio \\(\\mathbf{A}\\) podría estirar y rotar a los vectores. Sin embargo, a sus autovectores lo único que puede hacerles es estirarlos, no rotarlos.\nVeamos un caso concreto:\n\n\\[\\mathbf{A} = \\begin{bmatrix} 3 & 2 \\\\ 1 & 4 \\end{bmatrix} \\qquad \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\qquad \\mathbf{v} = \\begin{bmatrix} 1 \\\\ -0.5 \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\]\n\nEn este gráfico podemos ver los vectores antes de transformarlos (premultiplicarlos) mediante \\(\\mathbf{A}\\):\n\n\n\n\n\n\n\nY en este gráfico podemos ver como quedan luego de la transformación:\n\n\n\n\n\n\n\n\\(\\mathbf{u}\\) y \\(\\mathbf{v}\\) no cambiaron su dirección, sólo su norma: son autovectores de \\(\\mathbf{A}\\), asociados a los autovalores 5 y 2.\nEn cambio, la matriz \\(\\mathbf{A}\\) modificó la dirección de \\(\\mathbf{w}\\), entonces \\(\\mathbf{w}\\) no es un autovector de \\(\\mathbf{A}\\).\nHaciendo los cálculos:\n\n\\[\n\\mathbf{Au} = \\begin{bmatrix} 5 \\\\ 5 \\end{bmatrix} = 5 \\mathbf{u} \\qquad\n\\mathbf{Av} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = 2\\mathbf{v} \\qquad\n\\mathbf{Aw} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix}\n\\]\n\nDe forma general, si \\(\\mathbf x\\) es un autovector asociado con el autovalor real \\(\\lambda\\), entonces \\(\\mathbf{Ax} = \\lambda \\mathbf x\\), por lo que la matriz \\(\\mathbf{A}\\) transforma al vector \\(\\mathbf{x}\\) en un múltiplo escalar de sí mismo, con las siguientes opciones:\n\nSi \\(\\lambda &gt; 1\\), entonces \\(\\mathbf{A}\\) tiene el efecto de expandir \\(\\mathbf{x}\\) en un factor de \\(\\lambda\\).\nSi \\(0 &lt; \\lambda &lt; 1\\), entonces \\(\\mathbf{A}\\) comprime \\(\\mathbf{x}\\) en un factor de \\(\\lambda\\).\nSi \\(\\lambda &lt; 0\\), los efectos son similares, pero el sentido de \\(\\mathbf{Ax}\\) se invierte.\n\n\n\n\n\n\n\n\n1.1.1 Propiedades\n\nSe debe observar que si \\(\\mathbf{x}\\) es un autovector asociado con el autovalor \\(\\lambda\\) y \\(\\alpha\\) es cualquier constante diferente de cero, entonces \\(\\alpha \\mathbf x\\) también es un autovector asociado con el mismo autovalor ya que:\n\n\\[\n\\mathbf A(\\alpha \\mathbf x) = \\alpha (\\mathbf{Ax}) = \\alpha (\\lambda \\mathbf x) = \\lambda (\\alpha \\mathbf x)\n\\]\n\nEn el ejemplo anterior vimos que \\(\\mathbf u = (1, 1)^T\\) es un autovector de \\(\\mathbf u\\) asociado al autovalor \\(\\lambda = 5\\). Pero también lo es, por ejemplo, \\(\\mathbf z = 2\\mathbf u = (2, 2)^T\\), ya que \\(\\mathbf A \\mathbf z = (10, 10)^T = 5 (2, 2)^T = 5 \\mathbf z\\).\nSi bien hay infinitos autovectores asociados a un autovalor, para todos los autovalores y usando cualquier norma vectorial \\(||.||\\), siempre existe un autovector de norma 1, el cual puede ser hallado a partir de cualquier autovector \\(\\mathbf x\\) como \\(\\alpha \\mathbf x\\), con \\(\\alpha = ||\\mathbf x||^{-1}\\).\nDada una matriz \\(\\mathbf{A}\\) cuadradada de orden \\(n\\):\n\n\\(\\mathbf{A}\\) tiene \\(n\\) autovalores, \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\), los cuales no necesariamente son todos distintos. Si lo son, los autovectores forman un conjunto linealmente independiente.\n\\(tr(A) = \\sum_{i=1}^n a_{ii} = \\sum_{i=1}^n \\lambda_{i}\\).\n\\(\\det(A) = \\prod_{i=1}^n \\lambda_{i}\\).\nLos autovalores de \\(\\mathbf{A}^k\\) son \\(\\lambda_1^k, \\lambda_2^k, \\cdots, \\lambda_n^k\\).\nSi \\(\\mathbf{A}\\) es real y simétrica todos sus autovalores son reales y los autovectores correspondientes a distintos autovalores son ortogonales.\nSi \\(\\mathbf{A}\\) es triangular los valores propios son los elementos diagonales.\nLos autovalores de una matriz y su transpuesta son los mismos.\nSi \\(\\mathbf{A}\\) tiene inversa, los autovalores de \\(\\mathbf{A}^{-1}\\) son \\(1/\\lambda_1, 1/\\lambda_2, \\cdots, 1/\\lambda_n\\).\nLos autovalores de \\(\\alpha \\mathbf{A}\\) son \\(\\alpha \\lambda_1, \\alpha \\lambda_2, \\cdots, \\alpha \\lambda_n, \\, \\alpha \\in \\mathbb{R}\\).\nDos matrices cuadradas \\(\\mathbf{A}\\) y \\(\\mathbf{B}\\) son semejantes o similares si existe una matriz invertible \\(\\mathbf{Q}\\) tal que \\(\\mathbf{B} = \\mathbf{Q}^{-1}\\mathbf{A}\\mathbf{Q}\\). Las matrices semejantes tienen los mismos autovalores.\n\n\n\n\n1.1.2 Obtención de autovalores y autovectores\n\nComo estudiarán en Álgebra Lineal, para hallar autovalores y autovectores se deben seguir los siguientes dos pasos:\n\nSe determinan los autovalores encontrando las soluciones de la ecuación algebraica de grado \\(n\\): \\(det(\\mathbf A - \\lambda \\mathbf I) = 0\\) (la incógnita es \\(\\lambda\\)).\nPara cada autovalor \\(\\lambda\\), se determina un autovector al resolver el sistema lineal \\(n \\times n\\): \\((\\mathbf A - \\lambda \\mathbf I)\\mathbf x = \\mathbf 0\\).\n\nEstos pasos son el resultado de las siguientes consideraciones:\n\nA partir de la definición tenemos: \\(\\mathbf{Ax} = \\lambda \\mathbf{x} \\implies \\mathbf{Ax} - \\lambda \\mathbf{x} = \\mathbf{0} \\implies (\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{x} = \\mathbf{0}\\).\nEsto es un sistema de ecuaciones lineales con matriz de coeficientes \\(\\mathbf{A} - \\lambda \\mathbf{I}\\), vector de incógnitas \\(\\mathbf x\\) (el autovector) y vector de términos independientes \\(\\mathbf{0}\\). Es decir, es un sistema homogéneo.\nUn sistema homogéneo es siempre compatible, ya que al menos tiene la solución trivial \\(\\mathbf x = (0, \\cdots, 0)^T\\). Esta solución no nos interesa, puesto que buscamos autovectores y los mismos deben ser no nulos.\nComo sabemos, para que el sistema tenga otra solución además de la trivial, se tiene que tratar de un sistema indeterminado, con infinitas soluciones, ya que los sistemas compatibles o bien tienen una sola solución o infinitas. Esto tiene sentido, porque cada autovalor \\(\\lambda\\) tiene asociados infinitos autovectores. Entonces, para hallar autovectores necesitamos que el sistema \\((\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{x} = \\mathbf{0}\\) sea compatible indeterminado.\nPara que un sistema sea indeterminado, su matriz de coeficientes debe tener determinante igual a 0, es decir: \\(det(\\mathbf A - \\lambda \\mathbf I) = 0\\).\nPor eso sabemos que los autovalores de \\(\\mathbf A\\) tienen que ser aquellos valores \\(\\lambda\\) que satisfagan la igualdad anterior, que es una ecuación algebraica en \\(\\lambda\\) de grado \\(n\\). \\(det(\\mathbf A - \\lambda \\mathbf I)\\) recibe el nombre de polinomio característico de \\(\\mathbf A\\).\n\nEjemplo:\n\n\\[\\begin{gather*}\n\\mathbf{A} =\n\\begin{bmatrix}\n    5 & -2 & 0 \\\\\n    -2 & 3 & -1 \\\\\n    0 & -1 & 1    \n\\end{bmatrix}\n\\implies \\\\ \\\\\ndet(\\mathbf{A} - \\lambda \\mathbf{I}) =\n\\begin{vmatrix}\n    5 - \\lambda & -2 & 0 \\\\\n    -2 & 3 - \\lambda & -1 \\\\\n    0 & -1 & 1-\\lambda\n\\end{vmatrix}  =\n\\cdots  = -\\lambda^3 + 9 \\lambda^2 - 18 \\lambda + 6 = 0\n\\end{gather*}\\]\n\nComo pueden verificar ustedes (opcionalmente aplicando los métodos de la Unidad 2), las soluciones de la ecuación característica son \\(\\lambda_1 = 6.2899, \\lambda_2 = 2.2943\\) y \\(\\lambda_3 = 0.4158\\), los cuales son los autovalores de \\(\\mathbf{A}\\).\nPara hallar un autovector asociado a \\(\\lambda_1 = 6.2899\\), resolvemos el sistema de ecuaciones \\((\\mathbf{A} - 6.2899 \\, \\mathbf{I}) \\mathbf{x} = \\mathbf{0}\\):\n\n\\[\\begin{gather*}\n(\\mathbf{A} - 6.2899 \\, \\mathbf{I}) \\mathbf{x} = \\mathbf{0} \\implies\n\\begin{bmatrix}\n    -1.2899 & -2 & 0 \\\\\n    -2 & -3.2899 & -1 \\\\\n    0 & -1 & -5.2899\n\\end{bmatrix}\n\\begin{bmatrix}\n    x_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n\\\\ \\\\\n\\implies\n\\begin{cases}\n-1.2899 x_1 -2 x_2 &= 0 \\\\\n-2 x_1 - 3.2899 x_2 - x_3 &= 0\\\\\n-x_2 - 5.2899 x_3 &= 0\n\\end{cases} \\implies\n\\begin{cases}\n    x_1 = 8.2018 x_3\\\\\n    x_2 = -5.2899 x_2\\\\\n    x_3 \\in \\mathbb{R}\n\\end{cases}\n\\end{gather*}\\]\n\nComo se puede ver la solución de este sistema homogéneo no es única, representando los infinitos autovectores asociados a \\(\\lambda_1 = 6.2899\\). Por ejemplo, si elegimos \\(x_3 = 1\\), obtenemos el autovector:\n\n\\[\n\\mathbf{x}_1 =\n\\begin{bmatrix}\n    8.2018 \\\\ -5.2899 \\\\ 1\n\\end{bmatrix}\n\\]\n\nEn general, se acostumbra a informar el autovector de norma 1 (que sí es único).\nDe la misma forma se procede con los restantes autovalores \\(\\lambda_2\\) y \\(\\lambda_3\\).\nHallar la ecuación característica ya es demasiado trabajoso para \\(n=3\\), y mucho más será para mayor \\(n\\). Ni hablar de resolver el sistema para encontrar los autovectores.\nPor eso en esta unidad veremos métodos que directamente nos dan como resultados los autovectores y autovalores de una matriz.\nPor supuesto, Python trae una función para esto. Podemos usarla para chequear los resultados, pero no porque sea fácil emplearla nos libraremos de estudiar los algoritmos encargados de producir nuestros queridos autovectores y autovalores:\n\n\nimport numpy as np\nfrom scipy.linalg import eig\n\nA = np.array([[ 5, -2, 0],\n              [-2, 3, -1],\n              [ 0, -1, 1]])\n\nautovalores, autovectores = eig(A)\nprint(\"Autovalores:\")\n\nAutovalores:\n\nprint(autovalores)\n\n[6.2899+0.j 2.2943+0.j 0.4158+0.j]\n\nprint(\"\\nAutovectores:\")\n\n\nAutovectores:\n\nprint(autovectores)\n\n[[ 0.836   0.5049  0.2149]\n [-0.5392  0.6831  0.4927]\n [ 0.1019 -0.5277  0.8433]]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Valores y vectores propios</span>"
    ]
  },
  {
    "objectID": "05_autovalores.html#el-método-de-potencia",
    "href": "05_autovalores.html#el-método-de-potencia",
    "title": "1  Valores y vectores propios",
    "section": "1.2 El Método de Potencia",
    "text": "1.2 El Método de Potencia\n\nEl método de potencia (también conocido como de las potencias o de aproximaciones sucesivas) es una técnica iterativa que se usa para determinar el autovalor dominante de una matriz y un autovector asociado.\n\n\nDefinición: sean \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\) los autovalores de una matriz \\(n \\times n\\), \\(\\mathbf{A}\\). \\(\\lambda_1\\) es llamado autovalor dominante de \\(\\mathbf A\\) si:\n\\[\n|\\lambda_1| &gt; |\\lambda_i|, \\quad i=2, \\cdots,n\n\\]\nLos autovectores correspondientes a \\(\\lambda_1\\) se llaman autovectores dominantes de \\(\\mathbf A\\).\n\n\nEn primer lugar, se debe tomar un vector inicial \\(\\mathbf x^{(0)}\\) con norma \\(||\\mathbf x^{(0)}||_{\\infty} =1\\).\nPor ejemplo, para \\(n=3\\) puede ser \\(\\mathbf x^{(0)} = (1, 1, 1)^T\\) o \\(\\mathbf x^{(0)} = (1, 0, 0)^T\\), entre otros.\nLuego, para cada \\(k = 1, 2, \\cdots\\) se da lugar al siguiente proceso iterativo:\n\nCalcular \\(\\mathbf y^{(k)} = \\mathbf A \\mathbf x^{(k-1)}\\).\nDeterminar \\(\\mu^{(k)}\\) como la coordenada de mayor valor absoluto en \\(\\mathbf y^{(k)}\\).\nEs decir, tomar \\(\\mu^{(k)} / \\, |\\mu^{(k)}| = ||\\mathbf y^{(k)}||_{\\infty}\\). Si hay varias coordenadas que cumplen con esta característica, tomar la primera.\nCalcular: \\(\\mathbf x^{(k)} = \\frac{\\mathbf y^{(k)}}{\\mu^{(k)}}\\)\n\nDe esta forma, la sucesión \\(\\{\\mu^{(k)}\\}^{\\infty}_{k=0}\\) converge al autovalor dominante de \\(\\mathbf A\\), mientras que la sucesión \\(\\{\\mathbf x^{(k)}\\}^{\\infty}_{k=0}\\) converge a un autovector asociado de norma \\(L_{\\infty} = 1\\).\nLa deducción y justificación de este método puede leerse opcionalmente en las páginas 432-433 del libro.\nRetomando el ejemplo de la sección anterior, vamos a aplicar este proceso con:\n\n\\[\n\\mathbf{A} =\n\\begin{bmatrix}\n    5 & -2 & 0 \\\\\n    -2 & 3 & -1 \\\\\n    0 & -1 & 1    \n\\end{bmatrix} \\qquad\n\\mathbf{x}^{(0)} =\n\\begin{bmatrix}\n    1 \\\\\n    1 \\\\\n    1    \n\\end{bmatrix}\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(\\mathbf{x}^{(k)}\\)\n\\(\\mathbf{y}^{(k)} = \\mathbf{Ax}^{(k)}\\)\n\\(\\mu^{(k)}\\)\n\\(\\mathbf{x}^{(k+1)}\\) = \\(\\mathbf{y}^{(k)} / \\mu^{(k)}\\)\nError (\\(L_2\\))\n\n\n\n\n0\n[1 1 1]\\(^T\\)\n[3 0 0]\\(^T\\)\n3\n[1 0 0]\\(^T\\)\n1.4142\n\n\n1\n[1 0 0]\\(^T\\)\n[5 -2 0]\\(^T\\)\n5\n[1 -0.4 0]\\(^T\\)\n0.4\n\n\n2\n[1 -0.4 0]\\(^T\\)\n[5.8 -3.2 0.4]\\(^T\\)\n5.8\n[1 -0.5517 0.0690]\\(^T\\)\n0.1667\n\n\n3\n[1 -0.5517 0.0690]\\(^T\\)\n[6.1034 -3.7241 0.6207]\\(^T\\)\n6.1034\n[1 -0.6102 0.1017]\\(^T\\)\n0.0690\n\n\n4\n[1 -0.6102 0.1017]\\(^T\\)\n[6.2203 -3.9322 0.7119]\\(^T\\)\n6.2203\n[1 -0.6322 0.1144]\\(^T\\)\n0.0254\n\n\n…\n…\n…\n…\n…\n…\n\n\n16\n[1 -0.644972 0.1219239]\\(^T\\)\n[6.2899 -4.0568 0.7669]\\(^T\\)\n6.2899\n[1 -0.644972 0.1219241]\\(^T\\)\n3.956E-7\n\n\n\n\n1.2.1 Convergencia\n\nPara que la convergencia esté garantizada, se deben cumplir las siguientes condiciones:\n\nLos autovalores de \\(\\mathbf A\\), \\(\\lambda_1, \\lambda_2, \\cdots, \\lambda_n\\) están asociados a un conjunto de autovectores linealmente independientes1.\n\\(\\mathbf A\\) tiene un autovalor dominante, es decir, se verifica: \\(|\\lambda_1| &gt; |\\lambda_2| \\geq \\cdots \\geq |\\lambda_n| \\geq 0\\).\n\nSi se cumplen estas condiciones, en general el método converge con cualquier vector inicial \\(\\mathbf x^{(0)}\\)2.\nSi no se cumplen estas condiciones, el método puede converger o fallar.\nComo en la práctica no podemos verificar el cumplimiento de las mismas, sencillamente corremos el método y observamos el resultado.\nPara detener el proceso, podemos usar los mismos criterios vistos en la Unidad 3.\nCon el método así presentado, la convergencia será más rápida cuanto mayor sea el valor absoluto del autovalor dominante \\(|\\lambda_1|\\) comparado con el que le sigue, \\(|\\lambda_2|\\).\nTambién es más rápida cuando se aplica en matrices simétricas que en matrices asimétricas.\n\n\n\n1.2.2 Otras características\n\nLa división por la coordenada de mayor valor absoluto, \\(\\mu^{(k)}\\), produce como resultado en cada paso un vector de norma \\(L_{\\infty} = 1\\). Si no se incluyera esta normalización, el proceso iterativo resultaría igual a:\n\\[\n  \\begin{aligned}\n  \\mathbf{x}^{(1)} &= \\mathbf{Ax}^{(0)} \\\\\n  \\mathbf{x}^{(2)} &= \\mathbf{Ax}^{(1)} = \\mathbf{A}^2 \\mathbf{x}^{(0)}\\\\\n  \\mathbf{x}^{(3)} &= \\mathbf{Ax}^{(2)} = \\mathbf{A}^3 \\mathbf{x}^{(0)}\\\\\n  &\\vdots \\\\\n  \\mathbf{x}^{(k)} &= \\mathbf{Ax}^{(k-1)} = \\mathbf{A}^k \\mathbf{x}^{(0)}\\\\\n  &\\vdots \\\\\n  \\end{aligned}\n  \\]\nEsta sucesión también converge a un autovector dominante, pero no normalizado y no nos entrega el autovalor correspondiente, el cual puede ser calculado mediante el cociente de Rayleigh luego de detener el proceso: si \\(\\mathbf{x}\\) es un autovector de \\(\\mathbf{A}\\), entonces su correspondiente autovalor es:\n\\[\n  \\lambda = \\frac{(\\mathbf{Ax})^t\\mathbf{x}}{\\mathbf{x}^t\\mathbf{x}}\n  \\]\nSin embargo, las sucesivas potencias de \\(\\mathbf A\\) tienden a terminar en errores de desbordamiento o subdesbordamiento. Por eso resulta necesaria la introducción de la constante normalizadora, como se indicó inicialmente.\n\n\n\n1.2.3 Variantes para acelerar la convergencia\n\nSe han desarrollado modificaciones del método de potencia que logran una convergencia más rápida y que son importantes en problemas con matrices de gran dimensión.\nEn el caso de matrices generales, se pueden aplicar el método de potencia trasladada o el procedimiento de Aitkens.\nPara matrices simétricas, se puede mejorar significativamente la convergencia con algunas modificaciones en los cálculos, en lo que se conoce como método de potencia simétrica.\nNo nos detendremos en estas variantes.\n\n\n\n1.2.4 Variantes para hallar el autovalor más pequeño\n\nRecordatorio: los autovalores de \\(\\mathbf{A}^{-1}\\) son los recíprocos de los de \\(\\mathbf{A}\\).\nSi aplicamos el método a \\(\\mathbf{A}^{-1}\\), obtenemos su autovalor dominante.\nY, por la observación anterior, si tomamos el recíproco del autovalor así hallado, obtenemos el autovalor de \\(\\mathbf{A}\\) de menor valor absoluto.\n\n\n\n1.2.5 Variantes para hallar otros autovalores\nMétodo de potencia inversa\n\nEs una modificación que se usa para encontrar el autovalor de \\(\\mathbf A\\) que está más cerca de un número específico que hay que establecer de antemano, \\(q\\).\nEsto se utiliza en aplicaciones donde \\(q\\) es una aproximación a algún autovalor que se tiene disponible y que se desea mejorar.\nTampoco profundizaremos en este método, pero se lo puede consultar en las páginas 439-440.\n\nTécnicas de deflación\n\nLas técnicas de deflación permiten obtener los otros autovalores de la matriz, luego de haber obtenido el dominante con el método de potencia.\nConsisten en formar una nueva matriz \\(\\mathbf A_2\\) cuyos autovalores sean iguales a los de la matriz original \\(\\mathbf A\\), excepto por el autovalor dominante de \\(\\mathbf A\\), que es reemplazado por un autovalor igual a cero en \\(\\mathbf A_2\\).\nEntre estos algoritmos encontramos a la deflación de Wielandt y la deflación de Hotelling.\n\nLa deflación de Wielandt se puede utilizar de manera general para cualquier tipo de matriz. Si bien no reviste de demasiada complejidad, involucra numerosos cálculos y no nos detendremos en ello, pero puede ser consultada en la página 443 del libro.\nLa deflación de Hotelling se aplica para matrices simétricas. Una vez hallada una aproximación para el autovalor dominante \\(\\lambda_1\\) con un autovector asociado \\(\\mathbf x_1\\), se debe calcular la siguiente matriz:\n\\[\n  \\mathbf{A}_2 = \\mathbf{A} - \\lambda_1 \\mathbf{u}_1 \\mathbf{u}_1^T\n  \\]\ndonde \\(\\mathbf{u}_1 = \\mathbf{x}_1 / ||\\mathbf{x}_1||_2\\) (es decir, \\(\\mathbf{u}_1\\) es el autovector asociado a \\(\\lambda_1\\) de norma euclidiana igual a 1).\nLos autovalores de \\(\\mathbf A_2\\) son \\(\\{0, \\lambda_2, \\cdots, \\lambda_n\\}\\), de modo que al aplicar nuevamente el método de potencia sobre \\(\\mathbf A_2\\) para hallar su autovalor dominante, encontraremos el segundo autovalor de \\(\\mathbf A\\), \\(\\lambda_2\\).\nRepitiendo este proceso se pueden encontrar los restantes autovalores (por ejemplo, \\(\\mathbf{A}_3 = \\mathbf{A}_{2} - \\lambda_{2} \\mathbf{u}_{2} \\mathbf{u}_{2}^T\\)).\n\n\n\n\nNo obstante, se debe tener en cuenta que las técnicas de deflación en general no se aplican para calcular todos los autovalores de una matriz, sino sólo algunos, ya que presentan un grave inconveniente ligado al deterioro de las aproximaciones de los autovalores restantes.\nDado que el valor obtenido en la primera etapa es una aproximación del verdadero autovalor \\(\\lambda_1\\), los autovalores de \\(\\mathbf A_2\\) no son exactamente los restantes autovalores de \\(\\mathbf A\\) sino una aproximación a los mismos.\nAl aplicar el método otra vez, se obtiene una aproximación al autovalor dominante de \\(\\mathbf A_2\\), que es a su vez aproximado pero no igual al verdadero valor \\(\\lambda_2\\) que buscamos.\nEntonces, tras un cierto número de etapas de deflación, la acumulación de errores de redondeo y de truncamiento pueden deteriorar notablemente la aproximación.\nPor esta razón, si es necesario encontrar todos los autovalores de una matriz, es conveniente emplear otras técnicas, como la del algoritmo QR que veremos en la siguiente sección.\n\n\n\n1.2.6 Importancia del método\n\nSi hay otras técnicas que hallan todos los autovalores, ¿por qué nos preocupamos por el método de potencia que nos da sólo uno?\n\nPorque hallar todos los autovalores en matriz de gran dimensión es computacionalmente costoso.\nPorque es utilizado en muchas aplicaciones donde sólo se necesita obtener el autovalor dominante.\nPorque es eficiente cuando la matriz es dispersa (matriz de gran dimensión con la gran mayoría de sus entradas iguales a cero).\n\nDe hecho, Google utiliza el método de potencia en su algoritmo PageRank para buscar rankear los resultados de búsquedas de páginas web, desarrollado en Stanford University en 1996 por Larry Page y Sergey Brin. El exito de este algoritmo derivó en la creación de esta mega empresa que empezó siendo sólo un motor de búsqueda (pueden buscar en Wikipedia o leer el artículo The $25.000.000.000 eigenvector: the linear algebra behind Google).\nTwitter también lo usa para generar las recomendaciones acerca de a quién seguir.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Valores y vectores propios</span>"
    ]
  },
  {
    "objectID": "05_autovalores.html#el-algoritmo-qr",
    "href": "05_autovalores.html#el-algoritmo-qr",
    "title": "1  Valores y vectores propios",
    "section": "1.3 El algoritmo QR",
    "text": "1.3 El algoritmo QR\n\nEn esta sección consideramos el algoritmo QR, una técnica que se utiliza para determinar en forma sistemática todos los autovalores de una matriz cuadrada.\nPrimero vamos a ver de qué se trata la factorización QR y luego veremos el algoritmo QR para hallar los autovalores.\n\n\n1.3.1 Factorización QR\n\nYa hemos mencionado un tipo especial de factorización de matrices, la LU.\nAhora vamos a ver otra factorización, que también tiene numerosas aplicaciones:\n\nResolver sistemas de ecuaciones lineales.\nCalcular determinantes e inversas.\nEncontrar otras factorizacones (como la de Cholesky y la de Schur).\nOtras.\n\n\n\nTeorema::\n\nToda matriz real cuadrada no singular \\(\\mathbf A\\) de dimensión \\(n \\times n\\) puede factorizarse en la forma \\(\\mathbf A = \\mathbf{QR}\\), donde \\(\\mathbf Q\\) es una matriz ortogonal \\(n \\times n\\) y \\(\\mathbf R\\) es una matriz triangular superior \\(n \\times n\\). La factorización es única si se pide que los elementos diagonales de \\(\\mathbf R\\) sean positivos.\nToda matriz real rectangular \\(\\mathbf A\\) de dimensión \\(m \\times n\\) (\\(m &gt; n\\)), puede factorizarse en la forma \\(\\mathbf A = \\mathbf{QR}\\), donde \\(\\mathbf Q\\) es una matriz ortogonal \\(m \\times m\\) y \\(\\mathbf R\\) es una matriz triangular superior \\(m \\times n\\), en la cual sus últimas \\(m-n\\) filas son todos ceros. Dado que las últimas filas son nulas, las últimas columnas de \\(\\mathbf Q\\) no aportan al producto \\(\\mathbf Q\\mathbf R\\) y por lo tanto otras definiciones y algunos algoritmos presentan a \\(\\mathbf Q\\) como una matriz \\(m \\times n\\) con columnas ortonormales y \\(\\mathbf R\\) como una matriz triangular \\(n \\times n\\).\n\n\n\nRecordamos que una matriz ortogonal es una matriz cuadrada cuya matriz inversa coincide con su matriz traspuesta: \\(\\mathbf Q^T = \\mathbf Q^{-1}\\). Sus columnas son vectores ortogonales de norma 1.\n\n\n\nPara obtener \\(\\mathbf Q\\) se puede aplicar el proceso de Gram-Schmidt a las columnas de \\(\\mathbf A\\) (las columnas de \\(\\mathbf Q\\) son las de \\(\\mathbf A\\) luego de la ortonormalización).\nUna vez obtenida \\(\\mathbf Q\\), \\(\\mathbf R\\) se puede obtener como \\(\\mathbf R = \\mathbf Q^T \\mathbf A\\).\nHay otros métodos que también permiten hacer esto, pero en este curso no nos vamos a preocupar por el cálculo de la factorización y directamente emplearemos la función de R con la que se obtiene.\nEjemplo con una matriz cuadrada. Sea:\n\\[\n  \\mathbf A =\n  \\begin{bmatrix}\n  1 & -2 & 1 \\\\\n  -1 & 3 & 2 \\\\\n  1 & -1 & -4\n  \\end{bmatrix}\n  \\]\nSu factorización QR es:\n\\[\n  \\mathbf Q =\n  \\begin{bmatrix}\n  -\\frac{1}{\\sqrt 3} & 0                  & -\\frac{2}{\\sqrt 6} \\\\\n  \\frac{1}{\\sqrt 3} & -\\frac{1}{\\sqrt 2}  & -\\frac{1}{\\sqrt 6} \\\\\n  -\\frac{1}{\\sqrt 3} & -\\frac{1}{\\sqrt 2} & \\frac{1}{\\sqrt 6}\n  \\end{bmatrix}\n  \\qquad\n  \\mathbf R =\n      \\begin{bmatrix}\n  -\\sqrt 3 & 2\\sqrt 3  & \\frac{5\\sqrt 3}{3} \\\\\n  0        & -\\sqrt 2  &  \\sqrt 2 \\\\\n  0        & 0         & -\\frac{\\sqrt{96}}{3}\n  \\end{bmatrix}\n  \\]\nde modo que se verifica: \\(\\mathbf A = \\mathbf{QR}\\).\nLo comprobamos en Python.\nEjemplo con una matriz cuadrada.\n\n\nfrom scipy.linalg import qr\n\nA = np.array([[1, -2, 1],\n              [-1, 3, 2],\n              [1, -1, -4]])\n\nQ, R = qr(A)\n\nprint(\"Matriz Q (ortogonal):\")\n\nMatriz Q (ortogonal):\n\nprint(Q)\n\n[[-0.5774  0.     -0.8165]\n [ 0.5774 -0.7071 -0.4082]\n [-0.5774 -0.7071  0.4082]]\n\nprint(\"\\nMatriz R (triangular superior):\")\n\n\nMatriz R (triangular superior):\n\nprint(R)\n\n[[-1.7321  3.4641  2.8868]\n [ 0.     -1.4142  1.4142]\n [ 0.      0.     -3.266 ]]\n\n# Verificamos que Q es ortogonal\nQ_traspuesta = np.transpose(Q)\nQ_inversa = np.linalg.inv(Q)\nprint(\"\\n¿Q es ortogonal? (t(Q) == inv(Q)):\", np.allclose(Q_traspuesta, Q_inversa))\n\n\n¿Q es ortogonal? (t(Q) == inv(Q)): True\n\n# Verificamos A = QR\nresultado = np.dot(Q, R)\nprint(resultado)\n\n[[ 1. -2.  1.]\n [-1.  3.  2.]\n [ 1. -1. -4.]]\n\nprint(\"\\n¿A = QR?\", np.allclose(A, resultado))\n\n\n¿A = QR? True\n\n\n\nEjemplo con una matriz rectangular:\n\n\nA = np.array([[1, -2],\n              [-1, 3],\n              [1, -1]])\n              \n# Forma 1\nQ, R = qr(A)\nprint(\"Matriz Q (ortogonal):\")\n\nMatriz Q (ortogonal):\n\nprint(Q)\n\n[[-0.5774  0.     -0.8165]\n [ 0.5774 -0.7071 -0.4082]\n [-0.5774 -0.7071  0.4082]]\n\nprint(\"\\nMatriz R (triangular superior):\")\n\n\nMatriz R (triangular superior):\n\nprint(R)\n\n[[-1.7321  3.4641]\n [ 0.     -1.4142]\n [ 0.      0.    ]]\n\n# Verificamos que Q es ortogonal\nQ_traspuesta = np.transpose(Q)\nQ_inversa = np.linalg.inv(Q)\nprint(\"\\n¿Q es ortogonal? (t(Q) == inv(Q)):\", np.allclose(Q_traspuesta, Q_inversa))\n\n\n¿Q es ortogonal? (t(Q) == inv(Q)): True\n\n# Verificamos A = QR\nresultado = np.dot(Q, R)\nprint(resultado)\n\n[[ 1. -2.]\n [-1.  3.]\n [ 1. -1.]]\n\nprint(\"\\n¿A = QR?\", np.allclose(A, resultado))\n\n\n¿A = QR? True\n\n# Forma 2 (omite filas nulas de R)\nQ, R = qr(A, mode=\"economic\")\nprint(\"Matriz Q (ortogonal):\")\n\nMatriz Q (ortogonal):\n\nprint(Q)\n\n[[-0.5774  0.    ]\n [ 0.5774 -0.7071]\n [-0.5774 -0.7071]]\n\nprint(\"\\nMatriz R (triangular superior):\")\n\n\nMatriz R (triangular superior):\n\nprint(R)\n\n[[-1.7321  3.4641]\n [ 0.     -1.4142]]\n\n\n\n\n1.3.2 El algoritmo QR\n\nAhora estamos en condiciones de usar la factorización QR para obtener todos los autovalores de una matriz cuadrada \\(n \\times n\\), \\(\\mathbf A\\).\nEs un algoritmo tan sencillo, que sorprende que sea tan efectivo.\nPrimero se toma \\(\\mathbf A = \\mathbf A^{(0)}\\) como matriz inicial.\nLuego, para cada \\(k = 1, 2, \\cdots\\):\n\nRealizar la factorización QR de \\(\\mathbf A^{(k-1)}\\) para obtener \\(\\mathbf Q^{(k-1)}\\) y \\(\\mathbf R^{(k-1)}\\) (es decir: \\(\\mathbf A^{(k-1)}=\\mathbf Q^{(k-1)}\\mathbf R^{(k-1)}\\)).\nCalcular la siguiente matriz del proceso iterativo como: \\(\\mathbf A^{(k)} = \\mathbf R^{(k-1)}\\mathbf Q^{(k-1)}\\)\n\nLa sucesión \\(\\mathbf A^{(k)}\\) converge a una matriz triangular cuyos elementos diagonales son los autovalores de \\(\\mathbf A\\).\nLa idea detrás de este método es la siguiente: las sucesivas matrices \\(\\mathbf A^{(k)}\\) son semejantes (revisar sección de propiedades) y, por lo tanto, tienen los mismos autovalores. Además, estas operaciones van transformando de a poco a las matrices \\(\\mathbf A^{(k)}\\) en triangulares superiores y sabemos que en tales matrices los autovalores son los elementos diagonales (repasar las propiedades enunciadas al inicio del apunte).\nPara darnos cuenta de que las matrices \\(\\mathbf A^{(k)}\\) son semejantes debemos notar:\n\n\\[\n\\mathbf A^{(k)} = \\mathbf R^{(k-1)}\\mathbf Q^{(k-1)} =\n\\underbrace{{\\mathbf Q^{(k-1)}}^{-1} \\mathbf Q^{(k-1)}}_{\\mathbf I}\\mathbf R^{(k-1)}\\mathbf Q^{(k-1)} =\n{\\mathbf Q^{(k-1)}}^{-1}  \\mathbf A^{(k-1)}\\mathbf Q^{(k-1)}\n´\\implies A^{(k)} \\text{ y } A^{(k-1)} \\text{ son semejantes}\n\\]\n\n¿Y los autovectores?\n\nSi la matriz es simétrica, los autovectores son las columnas de \\(\\prod_{k=0} \\mathbf Q^{(k)}\\).\nSi la matriz no es simétrica, esta forma presentada del algoritmo, que es la más sencilla posible y por eso a veces es llamado “el algoritmo QR puro” no entrega los autovectores, pero hay otras variantes que sí lo hacen.\n\n\n\n\n\nEl proceso iterativo debe detenerse cuando se haya llegado a una matriz triangular superior (las entradas del triángulo inferior sin la diagonal deberían ser cero o muy cercanas). Para implementar un criterio más sencillo, podemos detenernos cuando la distancia entre los vectores formados por los elementos diagonales de la matriz sea tan pequeña como se desee.\n\n\n\nEjemplo:\n\\[\n  \\mathbf A =\n  \\begin{bmatrix}\n  1 & -2 & 1 \\\\\n  -1 & 3 & 2 \\\\\n  1 & -1 & -4\n  \\end{bmatrix}\n  \\]\nLlamamos a esta matriz con \\(\\mathbf A^{(0)}\\) y ya vimos que su factorización QR es:\n\\[\n  \\mathbf Q^{(0)} =\n  \\begin{bmatrix}\n  -\\frac{1}{\\sqrt 3} & 0                  & -\\frac{2}{\\sqrt 6} \\\\\n  \\frac{1}{\\sqrt 3} & -\\frac{1}{\\sqrt 2}  & -\\frac{1}{\\sqrt 6} \\\\\n  -\\frac{1}{\\sqrt 3} & -\\frac{1}{\\sqrt 2} & \\frac{1}{\\sqrt 6}\n  \\end{bmatrix}\n  \\qquad\n  \\mathbf R^{(0)} =\n      \\begin{bmatrix}\n  -\\sqrt 3 & 2\\sqrt 3  & \\frac{5\\sqrt 3}{3} \\\\\n  0        & -\\sqrt 2  &  \\sqrt 2 \\\\\n  0        & 0         & -\\frac{\\sqrt{96}}{3}\n  \\end{bmatrix}\n  \\]\nCon lo cual, la siguiente matriz de la sucesión es:\n\\[\n  \\mathbf A^{(1)} = \\mathbf R^{(0)}\\mathbf Q^{(0)}=\n  \\begin{bmatrix}\n  -\\frac{4}{3} & -\\frac{11}{6} \\sqrt 6                  & -\\frac{5}{6}\\sqrt2 \\\\\n  -\\frac{2}{3}\\sqrt 6 & 0  & \\frac{2}{3} \\sqrt 3 \\\\\n  \\frac{4}{3} \\sqrt 2 & \\frac{4}{3} \\sqrt 3 & -\\frac{4}{3}\n  \\end{bmatrix}\n  \\]\nVerificamos en Python este y los siguientes pasos:\n\n\nA = np.array([[1, -2, 1],\n              [-1, 3, 2],\n              [1, -1, -4]])\n\n# Iteración 1\nQ0, R0 = qr(A)\nA1 = np.dot(R0, Q0)\nprint(A1)\n\n[[ 1.3333 -4.4907  1.1785]\n [-1.633  -0.      1.1547]\n [ 1.8856  2.3094 -1.3333]]\n\n# Iteración 2\nQ1, R1 = qr(A1)\nA2 = np.dot(R1, Q1)\nprint(A2)\n\n[[ 1.      2.8772  0.2349]\n [ 4.0856 -1.2914  3.1605]\n [-0.3759  0.3028  0.2914]]\n\n# Iteración 3\nQ2, R2 = qr(A2)\nA3 = np.dot(R2, Q2)\nprint(A3)\n\n[[ 0.1495 -4.4805 -2.7609]\n [-3.0529 -0.7539 -0.1445]\n [ 0.0542  0.0489  0.6044]]\n\n\n\nSi seguimos iterando vamos a ver que la matriz converge y en su diagonal tendremos a los autovalores.\nUsando la función provista que implementa este algoritmo vemos el resultado:\n\n\nrtdo = algoritmo_qr(A)\n[print(keys, \"\\n\", value) for keys, value in rtdo.items()]\n\nconvergencia \n True\niteraciones \n 115\nautovalores \n [-4.      3.4142  0.5858]\npasos \n        i                                      autovalores_i       error_i\n0      0                                         [1, 3, -4]           NaN\n1      1  [1.3333333333333333, -1.300816253288535e-15, -...  4.027682e+00\n2      2  [0.999999999999999, -1.2913907284768211, 0.291...  2.102030e+00\n3      3  [0.14953271028037157, -0.7539198862276028, 0.6...  1.053630e+00\n4      4  [-0.3840000000000031, -0.19584605115074222, 0....  7.724674e-01\n..   ...                                                ...           ...\n111  111  [-3.999999941718327, 3.4142135040914274, 0.585...  1.789869e-07\n112  112  [-4.000000049746487, 3.4142136121195867, 0.585...  1.527749e-07\n113  113  [-3.9999999575386904, 3.4142135199117907, 0.58...  1.304015e-07\n114  114  [-4.000000036242971, 3.4142135986160715, 0.585...  1.113047e-07\n115  115  [-3.9999999690646675, 3.4142135314377686, 0.58...  9.500447e-08\n\n[116 rows x 3 columns]\n[None, None, None, None]\n\n\n\nLo comparamos con el resultado de la función eig() de Python:\n\n\nautovalores, autovectores = eig(A)\nprint(\"Autovalores:\")\n\nAutovalores:\n\nprint(autovalores)\n\n[-4.    +0.j  0.5858+0.j  3.4142+0.j]\n\nprint(\"\\nAutovectores:\")\n\n\nAutovectores:\n\nprint(autovectores)\n\n[[ 0.3015  0.9511 -0.6715]\n [ 0.3015  0.2711  0.7169]\n [-0.9045  0.1483 -0.1873]]\n\n\n\nAl algoritmo QR “puro” definido en esta sección también se lo conoce como “impráctico” porque tiene algunas desventajas:\n\nLa factorización QR en cada paso es costosa computacionalmente.\nLa convergencia de las entradas subdiagonales a cero es lineal (convergencia lenta).\n\nPor eso se han propuesto algunas modificaciones que mejoran notablemente el desempeño del método:\n\nEn primer lugar, se debe transformar a la matriz original \\(\\mathbf A\\) en otra similar (mismos autovalores) pero que sea tridiagonal (se logra con el método de Householder) o que sea una matriz de Hessenberg.\nLuego, en el proceso iteratvio, se debe usar un procedimiento de deflación cada vez que un elemento subdiagonal se hace 0 para disminuir la cantidad de cálculos.\nY también se debe implementar una estrategia de cambios de filas y columnas (shifted QR) que acelera la convergencia.\n\nEn este curso, no veremos estas variantes (están en el libro, que de hecho no presenta la forma simple que vimos acá).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Valores y vectores propios</span>"
    ]
  },
  {
    "objectID": "05_autovalores.html#descomposición-en-valores-singulares-dvs",
    "href": "05_autovalores.html#descomposición-en-valores-singulares-dvs",
    "title": "1  Valores y vectores propios",
    "section": "1.4 Descomposición en valores singulares (DVS)",
    "text": "1.4 Descomposición en valores singulares (DVS)\n\nfrom matplotlib.image import imread\nimport matplotlib.pyplot as plt\n\n\nUna matriz rectangular \\(\\mathbf A\\) no puede tener un autovalor porque \\(\\mathbf {Ax}\\) y \\(\\mathbf x\\) son vectores de diferentes tamaños.\nSin embargo, existen números que desempeñan un rol análogo al de los autovalores para las matrices no cuadradas.\nSe trata de los valores singulares de una matriz.\nLa Descomposición en Valores Singulares (DVS, también llamada SVD, por las siglas de Singular Value Decomposition) es una factorización para matrices rectangulares que tiene numerosas aplicaciones, por ejemplo en compresión de imágenes y análisis de señales.\nEn Estadística tiene gran importancia para tareas relacionadas con la reducción de dimensionalidad de grandes conjuntos de datos (tiene una vinculación directa con el Análisis de Componentes Principales, técnica que estudiarán en Análisis de Datos Multivariados). También se la puede utilizar para realizar ajustes por Mínimos Cuadrados.\n\n\nTeorema de Descomposición en Valores Singulares: una matriz rectangular \\(\\mathbf A\\) de dimensión \\(m \\times n\\) puede ser factorizada como:\n\\[\n\\mathbf A = \\mathbf U \\mathbf S \\mathbf V^T\n\\]\ndonde:\n\n\\(\\mathbf U\\) es una matriz ortogonal \\(m \\times m\\)\n\\(\\mathbf S\\) es una matriz diagonal \\(m \\times n\\) con elementos \\(\\sigma_i\\) (\\(\\mathbf s_{ij} = 0 \\,\\forall i \\neq j\\)).\n\\(\\mathbf V\\) es una matriz ortogonal \\(n \\times n\\)\n\nAdemás:\n\nLos elementos diagonales de \\(\\mathbf S\\), \\(\\sigma_i\\), son llamados valores singulares de \\(\\mathbf A\\). Son tales que \\(\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\sigma_k \\geq 0\\), con \\(k=min\\{m,n\\}\\) y son iguales a las raíces cuadradas positivas de los autovalores no nulos de \\(\\mathbf A^T\\mathbf A\\).\nLas columnas de \\(\\mathbf V\\) son los autovectores ortonormales de \\(\\mathbf A^T\\mathbf A\\) y se llaman vectores singulares derechos porque \\(\\mathbf {AV} = \\mathbf U \\mathbf S\\).\nLas columnas de \\(\\mathbf U\\) son los autovectores ortonormales de \\(\\mathbf A\\mathbf A^T\\) y se llaman vectores singulares izquierdos porque \\(\\mathbf {U}^T\\mathbf {A} = \\mathbf S \\mathbf V^T\\).\n\n\n\n\n\n\n\n\nEsas tres últimas observaciones proporcionan una forma de obtener la DVS.\n\n\n1.4.1 Ejemplo\n\nVamos a buscar la DVS de la siguiente matriz haciendo los cálculos en Python:\n\n\n\n\n\n\n\n\n\n\n\n\n\\[\n\\mathbf A =\n\\begin{bmatrix}\n4 & 2 & 0 \\\\\n1 & 5 & 6\n\\end{bmatrix}\n\\]\n\nA = np.array([[4,2,0],\n              [1,5,6]])\n\n\nPara generar la matriz diagonal \\(\\mathbf S\\), buscamos los valores singulares que son las raíces positivas de los autovalores no nulos de \\(\\mathbf A^T\\mathbf A\\).\n\n\nATA = A.T @ A\nprint(ATA)\n\n[[17 13  6]\n [13 29 30]\n [ 6 30 36]]\n\nautovalores, autovectores = np.linalg.eig(ATA)\n\nval_sing = np.sqrt(autovalores)\nprint(val_sing)\n\n[8.1387 3.97   0.    ]\n\n# Ponemos a los valores singulares en la matriz S\nS = np.zeros((A.shape[0], A.shape[1]))\nfor i in range(min(A.shape)):\n    S[i, i] = val_sing[i]\nprint(S)\n\n[[8.1387 0.     0.    ]\n [0.     3.97   0.    ]]\n\n\n\nLas columnas de \\(\\mathbf V\\) son los autovectores ortonormales de \\(\\mathbf A^T\\mathbf A\\):\n\n\nV = autovectores\nprint(V.T)\n\n[[-0.26   -0.6592 -0.7056]\n [-0.8913 -0.1172  0.438 ]\n [ 0.3714 -0.7428  0.5571]]\n\n\n\nLas columnas de \\(\\mathbf U\\) son los autovectores ortonormales de \\(\\mathbf A\\mathbf A^T\\):\n\n\nAAT = A @ A.T\nautovalores, autovectores = np.linalg.eig(AAT)\nprint(autovalores)\n\n[15.7611 66.2389]\n\n# Los necesitamos ordenados de mayor a menor\nindices = np.argsort(autovalores)[::-1]\nautovalores = autovalores[indices]\nprint(autovalores)\n\n[66.2389 15.7611]\n\nU = autovectores[:, indices] # reordenamos de la misma forma los autovectores\nprint(U)\n\n[[-0.2898 -0.9571]\n [-0.9571  0.2898]]\n\n\n\nCon los resultados obtenidos, podemos verificar que \\(\\mathbf A = \\mathbf U \\mathbf S \\mathbf V^T\\):\n\n\nprint(A)\n\n[[4 2 0]\n [1 5 6]]\n\nprint(U @ S @ V.T)\n\n[[ 4.  2. -0.]\n [ 1.  5.  6.]]\n\n\n\nEsta no es la forma más eficiente ni robusta de obtener la descomposición.\nEs sensible al ordenamiento de los autovalores y a los signos de los autovectores de \\(\\mathbf A^T\\mathbf A\\): y de \\(\\mathbf A\\mathbf A^T\\), que se obtienen de forma independiente entre sí.\nPor eso, en debemos utilizar programas creados específicametne para este fin.\nPython tiene una función que se encarga de aplicar esto: np.linalg.svd().\nDebemos notar que devuelve directamente la transpuesta de \\(V\\).\n\n\n\nU, val_sing, VT = np.linalg.svd(A, full_matrices=True)\n\nprint(\"Matriz U:\")\n\nMatriz U:\n\nprint(U)\n\n[[ 0.2898  0.9571]\n [ 0.9571 -0.2898]]\n\nprint(\"\\nValores singulares:\")\n\n\nValores singulares:\n\nprint(val_sing)\n\n[8.1387 3.97  ]\n\nS = np.zeros((A.shape[0], A.shape[1]))\nfor i in range(min(A.shape)):\n    S[i, i] = val_sing[i]\nprint(\"\\nMatriz S (valores singulares):\")\n\n\nMatriz S (valores singulares):\n\nprint(S)\n\n[[8.1387 0.     0.    ]\n [0.     3.97   0.    ]]\n\nprint(\"\\nMatriz VT (transpuesta de V):\")\n\n\nMatriz VT (transpuesta de V):\n\nprint(VT)\n\n[[ 0.26    0.6592  0.7056]\n [ 0.8913  0.1172 -0.438 ]\n [ 0.3714 -0.7428  0.5571]]\n\n# Comprobamos que se reconstruye la matriz A\nU @ S @ VT\n\narray([[ 4.,  2., -0.],\n       [ 1.,  5.,  6.]])\n\n\n\n\n1.4.2 Aplicaciones\n\nLa razón de la importancia de la DVS en muchas aplicaciones es que nos permite captar las características más importantes de una matriz \\(m \\times n\\) (en muchos casos, con \\(m\\) mucho mayor que \\(n\\)) usando una matriz que, a menudo, es de tamaño significativamente más pequeño.\nEl hecho de que los valores singulares están en la diagonal de \\(\\mathbf S\\) en orden decreciente implica que al hacer el producto \\(\\mathbf U \\mathbf S \\mathbf V^T\\) para reconstruir a \\(\\mathbf A\\), quienes aportan la mayor parte de la información son las primeras columnas de cada una de estas matrices.\nEntonces para reconstruir \\(\\mathbf A\\) de manera exacta necesitamos estas tres matrices completas, pero para construir una muy buena aproximación a \\(\\mathbf A\\) nos alcanza con hacer el mismo producto usando sólo sus primeras \\(k\\) columnas:\n\n\n\n\n\n\n\n¡Esto es un resultado impresionante! Significa que a un gran conjunto de datos lo podemos almacenar con mucho menos espacio mediante esas matrices reducidas, con muy poca pérdida de información.\nNo hay una forma anticipada de saber con cuántos valores singulares (\\(k\\)) alcanza para tener una buena aproximación, eso depende de cada caso3.\nLa matriz \\(\\mathbf A\\) de dimensión \\(m \\times n\\) requiere \\(mn\\) registros para su almacenamiento.\nSin embargo, la matriz \\(\\mathbf A_k\\), que aproxima a \\(\\mathbf A\\) y también es dimensión \\(m \\times n\\), sólo requiere de \\(k(m+n+1)\\) registros para su almacenamiento (\\(mk\\) para \\(\\mathbf U_k\\), \\(k\\) para \\(\\mathbf S_k\\) y \\(nk\\) para \\(\\mathbf V_k\\)).\nHacer las cuentas para ver cuánto se gana de “espacio” si \\(m=100\\), \\(n=10\\) y \\(k=4\\)…\nEsto se conoce como compresión de datos y de aquí que la DVS está tan relacionada con el Análisis de Componentes Principales, una técnica de reducción de la dimensionalidad.\nPara ponernos un poco más rigurosos, vale comentar que la matriz \\(\\mathbf A_k = \\mathbf U_k \\mathbf S_k \\mathbf V_k^T\\) es de rango \\(k\\) y se demuestra que es la mejor aproximación mediante una matriz de rango \\(k &lt; n\\) de la matriz de datos \\(\\mathbf A\\) (posiblemente de rango \\(n\\)), en el sentido que es la que minimiza el error cuadrático de la predicción4.\nPara finalizar vamos a ver un ejemplo de DVS aplicado al procesamiento de imágenes.\n¿Qué tienen que ver las imágenes con nuestros conocimientos de matrices? Toda imagen digital se representa en la computadora como una matriz de píxeles, es decir, como un gran conjunto de puntitos ordenados en forma de matriz con filas y columnas, cada uno de un color en particular, que visualizados juntos dan lugar a la figura. Por lo tanto, una imagen se puede representar por una matriz donde cada celda tiene información acerca del color del píxel correspondiente:\n\n\n\n\n\n\n\nExisten códigos para representar a los distintos colores, por ejemplo, en el sistema hexadecimal, el código para el rojo es FF0000. Entonces, en la matriz que representa a una imagen digital está el valor FF0000 por cada píxel rojo que la misma tenga. En ese caso, la matriz es de tipo caracter. Hay otros tipos de representación de colores que usan arreglos tridimensionales numéricos para indicar cuánto de rojo, de azúl y de verde tiene un píxel, ya que combinando esos tres se pueden formar el resto de los colores.\nCuando se trabaja con imágenes en escala de grises, la cuestión es más sencilla. En cada celda de la matriz hay un número que varía entre 0 y 1. Una celda con un valor de 0 indica un píxel negro, mientras que una celda con un valor de 1 indica un pixel blanco. Es decir, un valor cercano a 0 es un gris bien oscuro, mientras que un valor cercano a 1 es un gris bien clarito. Por ejemplo:\n\n\n\n\n\n\n\nCon el siguiente código leemos la imagen del Monumento Nacional a la Bandera mostrada anteriormente y la convertimos en escala de grises:\n\nCargar una imagen desde el archivo “monu.png” utilizando la función imread y representarla con la matriz A.\nConvertirla a escala de grises. Para esto, se calcular la media a lo largo del último eje de la matriz A utilizando la función np.mean de NumPy con el argumento -1. Esto es equivalente a tomar el promedio a través de los canales de color en una imagen. Cuando se trabaja con imágenes, el último eje suele representar los canales de color (por ejemplo, Rojo, Verde, Azul en una imagen RGB). Al tomar el promedio a lo largo del último eje, se obtiene una imagen en escala de grises en la que cada píxel representa el valor promedio de los canales de color en el píxel correspondiente. Por lo tanto, A queda como la matriz de valores entre 0 y 1 que que representa a la imagen en escala de grises.\n\n\n\n# Lectura de la imagen\nA = imread(\"Plots/U5/monu.png\")\n\n# Convertir a grises\nA = np.mean(A, -1) \n\n# Explorarla un poco\nnp.max(A)\n\n0.97647065\n\nnp.min(A)\n\n0.023529412\n\nA\n\narray([[0.1085, 0.1085, 0.1085, ..., 0.1203, 0.1203, 0.119 ],\n       [0.1124, 0.1124, 0.1085, ..., 0.1203, 0.1203, 0.119 ],\n       [0.1163, 0.1085, 0.1085, ..., 0.1203, 0.1203, 0.119 ],\n       ...,\n       [0.102 , 0.1137, 0.1242, ..., 0.4314, 0.4235, 0.4052],\n       [0.1033, 0.1124, 0.132 , ..., 0.4235, 0.4157, 0.4   ],\n       [0.1072, 0.1111, 0.1373, ..., 0.4549, 0.4353, 0.4196]], dtype=float32)\n\n# Graficarla\nplt.imshow(A, cmap='gray')\nplt.axis('off')\n\n(-0.5, 519.5, 768.5, -0.5)\n\nplt.show()\n\n\n\n\n\nSiendo la imagen de dimensión \\(m=769 \\times n=560\\), se requiere de \\(769 \\times 520 = 399880\\) valores para su registro.\n¿Será posible aplicarle una DVS para poder almacenarla con muchos menos valores, pero elegidos de forma tal que los mismos sirvan para reconstruir una buena aproximación de la imagen? Probemos…\n\n\n# Aplicar DVS\nU, val_sing, VT = np.linalg.svd(A, full_matrices = False)\nS = np.diag(val_sing)\n\n# Reconstruir la imagen de manera exacta (salvo errores de redondeo)\nA2 = U @ S @ VT\nplt.imshow(A2, cmap='gray')\nplt.axis('off')\n\n(-0.5, 519.5, 768.5, -0.5)\n\nplt.show()\n\n\n\n\n\n\nAhora vamos qué sucede si empleamos \\(k=20\\) valores singulares. En lugar de necesitar \\(399880\\) valores para almacenar la imagen, esto nos permitirá emplear sólo \\(k(m+n+1) = 20(520+769+1)=25800\\) (un 6.45% del original).\n\n\n# Reconstruir la imagen usando menos información\nk = 20\nimg = U[:, :k] @ S[:k, :k] @ VT[:k, :]\nplt.imshow(img, cmap='gray')\nplt.axis('off')\n\n(-0.5, 519.5, 768.5, -0.5)\n\nplt.show()\n\n\n\n\n\nVemos que con una cantidad de registros que representa tan sólo un 6.45% de la cantidad original, se logra reconstruir una imagen que conserva todos los rasgos principales.\nA continuación se presenta el resultado empleando distintos valores de \\(k\\). Calcular en cada caso cuántos registros se necesitan.\n\n\nvalores_k = [2, 5, 10, 50, 100, 200]\n\nfor i in range(len(valores_k)):\n\n    # Establecer k\n    k = valores_k[i]\n  \n    # Calcular la aproximación de la imagen\n    img = U[:, :k] @ S[:k, :k] @ VT[:k, :]\n    \n    # Mostrar la aproximación de la imagen en escala de grises\n    plt.subplot(3, 2, i+1)\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    plt.title(f'k = {k}')\n\nplt.show()\n\n\n\n\n\nPara determinar un buen valor de \\(k\\) es útil graficar los valores singulares en orden decreciente. Se puede elegir el valor para el cual se forma una especie de “codo”, a partir del cual los valores singulares son similares entre sí y tienen un valor pequeño con respecto a los primeros. En el caso de la imagen del monumento, parece que \\(k=20\\) estaría bien.\n\n\n# Trazar los valores singulares en función de k\nplt.plot(range(520) , val_sing, marker='o', color='b')\nplt.xlabel(\"k\")\nplt.ylabel(\"Valores singulares\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\nEn otras aplicaciones, aplicar una DVS a una imagen puede ser necesario para poder borrar “ruido”. Por ejemplo, si se trata de una fotografía que tal vez se tomó a gran distancia, como una imagen satelital, es probable que la misma incluya ruido, es decir, datos que no representan verdaderamente la imagen, sino el deterioro de ésta mediante partículas atmosféricas, la calidad de las lentes, procesos de reproducción, etc. Los datos de ruido se incorporan en los datos de la matriz \\(\\mathbf A\\), pero con suerte este ruido es mucho menos significativo que la verdadera imagen. Se espera que los valores singulares más grandes representen a la verdadera imagen y que los más pequeños, los más cercanos a cero, sean las contribuciones del ruido. Al realizar la DVS que solamente retiene esos valores singulares por encima de cierto umbral, podríamos ser capaces de eliminar la mayor parte del ruido y, en realidad, obtener una imagen que no sólo sea de menor tamaño sino también una representación más clara de la superficie.\n\n\n\n1.4.3 No está de más saber que…\n\nYa quedó claro que DVS será importante a la hora de estudiar Análisis de Datos Multivariados.\nEn esta sección vamos a mencionar un par de detalles adicionales que son un nexo entre lo que han estudiado de Álgebra Lineal, estamos aplicando ahora mediante Métodos Numéricos y verán su utilidad en Análisis de Datos Multivariados:\n\nUn poquito más arriba mencionamos al rango de una matriz. Recordamos que el rango de una matriz es el número máximo de vectores fila o columna que linealmente independientes. Si \\(\\mathbf A_{m\\times n}\\), \\(rg(\\mathbf A) \\leq min(m, n)\\). Si \\(rg(\\mathbf A) = min(m, n)\\), se dice que la matriz es de rango completo. En Estadística, el rango de una matriz de datos nos indica la dimensión real necesaria para representar el conjunto de datos, o el número real de variables distintas que disponemos. Analizar el rango de una matriz de datos es la clave para reducir el número de variables sin pérdida de información.\nTambién ha aparecido en la DVS la matriz \\(\\mathbf A^TA\\). Pasó por ahí casi desapercibida, pero esta matriz es fundamental en Estadística. Si \\(\\mathbf A\\) es nuestra matriz de datos, que generalmente denotamos con \\(\\mathbf X\\), entonces \\(\\mathbf X^T \\mathbf X\\) es proporcional a la matriz de variancias y covariancias. Su determinante es una medida global de la independencia entre las variables. A mayor determinante, mayor independencia. Para que la DVS pueda hacer una buena compresión con pocos valores singulares \\(k\\), se necesita que las variables estén correlacionadas.\nLa traza de una matriz es una medida global de su tamaño que se obtiene sumando sus elementos diagonales. Por ejemplo, la traza de una matriz de variancias y covariancias es la suma de todas las variancias de las variables. Entonces, la suma de los elementos diagonales es una medida de variabilidad que, a diferencia del determinante, no tiene en cuenta las relaciones entre las variables.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Valores y vectores propios</span>"
    ]
  }
]