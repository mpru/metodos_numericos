[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Métodos Numéricos con Python",
    "section": "",
    "text": "Prefacio\nEl presente documento es la guía de estudio para la asignatura Métodos Numéricos de la Licenciatura en Estadística (Universidad Nacional de Rosario). Se ha utilizado como fuente para la creación de este material a la bibliografía mencionada en el programa de la asignatura. La asignatura se complementa con variados materiales (prácticas, ejemplos, proyectos) disponibles en el aula virtual del curso de acceso privado.\nEstos apuntes no están libres de contener errores. Sugerencias para corregirlos o para expresar de manera más adecuada las ideas volcadas son siempre bienvenidas1.\nPrimera publicación: enero 2024.\n\n\n\n\n\n\nEn general, no se cuenta con derechos para las imágenes empleadas (a menos que sean de creación propia). Ante cualquier problema, contactar al autor.↩︎",
    "crumbs": [
      "Prefacio"
    ]
  },
  {
    "objectID": "03_sistemas_lineales.html#introducción",
    "href": "03_sistemas_lineales.html#introducción",
    "title": "1  Unidad 3: Resolución de sistemas de ecuaciones lineales",
    "section": "1.1 Introducción",
    "text": "1.1 Introducción\n\nObjetivo de la unidad: examinar los aspectos numéricos que se presentan al resolver sistemas de ecuaciones lineales de la forma:\n\n\\[\n\\begin{cases}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2 \\\\\n\\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n = b_n\n\\end{cases}\n\\]\n\nEl anterior es un sistema de \\(n\\) ecuaciones con \\(n\\) incógnitas: decimos que es un sistema de orden \\(n \\times n\\), en el cual los coeficientes \\(a_{ij}\\) y los términos independientes \\(b_i\\) son reales fijos.\nRecordemos que los sistemas de ecuaciones lineales se puede representar matricialmente: \\(\\mathbf{Ax=b}\\), de dimensión \\(n\\times n\\), \\(n\\times 1\\) y \\(n\\times 1\\), respectivamente.\n\n\\[\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{n1} & a_{n2} & \\cdots & a_{nn}\n\\end{bmatrix}\n\\times\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n\\end{bmatrix}\n\\]\n\nLlamamos matriz ampliada o aumentada a:\n\n\\[\n\\begin{bmatrix}\n    \\mathbf{A}, & \\mathbf{b}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\na_{11} & a_{12} & \\cdots & a_{1n} & b_1\\\\\na_{21} & a_{22} & \\cdots & a_{2n} & b_2\\\\\n\\vdots & \\vdots & \\ddots & \\vdots & \\vdots\\\\\na_{n1} & a_{n2} & \\cdots & a_{nn} & b_n\n\\end{array}\n\\end{bmatrix}\n\\]\n\n1.1.1 Repaso de lo que ya sabemos\n\nUn sistema de ecuaciones lineales se clasifica en:\n\nCompatible determinado: tiene una única solución\nCompatible indeterminado: tiene infinitas soluciones\nIncompatible: no existe solución\n\n\n\n\nAdemás, las siguientes condiciones son equivalentes:\n\nEl sistema \\(\\mathbf{Ax=b}\\) tiene solución única (es compatible determinado).\nLa matriz \\(\\mathbf{A}\\) es invertible (existe \\(\\mathbf{A^{-1}}\\)).\nLa matriz \\(\\mathbf{A}\\) es no singular (\\(\\det \\mathbf{A} \\neq 0\\)).\nEl sistema \\(\\mathbf{Ax=0}\\) tiene como única solución \\(\\mathbf{x=0}\\).\n\nDos sistemas de orden \\(n \\times n\\) son equivalentes si tienen el mismo conjunto de soluciones.\nExisten ciertas transformaciones sobre las ecuaciones de un sistema que no cambian el conjunto de soluciones (producen un sistema equivalente). Si llamamos con \\(E_i\\) a cada una de las ecuaciones del sistema:\n\nIntercambio. El orden de las ecuaciones puede cambiarse: \\(E_i \\leftrightarrow E_j\\)\nEscalado. Multiplicar una ecuación por una constante no nula: \\(\\lambda E_i \\rightarrow E_i\\)\nSustitución: una ecuación puede ser reemplazada por una combinación lineal de las ecuaciones del sistema (teorema fundamental de la equivalencia). Por ejemplo: \\(E_i + \\lambda E_j \\rightarrow E_i\\)\n\nMediante una secuencia de estas operaciones, un sistema lineal se transforma en uno nuevo más fácil de resolver y con las mismas soluciones.\nRealizar estas transformaciones sobre las ecuaciones es equivalente a realizar las mismas operaciones sobre las filas de la matriz aumentada.\n\n\n\n1.1.2 Notación\n\nEn esta sección presentamos la notación que utilizaremos para expresar algoritmos con operaciones matriciales (facilitando su programación).\nDada una matriz \\(\\mathbf{Z}\\) de dimensión \\(n \\times m\\), anotamos:\n\n\\(z_{ij} = \\mathbf{Z}[i, j]\\): elemento en la fila \\(i\\) y columna \\(j\\) de la matriz \\(\\mathbf{Z}\\)\n\\(\\mathbf{Z}[i,]\\): vector fila de dimensión \\(1\\times m\\) constituido por la \\(i\\)-ésima fila de la matriz \\(\\mathbf{Z}\\)\n\\(\\mathbf{Z}[,j]\\): vector columna \\(n\\times 1\\) constituido por la \\(j\\)-ésima columna de la matriz \\(\\mathbf{Z}\\)\n\\(\\mathbf{Z}[i,k:l]\\): matriz de dimensión \\(1\\times (l-k+1)\\) constituida con los elementos \\(z_{i,k}, z_{i,k+1}, \\cdots, z_{i,l}\\) de la matriz \\(\\mathbf{Z}\\), \\(l \\geq k\\).\n\\(\\mathbf{Z}[c:d,k:l]\\): matriz de dimensión \\((d-c+1)\\times (l-k+1)\\) constituida por la submatriz que contiene las filas de \\(\\mathbf{Z}\\) desde la \\(c\\) hasta \\(d\\) y las columnas de \\(\\mathbf{Z}\\) desde la \\(k\\) hasta la \\(l\\), \\(d \\geq c\\), \\(l \\geq k\\).\n\nDado un vector \\(\\mathbf{Z}\\) de largo \\(n\\), anotamos:\n\n\\(\\mathbf{Z}[i]\\): elemento \\(i\\)-ésimo del vector \\(\\mathbf{Z}\\)\n\\(\\mathbf{Z}[k:l]\\): vector de largo \\((l-k+1)\\) constituido con los elementos \\(z_{k}, z_{k+1}, \\cdots, z_{l}\\) del vector \\(\\mathbf{Z}\\), \\(l \\geq k\\).\n\n\n\n\n1.1.3 Métodos de resolución de sistemas de ecuaciones\n\nMétodos exactos: permiten obtener la solución del sistema de manera directa.\n\nMétodo de sustitución hacia atrás o hacia adelante\nMétodo de Eliminación de Gauss\nMétodo de Gauss-Jordan\n\nMétodos aproximados: utilizan algoritmos iterativos que calculan las solución del sistema por aproximaciones sucesivas.\n\nMétodo de Jacobi\nMétodo de Gauss-Seidel\n\nEn muchas ocasiones los métodos aproximados permiten obtener un grado de exactitud superior al que se puede obtener empleando los denominados métodos exactos, debido fundamentalmente a los errores de redondeo que se producen en el proceso.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unidad 3: Resolución de sistemas de ecuaciones lineales</span>"
    ]
  },
  {
    "objectID": "03_sistemas_lineales.html#métodos-exactos",
    "href": "03_sistemas_lineales.html#métodos-exactos",
    "title": "1  Unidad 3: Resolución de sistemas de ecuaciones lineales",
    "section": "1.2 Métodos exactos",
    "text": "1.2 Métodos exactos\n\nEn esta sección repasaremos los métodos que ya conocen y que han aplicado “a mano” muchas veces para resolver sistemas de ecuaciones, con la particularidad de que pensaremos cómo expresar sus algoritmos para poder programarlos en la computadora.\nEntre los aspectos que tendremos que considerar, se encuentra la necesidad de aplicar estrategias de pivoteo (para evitar un pivote igual a cero cuando se resuelve el sistema) y la posibilidad de realizar reordenamientos de la matriz de coeficientes para disminuir los errores de redondeo producidos en los cálculos computacionales.\n\n\n1.2.1 Sistemas con matriz de coeficientes diagonal\n\nSi la matriz de coeficientes es de esta forma:\n\\[\n  \\begin{bmatrix}\n  a_{11} & 0 & \\cdots & 0 \\\\\n  0 & a_{22} & \\cdots & 0 \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  0 & 0 & \\cdots & a_{nn}\n  \\end{bmatrix}\n  \\times\n  \\begin{bmatrix}\n  x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n  \\end{bmatrix}\n  =\n  \\begin{bmatrix}\n  b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n  \\end{bmatrix}\n  \\]\nel sistema se reduce a \\(n\\) ecuaciones simples y la solución sencillamente es:\n\\[\n  \\mathbf{x} =\n  \\begin{bmatrix}\n      b_1/a_{11} \\\\ b_2/a_{22} \\\\ \\vdots \\\\ b_n/a_{nn}\n  \\end{bmatrix}\n  \\]\n\n\n\n1.2.2 Sistemas con matriz de coeficientes triangular\n\nVeamos el siguiente ejemplo y su resolución, que seguramente comprendemos sin problema:\n\n\nknitr::include_graphics(\"Plots/U3/pizarra1.png\")\n\n\n\n\n\nHemos encontrado el valor de la última incógnita y fuimos haciendo reemplazos en las ecuaciones desde abajo hacia arriba para encontrar las restantes.\nEsto se conoce como sustitución regresiva o hacia atrás.\nPara poder formalizar este procedimiento que nos resulta natural y así estar en condiciones para implementarlo computacionalmente, tenemos que encontrar una fórmula que exprese sin ambigüedad cómo hacer todos esos cálculos.\nDe manera genera, vamos a considerar que la matriz \\(\\mathbf{A}\\) es triangular superior:\n\\[\n  \\begin{bmatrix}\n  a_{11} & a_{12} & a_{13} & \\cdots & a_{1n} \\\\\n  0 & a_{22} & a_{23} & \\cdots & a_{2n} \\\\\n  0 & 0 & a_{33} & \\cdots & a_{3n} \\\\\n  \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n  0 & 0 & 0 & \\cdots & a_{nn}\n  \\end{bmatrix}\n  \\times\n  \\begin{bmatrix}\n  x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n\n  \\end{bmatrix}\n  =\n  \\begin{bmatrix}\n  b_1 \\\\ b_2 \\\\ \\vdots \\\\ b_n\n  \\end{bmatrix}\n  \\]\nLa solución de \\(x_n\\) es inmediata y a partir de ella se encuentran las restantes en orden inverso \\(x_{n-1}, \\cdots, x_1\\), aplicando el algoritmo de sustitución regresiva:\n\n\\[\nx_n = \\frac{b_n}{a_{nn}} \\text{ y } x_k = \\frac{b_k - \\sum_{j = k+1}^{n}a_{kj}x_j}{a_{kk}} \\quad k = n-1, n-2, \\cdots, 1\n\\]\n\nEmpleando la notación matricial vista, la suma en el algoritmo puede reescribirse como un producto matricial:\n\n\\[\n\\mathbf{x}[n] = \\frac{\\mathbf{b}[n]}{\\mathbf{A}[n,n]} \\qquad \\mathbf{x}[k] = \\frac{\\mathbf{b}[k] - \\mathbf{A}[k, (k+1):n] \\times \\mathbf{x}[(k+1):n]}{\\mathbf{A}[k,k]}\n\\qquad k = n-1, n-2, \\cdots, 1\n\\]\n\nIniciando el vector solución con ceros, \\(\\mathbf{x} = [0 ~ 0 \\cdots 0]\\), la expresión anterior se simplifica aún más:\n\n\\[\n\\mathbf{x}[n] = \\frac{\\mathbf{b}[n]}{\\mathbf{A}[n,n]} \\qquad \\mathbf{x}[k] = \\frac{\\mathbf{b}[k] - \\mathbf{A}[k, ] \\times \\mathbf{x}}{\\mathbf{A}[k,k]}\n\\qquad k = n-1, n-2, \\cdots, 1\n\\]\n\nEsto nos permite escribir un algoritmo para la implementación de este método:\n\n\nknitr::include_graphics(\"Plots/U3/alg1.png\")\n\n\n\n\n\nEjercicio:\nOperar “a mano” de forma matricial siguiendo los pasos del algoritmo para corroborar su funcionamiento con el sistema de ecuaciones del ejemplo.\n\n\nSi la matriz \\(\\mathbf{A}\\) es triangular inferior, la obtención de la solución es análoga al caso anterior y el algoritmo recibe el nombre de sustitución hacia adelante o progresiva.\n\n\n\n1.2.3 Eliminación gaussiana\n\nEs un método para resolver un sistema lineal general \\(\\mathbf{Ax=b}\\) de \\(n\\) ecuaciones con \\(n\\) incógnitas.\nEl objetivo es construir un sistema equivalente donde la matriz de coeficientes sea triangular superior para obtener las soluciones con el algoritmo de sustitución regresiva.\nEl método consiste en ir eliminando incógnitas en las ecuaciones de manera sucesiva, aplicando las operaciones entre ecuaciones que producen sistemas equivalentes (repasadas en la intro).\nEjemplo: resolver el siguiente sistema\n\n\\[\n\\begin{cases}\nx+2y-z+3t=-8 \\\\\n2x+2z-t=13 \\\\\n-x+y+z-t=8\\\\\n3x+3y-z+2t = -1\n\\end{cases}\n\\quad\n\\mathbf{A}=\n\\begin{bmatrix}\n1 & 2 & -1 & 3 \\\\\n2 & 0 & 2 & -1 \\\\\n-1 & 1 & 1 & -1 \\\\\n3 & 3 & -1 & 2   \n\\end{bmatrix}\n\\quad\n\\mathbf{x} =\n\\begin{bmatrix}\nx \\\\ y \\\\ z \\\\ t\n\\end{bmatrix}\n\\quad\n\\mathbf{b} =\n\\begin{bmatrix}\n-8 \\\\ 13 \\\\ 8 \\\\ -1\n\\end{bmatrix}\n\\]\n\nMatriz aumentada:\n\n\\[\n\\begin{bmatrix}\n    \\mathbf{A} & \\mathbf{b}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n    1 & 2 & -1 & 3 &|& -8\\\\\n    2 & 0 & 2 & -1 &|& 13\\\\\n    -1 & 1 & 1 & -1 &|& 8\\\\\n    3 & 3 & -1 & 2 &|& -1  \n\\end{bmatrix}\n\\] “A mano”\n\nPrimero recordemos cómo resolvemos este tipo de sistemas “a mano”. Seguramente nos resulte familiar la aplicación del método de Gauss, representada en la siguiente imagen:\n\n\nknitr::include_graphics(\"Plots/U3/pizarra2.png\")\n\n\n\n\n\nComo podemos ver, aunque no lo pensemos, al hacer “este por este menos este por este” estamos pasando sucesivamente de un sistema a otro equivalente, con una matriz de coeficientes más sencilla (van apareciendo ceros, es decir, se van elminando incógnitas) hasta llegar a tener una matriz triangular superior, como en el caso anterior.\nSi bien este procedimiento puede ser largo, estamos acostumbrados y no nos presenta ninguna dificultad.\nEl desafío surge si tenemos que pensar en una forma de expresarlo formalmente y de hallar fórmulas que puedan ser programadas, de modo que la computadora pueda resolver el sistema por nosotros.\nA continuación vamos a repetir el proceso paso por paso para poder deducir un algoritmo.\n\nPrimer paso\n\nEn el primer paso eliminamos la incógnita \\(x\\) en las ecuaciones 2, 3 y 4, buscando que queden ceros en toda la primera columna excepto en el elemento diagonal.\nObservando con detenimiento, esto se logra realizando reemplazos en las ecuaciones de esta forma:\n\\[E_r - m_{r1} \\times E_1 \\rightarrow E_r \\qquad r=2, 3, 4\\]\nLos valores \\(m_{r1}\\) se llaman multiplicadores y se definen como:\n\\[m_{r1} = \\frac{a_{r1}}{a_{11}}, \\quad r = 2, 3, 4\\]\nEsta elección de multiplicadores hace que se generen ceros en la primera columna desde la fila 2 hasta la última.\nEn este ejemplo, nos quedan:\n\\[\nm_{21} = \\frac{a_{21}}{a_{11}} = 2 \\qquad m_{31} = \\frac{a_{31}}{a_{11}} = -1 \\qquad m_{41} = \\frac{a_{41}}{a_{11}} = 3\n\\]\nLos reemplazos a realizar entonces son:\n\\[\n\\begin{array}{rc}\nE_2 - 2 \\times E_1 & \\rightarrow E_2 \\\\\nE_3 - (-1) \\times E_1 & \\rightarrow E_3 \\\\\nE_4 - 3 \\times E_1 & \\rightarrow E_4\n\\end{array}\n\\]\nEl elemento \\(a_{11}=1\\) que está en el denominador de todos los multiplicadores se le dice pivote y a la fila 1 que se usa en los reemplazos se le dice fila pivote.\nEl resultado de los reemplazos anteriores es:\n\\[\n\\begin{matrix}\n\\text{pivote} \\rightarrow \\\\ m_{21} = 2 \\\\ m_{31} = -1 \\\\ m_{41} = 3\n\\end{matrix}\n\\begin{bmatrix}\n1 & 2 & -1 & 3 &|& -8\\\\\n2 & 0 & 2 & -1 &|& 13\\\\\n-1 & 1 & 1 & -1 &|& 8\\\\\n3 & 3 & -1 & 2 &|& -1  \n\\end{bmatrix}\n\\implies\n\\begin{bmatrix}\n1 & 2 & -1 & 3 &|& -8\\\\\n0 & -4 & 4 & -7 &|& 29\\\\\n0 & 3 & 0 & 2 &|& 0\\\\\n0 & -3 & 2 & -7 &|& 23  \n\\end{bmatrix}\n\\]\n\nSegundo paso\n\nEn el segundo paso, eliminamos la incógnita \\(y\\) en las ecuaciones 3 y 4.\nLa fila pivote pasa a ser la segunda y el pivote es \\(a_{22}=-4\\).\nLos multiplicadores son \\(m_{r2}=a_{r2}/a_{22}\\), \\(r=3,4\\):\n\\[\nm_{32} = \\frac{a_{32}}{a_{22}} = -3/4 \\qquad m_{42} = \\frac{a_{42}}{a_{22}} = 3/4\n\\]\ndando lugar a los reemplazos:\n\\[\n\\begin{array}{rc}\nE_3 - (-3/4) \\times E_2 & \\rightarrow E_3 \\\\\nE_4 - 3/4 \\times E_2 & \\rightarrow E_4\n\\end{array}\n\\]\nEl resultado es:\n\n\\[\n\\begin{matrix}\n\\\\ \\text{pivote} \\rightarrow \\\\ m_{32} = -3/4 \\\\ m_{42} = 3/4\n\\end{matrix}\n\\begin{bmatrix}\n1 & 2 & -1 & 3 &|& -8\\\\\n0 & -4 & 4 & -7 &|& 29\\\\\n0 & 3 & 0 & 2 &|& 0\\\\\n0 & -3 & 2 & -7 &|& 23  \n\\end{bmatrix}\n\\implies\n\\begin{bmatrix}\n1 & 2 & -1 & 3 &|& -8\\\\\n0 & -4 & 4 & -7 &|& 29\\\\\n0 & 0 & 3 & -13/4 &|& 87/4\\\\\n0 & 0 & -1 & -7/4 &|& 5/4  \n\\end{bmatrix}\n\\]\nTercer paso\n\nFinalmente, eliminamos la incógnita \\(z\\) en la última ecuación.\nLa fila pivote es la 3º y el pivote es \\(a_{33}=3\\).\nEl multiplicador es \\(m_{43}=a_{43}/a_{33}=-1/3\\).\nEl reemplazo a realizar es:\n\\[E_4 - (-1/3) \\times E_3  \\rightarrow E_4\\]\nEl resultado es:\n\n\\[\n\\begin{matrix}\n\\\\ \\\\ \\text{pivote} \\rightarrow \\\\ m_{43} = -1/3\n\\end{matrix}\n\\begin{bmatrix}\n1 & 2 & -1 & 3 &|& -8\\\\\n0 & -4 & 4 & -7 &|& 29\\\\\n0 & 0 & 3 & -13/4 &|& 87/4\\\\\n0 & 0 & -1 & -7/4 &|& 5/4  \n\\end{bmatrix}\n\\implies\n\\begin{bmatrix}\n1 & 2 & -1 & 3 &|& -8\\\\\n0 & -4 & 4 & -7 &|& 29\\\\\n0 & 0 & 3 & -13/4 &|& 87/4\\\\\n0 & 0 & 0 & -17/6 &|& 17/2  \n\\end{bmatrix}\n\\]\n\nHemos llegado a un sistema equivalente cuya matriz de coeficientes es triangular superior, en el que aplicamos el algoritmo de sustitución regresiva:\n\\[\n\\left\\{\n\\begin{aligned}\nx+2y-z-3t &=-8 \\cr\n-4y+4z-7t &=29\\cr\n3z-13/4t &=87/4\\cr\n-17/6t &=17/2\n\\end{aligned}\n\\right.\n\\implies\n\\left\\{\n\\begin{aligned}\nx &= 1\\\\ y &= 2\\\\ z &= 4\\\\ t &= -3\n\\end{aligned}\n\\right.\n\\]\nEsta matriz triangular es algo diferente a la que obtuvimos cuando hicimos las cuentas con el “cuadrito” de Gauss a mano, pero es equivalente (si prestamos atención, las filas que son distintas en realidad son los mismos valores multiplicados todos por una constante).\nLa ventaja de haber hecho los cálculos en términos de multiplicadores y reemplazos de filas, es que ahora tenemos un sistema para crear un algoritmo y poder programarlo.\nPara cada fila \\(q\\) desde \\(1\\) hasta \\(n-1\\) y luego para cada fila \\(r\\) desde \\(q+1\\) hasta \\(n\\), hay que hacer los reemplazos:\n\\[\n  m_{rq} = \\frac{a_{rq}}{a_{qq}} \\qquad E_r - m_{rq} \\times E_q \\rightarrow E_r \\qquad q=1,...,n-1 \\qquad r = q+1, ..., n\n  \\]\nEl algoritmo resulta ser:\n\n\nknitr::include_graphics(\"Plots/U3/alg2.png\")\n\n\n\n\n\n\n1.2.4 Eliminación gaussiana con estrategias de pivoteo\n\n1.2.4.1 Pivoteo trivial\n\nEn el algoritmo anterior es necesario que los pivotes \\(a_{qq} \\neq 0 ~\\forall q\\).\nSi en uno de los pasos encontramos un \\(a_{qq} = 0\\), debemos intercambiar la \\(q\\)-ésima fila por una cualquiera de las siguientes, por ejemplo la fila \\(k\\), en la que \\(a_{kq} \\neq 0, k&gt;q\\).\nEsta estrategia para hallar un pivote no nulo se llama pivoteo trivial.\nEjemplo: verificar “a mano” que con el siguiente sistema, si seguimos el algoritmo anterior, incurrimos en este problema. En cambio, si seguimos el algoritmo con pivoteo trivial llegamos a la solución.\n\\[\n\\begin{cases}\nx-2y+z=-4 \\\\\n-2x+4y-3z=3 \\\\\nx-3y-4z=-1\n\\end{cases}\n\\]\n\n\n1.2.4.1.1 Estrategias de pivoteo para reducir los errores de redondeo\n\nComo ya sabemos, dado que las computadoras usan una aritmética cuya precisión está fijada de antemano, es posible que cada vez que se realice una operación aritmética se introduzca un pequeño error de redondeo.\nEn la resolución de ecuaciones por eliminación gaussiana, un adecuado reordenamiento de las filas en el momento indicado puede reducir notablemente los errores cometidos.\nPor ejemplo, se puede mostrar cómo buscar en cada paso un multiplicador de menor magnitud (es decir, un pivote de mayor magnitud) mejora la precisión de los resultados.\nPor eso, existen estrategias de pivoteo que no solamente hacen intercambio de filas cuando se tiene un pivote nulo, si no cuando se detecta que un reordenamiento puede reducir el error de redondeo.\n\nPivoteo parcial\n\nPara reducir la propagación de los errores de redondeo, antes de comenzar una nueva ronda de reemplazos con el pivote \\(a_{qq}\\) se evalúa si debajo en la misma columna hay algún elemento con mayor valor absoluto y en ese caso se intercambian las respectivas filas.\nEs decir, se busca si existe \\(r\\) tal que \\(|a_{rq}| &gt; |a_{qq}|,\\quad r&gt;q\\) para luego intercambiar las filas \\(q\\) y \\(r\\).\nEste proceso suele conservar las magnitudes relativas de los elementos de la matriz triangular superior en el mismo orden de magnitud que las de los coeficientes de la matriz original.\nVer ejemplos 1 y 2 de las páginas 280 y 281.\n\nPivoteo parcial escalado\n\nReduce aún más los efectos de la propagación de los errores.\nSe elige el elemento de la columna \\(q\\)-ésima, en o por debajo de la diagonal principal, que tiene mayor tamaño relativo con respecto al resto de los elementos de su fila.\nAntes de definir el multiplicador y hacer las operaciones correspondientes, en cada paso se debe:\n\nBuscar el máximo valor absoluto en cada fila:\n\n\\[\n  s_r = max\\{|a_{rq}|, |a_{r,q+1}|, \\cdots, |a_{rn}| \\} \\quad r = q, q+1, \\cdots, n\n  \\]\n\nElegir como fila pivote a la que tenga el mayor valor de \\(\\frac{|a_{rq}|}{s_r}\\), \\(r = q, q+1, \\cdots, n\\).\nIntercambiar la fila \\(q\\) con la fila hallada en el punto anterior.\n\nVer los ejemplos bajo el título “Ilustración” de las páginas 283 y 284.\n\n\n\n\n1.2.4.2 Algoritmos\n\nEn el siguiente algoritmo se detalla la sistematización necesaria para implementar las tres estrategias de pivoteo mencionadas.\nNo se pedirá su programación, se puede usar la función provista.\nSe sugiere la lectura tanto del algoritmo como de la función para distinguir cómo son aplicadas las estrategias.\n\n\nknitr::include_graphics(\"Plots/U3/alg3a.png\")\n\n\n\nknitr::include_graphics(\"Plots/U3/alg3b.png\")\n\n\n\nknitr::include_graphics(\"Plots/U3/alg3c.png\")\n\n\n\n# knitr::include_graphics(\"Plots/U3/alg3d.png\")\n\n\n\n\n1.2.5 Método de eliminación de Gauss-Jordan\n\nYa vimos que el método de Gauss transforma la matriz de coeficientes \\(\\mathbf{A}\\) en una matriz triangular superior, haciendo estos reemplazos:\n\\[\n  m_{rq} = \\frac{a_{rq}}{a_{qq}} \\qquad E_r - m_{rq} \\times E_q \\rightarrow E_r \\qquad q=1,...,n-1 \\qquad r = q+1, ..., n\n  \\]\nEl método de Gauss-Jordan transforma \\(\\mathbf{A}\\) hasta obtener la matriz identidad.\nLos multiplicadores se definen de la misma forma, pero en cada paso \\(q\\) se sustituyen todas las filas, no sólo las filas posteriores a la fila \\(q\\).\nPara cada fila \\(q\\) desde \\(1\\) hasta \\(n-1\\) y luego para cada fila \\(r\\) desde \\(1\\) hasta \\(n\\), hay que hacer los reemplazos:\n\\[\n  \\text{Si } r \\neq q:\\, m_{rq} = \\frac{a_{rq}}{a_{qq}} \\qquad E_r - m_{rq} \\times E_q \\rightarrow E_r\n  \\]\n\\[\n  \\text{Si } r = q:\\,E_r /a_{rr} \\rightarrow E_r\n  \\]\nTrabajando de esta forma, además de resolver el sistema se puede hallar fácilmente la inversa de \\(\\mathbf{A}\\).\nPara esto hay que concatenar a la derecha de la matriz aumentada una matriz identidad de orden \\(n\\).\nCuando en la submatriz izquierda se llega a la matriz identidad, en el centro habrá quedado el vector solución y a su derecha la matriz inversa.\nRetomemos el último ejemplo para resolver el sistema mediante Gauss-Jordan y, de paso, obtener \\(\\mathbf{A}^{-1}\\).\n\n\\[\n\\begin{bmatrix}\n1 & 2 & -1 & 3 &|& -8\\\\\n2 & 0 & 2 & -1 &|& 13\\\\\n-1 & 1 & 1 & -1 &|& 8\\\\\n3 & 3 & -1 & 2  &|& -1\n\\end{bmatrix}\n\\]\n\nAgregamos la matriz identidad a su derecha:\n\n\\[\n\\begin{bmatrix}\n1  & 2 & -1 & 3  &|& -8 &|& 1 & 0 & 0 & 0\\\\\n2  & 0 & 2  & -1 &|& 13 &|& 0 & 1 & 0 & 0 \\\\\n-1 & 1 & 1  & -1 &|& 8  &|& 0 & 0 & 1 & 0\\\\\n3  & 3 & -1 & 2  &|& -1 &|& 0 & 0 & 0 &  1\n\\end{bmatrix}\n\\]\n\nDefinimos los multiplicadores y aplicamos las sustituciones:\n\nPaso 1\n\\[\n\\begin{matrix}\nE_2 - 2 E_1 \\rightarrow E_2\\\\\nE_3 + E_1 \\rightarrow E_3 \\\\\nE_4 - 3 E_1 \\rightarrow E_4 \\\\\nE_1 / 1 \\rightarrow E_1\n\\end{matrix}\n\\implies\n\\begin{bmatrix}\n1 & 2 & -1 & 3 &|& -8 &|& 1 & 0 & 0 & 0\\\\\n0 & -4 & 4 & -7 &|& 29 &|& -2 & 1 & 0 & 0\\\\\n0 & 3 & 0 & 2 &|& 0 &|& 1 & 0 & 1 & 0\\\\\n0 & -3 & 2 & -7 &|& 23  &|& -3 & 0 & 0 & 1\n\\end{bmatrix}\n\\]\nPaso 2\n\\[\n\\begin{matrix}\nE_1 + 1/2 E_2 \\rightarrow E_1\\\\\nE_3 + 3/4 E_2 \\rightarrow E_3 \\\\\nE_4 - 3/4 E_2 \\rightarrow E_4 \\\\\nE_2 / (-4) \\rightarrow E_2\n\\end{matrix}\n\\implies\n\\begin{bmatrix}\n1 & 0 & 1 & -1/2 &|& 13/2 &|& 0 & 1/2 & 0 & 0\\\\\n0 & 1 & -1 & 7/4 &|& -29/4 &|& 1/2 & -1/4 & 0 & 0\\\\\n0 & 0 & 3 & -13/4 &|& 87/4 &|& -1/2 & 3/4 & 1 & 0\\\\\n0 & 0 & -1 & -7/4 &|& 5/4  &|& -3/2 & -3/4 & 0 & 1\n\\end{bmatrix}\n\\]\nPaso 3\n\\[\n\\begin{matrix}\nE_1 - 1/3 E_3 \\rightarrow E_1\\\\\nE_2 + 1/3 E_3 \\rightarrow E_2 \\\\\nE_4 + 1/3 E_3 \\rightarrow E_4 \\\\\nE_3 / 3 \\rightarrow E_3\n\\end{matrix}\n\\implies\n\\begin{bmatrix}\n1 & 0 & 0 & 7/12 &|& -3/4    &|& 1/6 & 1/4 & -1/3 & 0\\\\\n0 & 1 & 0 & 2/3 &|& 0       &|& 1/3 & 0 & 1/3 & 0\\\\\n0 & 0 & 1 & -13/12 &|& 29/4 &|& -1/6 & 1/4 & 1/3 & 0\\\\\n0 & 0 & 0 & -17/6 &|& 17/2  &|& -5/3 & -1/2 & 1/3 & 1\n\\end{bmatrix}\n\\]\nPaso 4\n\\[\n\\begin{matrix}\nE_1 + 7/34 E_4 \\rightarrow E_1\\\\\nE_2 + 4/17 E_4 \\rightarrow E_2 \\\\\nE_3 - 13/34 E_4 \\rightarrow E_3 \\\\\nE_4 / (-17/6) \\rightarrow E_4\n\\end{matrix}\n\\implies\n\\begin{bmatrix}\n1 & 0 & 0 & 0 &|& 1   &|& -3/17 & 5/34 & -9/34 & 7/34\\\\\n0 & 1 & 0 & 0 &|& 2   &|& -2/17 & -1/17 & 7/17 & 4/17\\\\\n0 & 0 & 1 & 0 &|& 4   &|& 8/17 & 15/34 & 7/34 & -13/34\\\\\n0 & 0 & 0 & 1 &|& -3  &|& 10/17 & 3/17 & -2/17 & -6/17\n\\end{bmatrix}\n\\]\n\nEn el la columna central tenemos la solución del sistema: \\(x=1\\), \\(y=2\\), \\(z=4\\) y \\(t=-3\\) y en la submatriz de la derecha, la inversa de \\(\\mathbf{A}\\):\n\n\\[\n\\mathbf{A}^{-1} =\n  \\begin{bmatrix}\n  -3/17 & 5/34 & -9/34 & 7/34\\\\\n   -2/17 & -1/17 & 7/17 & 4/17\\\\\n   8/17 & 15/34 & 7/34 & -13/34\\\\\n  10/17 & 3/17 & -2/17 & -6/17\n\\end{bmatrix}\n\\]\n\n¿Por qué este procedimiento nos devuelve la inversa de la matriz de coeficientes?\nRecordar que la inversa de \\(\\mathbf{A}\\) es aquella matriz \\(\\mathbf{A}^{-1}\\) que verifica \\(\\mathbf{AA}^{-1} =\\mathbf{A}^{-1}\\mathbf{A}=\\mathbf{I}\\)\nEntonces si queremos hallar la matriz \\(\\mathbf{A}^{-1}\\):\n\\[\n  \\mathbf{A}=\n\\begin{bmatrix}\n1 & 2 & -1 & 3 \\\\\n2 & 0 & 2 & -1 \\\\\n-1 & 1 & 1 & -1 \\\\\n3 & 3 & -1 & 2   \n\\end{bmatrix}\n\\qquad\n  \\mathbf{A}^{-1} =\n  \\begin{bmatrix}\n  c_{11} & c_{12} & c_{13} & c_{14} \\\\\n  c_{21} & c_{22} & c_{23} & c_{24} \\\\\n  c_{31} & c_{32} & c_{33} & c_{34} \\\\\n  c_{41} & c_{42} & c_{43} & c_{44}\n  \\end{bmatrix}\n  \\]\npodemos plantear el siguiente producto matricial:\n\\[\n  \\begin{bmatrix}\n1 & 2 & -1 & 3 \\\\\n2 & 0 & 2 & -1 \\\\\n-1 & 1 & 1 & -1 \\\\\n3 & 3 & -1 & 2   \n\\end{bmatrix}\n  \\times\n  \\begin{bmatrix}\n  c_{11} & c_{12} & c_{13} & c_{14} \\\\\n  c_{21} & c_{22} & c_{23} & c_{24} \\\\\n  c_{31} & c_{32} & c_{33} & c_{34} \\\\\n  c_{41} & c_{42} & c_{43} & c_{44}\n  \\end{bmatrix}\n  =\n  \\begin{bmatrix}\n  1 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 0\\\\\n  0 & 0 & 1 & 0  \\\\\n  0 & 0 & 0 & 1\n  \\end{bmatrix}\n  \\]\nque da lugar a cuatro sistemas de ecuaciones con la misma matriz de coeficientes, pero distintos vectores de términos independientes, cada uno de los cuales es una columna de la matriz identidad:\n\\[\n  \\begin{bmatrix}\n  c_{11}+2c_{21}-c_{31}+3c_{41}   & c_{12}+2c_{22}-c_{32}+3c_{42}     & c_{13}+2c_{23}-c_{33}+3c_{43}     & c_{14}+2c_{24}-c_{34}+3c_{44}     \\\\\n  2c_{11}+2c_{31}-c_{41}                  & 2c_{12}+2c_{32}-c_{42}                    & 2c_{13}+2c_{33}-c_{43}                    & 2c_{14}+2c_{34}-c_{44}                    \\\\\n  -c_{11}+c_{21}+c_{31}-c_{41}        & -c_{12}+c_{22}+c_{32}-c_{42}      & -c_{13}+c_{23}+c_{33}-c_{43}      & -c_{14}+c_{24}+c_{34}-c_{44}      \\\\\n  3c_{11}+3c_{21}-c_{31}+2c_{41}  & 3c_{12}+3c_{22}-c_{32}+2c_{42}    & 3c_{13}+3c_{23}-c_{33}+2c_{43}    & 3c_{14}+3c_{24}-c_{34}+2c_{44}          \n  \\end{bmatrix}\n  =\n  \\begin{bmatrix}\n  1 & 0 & 0 & 0 \\\\\n  0 & 1 & 0 & 0\\\\\n  0 & 0 & 1 & 0  \\\\\n  0 & 0 & 0 & 1\n  \\end{bmatrix}\n  \\]\n\\[\n  \\begin{cases}\n  c_{11}+2c_{21}-c_{31}+3c_{41}=1 \\\\\n  2c_{11}+2c_{31}-c_{41} =0 \\\\\n  -c_{11}+c_{21}+c_{31}-c_{41} =0 \\\\\n  3c_{11}+3c_{21}-c_{31}+2c_{41}= 0\n  \\end{cases}\n  \\quad\n  \\begin{cases}\n  c_{12}+2c_{22}-c_{32}+3c_{42}=0 \\\\\n  2c_{12}+2c_{32}-c_{42} =1 \\\\\n  -c_{12}+c_{22}+c_{32}-c_{42} =0 \\\\\n  3c_{12}+3c_{22}-c_{32}+2c_{42}= 0\n  \\end{cases}\n      \\quad\n  \\begin{cases}\n  c_{13}+2c_{23}-c_{33}+3c_{43}=0 \\\\\n  2c_{13}+2c_{33}-c_{43} =0 \\\\\n  -c_{13}+c_{23}+c_{33}-c_{43} =1 \\\\\n  3c_{13}+3c_{23}-c_{33}+2c_{43}= 0\n  \\end{cases}\n      \\quad\n  \\begin{cases}\n  c_{14}+2c_{24}-c_{34}+3c_{44}=0 \\\\\n  2c_{14}+2c_{34}-c_{44} =0 \\\\\n  -c_{14}+c_{24}+c_{34}-c_{44} =0 \\\\\n  3c_{14}+3c_{24}-c_{34}+2c_{44}= 1\n  \\end{cases}\n  \\]\nAl agregar la matriz identidad en la matriz aumentada, lo que estamos haciendo es resolver simultáneamente 5 sistemas de ecuaciones: el original y los 4 que nos permiten encontrar las columnas de la matriz inversa de \\(\\mathbf{A}\\).\nA continuación podemos ver el algoritmo de Gauss-Jordan. Se provee también la función de Python que lo implementa:\n::: {.cell}\nknitr::include_graphics(\"Plots/U3/alg4.png\")\n::: {.cell-output-display}  ::: :::\n\n\n\n1.2.6 Factorización LU\n\nDefinición: En álgebra lineal la factorización de una matriz es la descomposición de la misma como producto de dos o más matrices que toman una forma especificada.\n\n\nLas factorizaciones de las matrices se utilizan para facilitar el cálculo de diversos elementos como determinantes e inversas y para optimizar algoritmos.\nLa factorización LU se obtiene cuando se expresa a una matriz cuadrada \\(\\mathbf{A}\\) como el producto entre una matriz triangular inferior \\(\\mathbf{L}\\) y una matriz triangular superior \\(\\mathbf{U}\\), es decir, \\(\\mathbf{A} = \\mathbf{L} \\mathbf{U}\\).\nSi bien existe más de una forma de encontrar la descomposición LU de una matriz \\(\\mathbf{A}\\), la misma se puede obtener fácilmente aprovechando los cálculos que se realizan con el método de eliminación gaussiana para resolver un sistema de la forma \\(\\mathbf{Ax} = \\mathbf{b}\\).\nRecordemos el ejemplo visto en la sección sobre eliminación gaussiana.\n\n\\[\n\\begin{cases}\nx+2y-z+3t=-8 \\\\\n2x+2z-t=13 \\\\\n-x+y+z-t=8\\\\\n3x+3y-z+2t = -1\n\\end{cases}\n\\quad\n\\mathbf{A}=\n\\begin{bmatrix}\n1 & 2 & -1 & 3 \\\\\n2 & 0 & 2 & -1 \\\\\n-1 & 1 & 1 & -1 \\\\\n3 & 3 & -1 & 2   \n\\end{bmatrix}\n\\quad\n\\mathbf{x} =\n\\begin{bmatrix}\nx \\\\ y \\\\ z \\\\ t\n\\end{bmatrix}\n\\quad\n\\mathbf{b} =\n\\begin{bmatrix}\n-8 \\\\ 13 \\\\ 8 \\\\ -1\n\\end{bmatrix}\n\\]\n\nLa factorización LU de la matriz \\(\\mathbf{A}\\) se obtiene al considerar como \\(\\mathbf{U}\\) a la matriz triangular superior que obtuvimos al finalizar la eliminación gaussiana y al crear \\(\\mathbf{L}\\) con los multiplicadores acomodados como se indica a continuación:\n\n\\[\n\\mathbf{L}=\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\nm_{21} & 1 & 0 & 0 \\\\\nm_{31} & m_{32} & 1 & 0 \\\\\nm_{41} & m_{42} & m_{43} & 1   \n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 0 & 0 & 0 \\\\\n2 & 1 & 0 & 0 \\\\\n-1 & -3/4 & 1 & 0 \\\\\n3 & 3/4 & -1/3 & 1   \n\\end{bmatrix}\n\\qquad\n\\mathbf{U}=\n\\begin{bmatrix}\n1 & 2 & -1 & 3 \\\\\n0 & -4 & 4 & -7 \\\\\n0 & 0 & 3 & -13/4 \\\\\n0 & 0 & 0 & -17/6   \n\\end{bmatrix}\n\\]\n\nPodemos verificar estos cálculos definiendo estas matrices y encontrando que su producto es igual a \\(\\mathbf{A}\\).\n\n\n1.2.6.1 Usos de la factorización LU\n\nUna vez encontrada la factorización, esta se puede emplear para facilitar el cálculo de determinantes e inversas y también para resolver otros sistemas del tipo \\(\\mathbf{Ax} = \\mathbf{b}\\), con la misma \\(\\mathbf{A}\\) pero cualquier \\(\\mathbf{b}\\).\n\na. Uso de LU para resolver sistemas de ecuaciones lineales\n\nSea \\(\\mathbf{Ax} = \\mathbf{b}\\) un sistema lineal que debe resolverse para \\(\\mathbf{x}\\) y supongamos que contamos con la factorización LU de \\(\\mathbf{A}\\) de modo que \\(\\mathbf{A}\\): \\(\\mathbf{A} = \\mathbf{LU}\\).\nLa solución \\(\\mathbf{x}\\) se encuentra con estos dos pasos:\n\nResolver el sistema \\(\\mathbf{Ly} = \\mathbf{b}\\), donde \\(\\mathbf{y}\\) es el vector de incógnitas. Siendo \\(\\mathbf{L}\\) triangular inferior, este sistema se resuelve fácilmente mediante sustitución hacia adelante.\nResolver el sistema \\(\\mathbf{Ux} = \\mathbf{y}\\), donde \\(\\mathbf{y}\\) es el vector obtenido en el punto anterior. Siendo \\(\\mathbf{U}\\) triangular superior, este sistema se resuelve fácilmente mediante sustitución hacia atrás.\n\nComo se puede ver, los dos pasos consisten resolver sistemas de ecuaciones con matrices de coeficientes triangulares, lo cual es sencillo y no requiere de la implementación de eliminación gaussiana (de todos modos, se necesita aplicar eliminación gaussiana o algún otro proceso para obtener la factorización LU en primer lugar).\nSin embargo, este procedimiento se puede aplicar para resolver el sistema múltiples veces para diferentes \\(\\mathbf{b}\\). Aplicar eliminación gaussiana una vez para obtener la factorización LU y luego emplearla en la solución de cada sistema es más eficiente que aplicar siempre eliminación gaussiana para cada caso.\nSe dice que las matrices \\(\\mathbf{L}\\) y \\(\\mathbf{U}\\) representan en sí mismas el proceso de eliminación gaussiana.\nEjemplo: resolver el sistema \\(\\mathbf{Ax} = \\mathbf{b}\\) con \\(\\mathbf{b}^t =[-8 \\quad 13 \\quad 8 \\quad -1]\\) a partir de la descomposición LU dada por:\n\\[\n  \\mathbf{L}=\n  \\begin{bmatrix}\n  1 & 0 & 0 & 0 \\\\\n  2 & 1 & 0 & 0 \\\\\n  -1 & -3/4 & 1 & 0 \\\\\n  3 & 3/4 & -1/3 & 1   \n  \\end{bmatrix}\n  \\qquad\n  \\mathbf{U}=\n  \\begin{bmatrix}\n  1 & 2 & -1 & 3 \\\\\n  0 & -4 & 4 & -7 \\\\\n  0 & 0 & 3 & -13/4 \\\\\n  0 & 0 & 0 & -17/6   \n  \\end{bmatrix}\n  \\]\n\nb. Uso de LU para calcular la inversa de \\(\\mathbf{A}\\)\n\nLa inversa de \\(\\mathbf{A}\\) se puede obtener como \\(\\mathbf{A}^{-1} = \\mathbf{U}^{-1}\\mathbf{L}^{-1}\\).\nO también se pueden aplicar los dos pasos anteriores para resolver \\(n\\) veces el sistema \\(\\mathbf{Ax} = \\mathbf{b}\\), haciendo que \\(\\mathbf{b}\\) sea igual a cada una de las columnas de la matriz identidad en cada oportunidad (cada vez \\(x\\) nos dará una columna de \\(\\mathbf{A}^{-1}\\)).\nEstas formas no son más eficientes que el método tradicional de hallar la inversa de una matriz.\n\nc. Uso de LU para calcular el determinante de \\(\\mathbf{A}\\)\n\n\\(det(\\mathbf{A}) = det(\\mathbf{L}) det(\\mathbf{U})\\).\nEl determinante de una matriz triangular se obtiene fácilmente como el producto de los elementos diagonales.\n\n\n\n1.2.6.2 Otros detalles\n\nLa factorización LU puede llegar a fallar en algunos casos. Por ejemplo, si la obtenemos a través de la eliminación gaussiana, no podríamos completarla cuando algún pivote es cero.\nAsí como en la eliminación gaussiana aplicamos pivoteo para evitar ese problema, en el proceso de factorización LU intercambiar el orden de las filas de la matriz también es la solución.\nDe hecho, se demuestra que toda matriz cuadrada admite una factorización LU si previamente se reordenan convenientemente sus filas.\nEl reordenamiento de filas se representa matemáticamente a través de una matriz \\(\\mathbf{P}\\) de unos y ceros que premultiplicada a \\(\\mathbf{A}\\) produce el efecto de intercambiar filas. Esta matriz se llama matriz de permutación. Luego, toda matriz cuadrada admite una factorización LU del tipo \\(\\mathbf{PA} = \\mathbf{L} \\mathbf{U}\\) (también llamada factorización PLU).\nLa forma vista para obtener de la factorización LU siguiendo los pasos de la eliminación gaussiana recibe el nombre de método de Doolittle y como resultado la matriz \\(\\mathbf{L}\\) tiene unos en su diagonal.\nOtro método que resulta en que la matriz \\(\\mathbf{U}\\) sea la que tenga unos en la diagonal se llama método de Crout.\nLa factorización de Cholesky es un caso particular de la factorización LU que se aplica a matrices semidefinidas positivas y que resulta en que \\(\\mathbf{U} = \\mathbf{L}^t\\), es decir, \\(\\mathbf{A} = \\mathbf{L} \\mathbf{L}^t\\).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unidad 3: Resolución de sistemas de ecuaciones lineales</span>"
    ]
  },
  {
    "objectID": "03_sistemas_lineales.html#métodos-aproximados-o-iterativos",
    "href": "03_sistemas_lineales.html#métodos-aproximados-o-iterativos",
    "title": "1  Unidad 3: Resolución de sistemas de ecuaciones lineales",
    "section": "1.3 Métodos Aproximados o Iterativos",
    "text": "1.3 Métodos Aproximados o Iterativos\n\nEn esta sección describimos los métodos iterativos de Jacobi y de Gauss-Seidel para resolver sistemas de ecuaciones lineales.\nUna técnica iterativa para resolver el sistema lineal \\(\\mathbf{Ax} = \\mathbf{b}\\) inicia con una aproximación \\(\\mathbf{x}^{(0)}\\) para la solución \\(\\mathbf{x}\\) y genera una sucesión de vectores \\(\\{\\mathbf{x}^{(k)}\\}_{k=0}^{\\infty}\\) que convergen a \\(\\mathbf{x}\\).\nLas técnicas iterativas casi nunca se usan para resolver sistemas lineales de dimensiones pequeñas ya que el tiempo requerido para conseguir una precisión suficiente excede el requerido para las técnicas directas, como la eliminación gaussiana.\nPara grandes sistemas con un alto porcentaje de entradas 0, sin embargo, estas técnicas son eficientes en términos tanto de almacenamiento como de cálculo computacional.\n\n\n1.3.1 Método de Jacobi\n\nConsideremos el sistema:\n\\[\n  \\begin{cases}\n  4x-y+z=7 \\\\\n  4x-8y+z=-21 \\\\\n  -2x+y+5z=15\n  \\end{cases}\n  \\implies\n  \\begin{cases}\n  x=(7+y-z)/4 \\\\\n  y=(21+4x+z)/8 \\\\\n  z=(15+2x-y)/5\n  \\end{cases}\n  \\]\nDespejar una incógnita en cada ecuación provee expresiones que sugieren la idea de un proceso iterativo.\nDado un vector de valores iniciales \\(\\mathbf{x}^{(0)}=(x^{(0)}, y^{(0)}, z^{(0)})\\), operar con la siguiente fórmula de recurrencia hasta la convergencia:\n\\[\n  \\begin{cases}\n  x^{(k+1)}=(7+y^{(k)}-z^{(k)})/4 \\\\\n  y^{(k+1)}=(21+4x^{(k)}+z^{(k)})/8 \\\\\n  z^{(k+1)}=(15+2x^{(k)}-y^{(k)})/5\n  \\end{cases}\n  \\]\nPor ejemplo, tomando el valor inicial \\((1, 2, 2)\\), el proceso converge hacia la solución exacta del sistema \\((2, 4, 3)\\).\nUsando 4 posiciones decimales con redondeo luego de la coma:\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(x^{(k)}\\)\n\\(y^{(k)}\\)\n\\(z^{(k)}\\)\n\n\n\n\n0\n1\n2\n2\n\n\n1\n1.7500\n3.3750\n3.0000\n\n\n2\n1.8438\n3.8750\n3.0250\n\n\n3\n1.9625\n3.9250\n2.9625\n\n\n4\n1.9906\n3.9766\n3.0000\n\n\n5\n1.9942\n3.9953\n3.0009\n\n\n6\n1.9986\n3.9972\n2.9986\n\n\n7\n1.9997\n3.9991\n3.0000\n\n\n8\n1.9998\n3.9999\n3.0001\n\n\n9\n2.0000\n3.9999\n2.9999\n\n\n10\n2.0000\n4.0000\n3.0000\n\n\n\nObservación: no siempre este método converge. Es sensible al ordenamiento de las ecuaciones dentro del sistema.\nEjemplo: tomamos el mismo sistema de antes pero intercambiamos las filas 1 y 3:\n\n\\[\n\\begin{cases}\n4x-y+z=7 \\\\\n4x-8y+z=-21 \\\\\n-2x+y+5z=15\n\\end{cases}\n\\implies\n\\begin{cases}\n-2x+y+5z=15 \\\\\n4x-8y+z=-21 \\\\\n4x-y+z=7\n\\end{cases}\n\\]\n\\[\n\\implies\n\\begin{cases}\nx=(-15+y+5z)/2 \\\\\ny=(21+4x+z)/8 \\\\\nz=7-4x+y\n\\end{cases}\n\\implies\n\\begin{cases}\nx^{(k+1)}=(-15+y^{(k)}+5z^{(k)})/2 \\\\\ny^{(k+1)}=(21+4x^{(k)}+z^{(k)})/8 \\\\\nz^{(k+1)}=7-4x^{(k)}+y^{(k)}\n\\end{cases}\n\\]\n\nTomando el mismo valor inicial \\((1, 2, 2)\\), esta vez el proceso diverge:\n\n\n\n\n\n\n\n\n\n\\(k\\)\n\\(x^{(k)}\\)\n\\(y^{(k)}\\)\n\\(z^{(k)}\\)\n\n\n\n\n0\n1\n2\n2\n\n\n1\n-1.5000\n3.3750\n5.0000\n\n\n2\n6.6875\n2.5000\n16.3750\n\n\n3\n34.6875\n8.0156\n-17.2500\n\n\n4\n-46.6172\n17.8125\n-123.7344\n\n\n5\n-307.9298\n-36.1504\n211.2813\n\n\n6\n502.6281\n-124.9297\n1202.5688\n\n\n7\n2936.4572\n404.2602\n-2128.4421\n\n\n8\n-5126.4752\n1204.7983\n-11334.5686\n\n\n9\n-27741.5224\n-3977.4337\n21717.6991\n\n\n10\n52298.0309\n-11153.4238\n106995.6559\n\n\n\nVamos a desarrollar fórmulas para la expresión general de este método:\n\n\\[\n\\begin{cases}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2 \\\\\n\\vdots \\\\\na_{j1}x_1 + a_{j2}x_2 + \\cdots + a_{jn}x_n = b_j \\\\\n\\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n = b_n\n\\end{cases}\n\\implies\n\\begin{cases}\nx_1 = \\frac{b_1 - a_{12}x_2 - \\cdots - a_{1n}x_n}{ a_{11}}\\\\\nx_2 = \\frac{b_2 - a_{21}x_1 - a_{23}x_3 - \\cdots - a_{2n}x_n}{ a_{22}}\\\\\n\\vdots \\\\\nx_j = \\frac{b_j - a_{j1}x_1 \\cdots - a_{j(j-1)}x_{j-1} - a_{j(j+1)}x_{j+1} - \\cdots - a_{jn}x_n}{ a_{jj}}\\\\\n\\vdots \\\\\nx_n = \\frac{b_n - a_{n1}x_1 - \\cdots - a_{n(n-1)}x_{n-1}}{a_{nn}}\\\\\n\\end{cases}\n\\]\n\nA partir de estas expresiones, se plantea el proceso iterativo que arranca con valores iniciales \\((x_1^{(0)}, ..., x_n^{(0)})\\):\n\n\\[\n\\begin{cases}\nx_1^{(k+1)} = \\frac{b_1 - a_{12}x_2^{(k)} - \\cdots - a_{1n}x_n^{(k)}}{ a_{11}}\\\\\nx_2^{(k+1)} = \\frac{b_2 - a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - \\cdots - a_{2n}x_n^{(k)}}{ a_{22}}\\\\\n\\vdots \\\\\nx_j^{(k+1)} = \\frac{b_j - a_{j1}x_1^{(k)} \\cdots - a_{j(j-1)}x_{j-1}^{(k)} - a_{j(j+1)}x_{j+1}^{(k)} - \\cdots - a_{jn}x_n^{(k)}}{ a_{jj}}\\\\\n\\vdots \\\\\nx_n^{(k+1)} = \\frac{b_n - a_{n1}x_1^{(k)} - \\cdots - a_{n(n-1)}x_{n-1}^{(k)}}{a_{nn}}\\\\\n\\end{cases}\n\\]\n\\[\n\\implies\nx_j^{(k+1)} = \\frac{1}{a_{jj}} \\Big[ b_j - \\sum_{\\substack{i=1 \\\\ i \\neq j}}^{n}  a_{ji}x_{j}^{(k)} \\Big]\n\\qquad j=1, 2, \\cdots n \\qquad k=0, 1, 2, \\cdots\n\\]\n\nLo anterior se puede expresar de forma matricial para simplificar la tarea de programación o para tener una expresión más cómoda a fines de estudiar propiedades teóricas del método.\nSi descomponemos a la matriz \\(\\mathbf{A}\\) como \\(\\mathbf{A=D+R}\\), donde \\(\\mathbf{D}\\) es la matriz diagonal formada con la diagonal de \\(\\mathbf{A}\\) y \\(\\mathbf{R}\\) es igual a \\(\\mathbf{A}\\) excepto en la diagonal donde posee todos ceros, tenemos:\n\\[\\begin{align*}\n  \\mathbf{Ax} &= \\mathbf{b} \\\\\n  \\mathbf{(D+R)x} &= \\mathbf{b} \\\\\n  \\mathbf{Dx} &= \\mathbf{b} - \\mathbf{Rx}  \\\\\n  \\mathbf{x} &= \\mathbf{D}^{-1} (\\mathbf{b} - \\mathbf{Rx})  \\\\\n  \\end{align*}\\]\nEsto da lugar a la siguiente fórmula de recurrencia, que es equivalente a la vista anteriormente:\n\\[\n  \\mathbf{x}^{(k+1)} = \\mathbf{D}^{-1} (\\mathbf{b} - \\mathbf{Rx}^{(k)})\n  \\]\nRequisito: ningún elemento en la diagonal de \\(\\mathbf{A}\\) es cero.\n\n\n\n1.3.2 Método de Gauss-Seidel\n\nToma la misma idea que Jacobi, pero con una pequeña modificación para acelerar la convergencia.\nLa diferencia está en que apenas calcula un nuevo valor de las incógnitas, Gauss-Seidel lo usa inmediatamente en el cálculo de las restantes dentro del mismo paso iterativo, en lugar esperar a la próxima ronda.\nRetomando el ejemplo anterior:\n\n\\[\n\\begin{cases}\n4x-y+z=7 \\\\\n4x-8y+z=-21 \\\\\n-2x+y+5z=15\n\\end{cases}\n\\implies\n\\begin{cases}\nx=(7+y-z)/4 \\\\\ny=(21+4x+z)/8 \\\\\nz=(15+2x-y)/5\n\\end{cases}\n\\stackrel{Jacobi}{\\implies}\n\\begin{cases}\nx^{(k+1)}=(7+y^{(k)}-z^{(k)})/4 \\\\\ny^{(k+1)}=(21+4x^{(k)}+z^{(k)})/8 \\\\\nz^{(k+1)}=(15+2x^{(k)}-y^{(k)})/5\n\\end{cases}\n\\]\n\nEl proceso iterativo de Gauss-Seidel es:\n\n\\[\n\\begin{cases}\nx^{(k+1)}=(7+y^{(k)}-z^{(k)})/4 \\\\\ny^{(k+1)}=(21+4x^{(k+1)}+z^{(k)})/8 \\\\\nz^{(k+1)}=(15+2x^{(k+1)}-y^{(k+1)})/5\n\\end{cases}\n\\]\n\nTomando otra vez el valor inicial \\((1, 2, 2)\\) y operando con 4 posiciones decimales con redondeo luego de la coma:\n\n\n\n\n\n\n\n\n\n\n\n\n\nJacobi\n\n\n\nGauss-Seidel\n\n\n\n\n\n\n\n\\(k\\)\n\\(x^{(k)}\\)\n\\(y^{(k)}\\)\n\\(z^{(k)}\\)\n\\(k\\)\n\\(x^{(k)}\\)\n\\(y^{(k)}\\)\n\\(z^{(k)}\\)\n\n\n0\n1\n2\n2\n0\n1\n2\n2\n\n\n1\n1.7500\n3.3750\n3.0000\n1\n1.7500\n3.7500\n2.9500\n\n\n2\n1.8438\n3.8750\n3.0250\n2\n1.9500\n3.9688\n2.9862\n\n\n3\n1.9625\n3.9250\n2.9625\n3\n1.9957\n3.9961\n2.9991\n\n\n4\n1.9906\n3.9766\n3.0000\n4\n1.9993\n3.9995\n2.9998\n\n\n5\n1.9942\n3.9953\n3.0009\n5\n1.9999\n3.9999\n3.0000\n\n\n6\n1.9986\n3.9972\n2.9986\n6\n2.0000\n4.0000\n3.0000\n\n\n7\n1.9997\n3.9991\n3.0000\n\n\n\n\n\n\n8\n1.9998\n3.9999\n3.0001\n\n\n\n\n\n\n9\n2.0000\n3.9999\n2.9999\n\n\n\n\n\n\n10\n2.0000\n4.0000\n3.0000\n\n\n\n\n\n\n\nEn general, este método converge más rápidamente que el de Jacobi, pero no siempre es así.\nExisten sistemas lineales para los que el método de Jacobi converge y el de Gauss-Seidel no.\nLa expresión general para el método es:\n\n\\[\n\\begin{cases}\na_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n = b_1 \\\\\na_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n = b_2 \\\\\n\\vdots \\\\\na_{j1}x_1 + a_{j2}x_2 + \\cdots + a_{jn}x_n = b_j \\\\\n\\vdots \\\\\na_{n1}x_1 + a_{n2}x_2 + \\cdots + a_{nn}x_n = b_n\n\\end{cases}\n\\implies\n\\begin{cases}\nx_1 = \\frac{b_1 - a_{12}x_2 - \\cdots - a_{1n}x_n}{ a_{11}}\\\\\nx_2 = \\frac{b_2 - a_{21}x_1 - a_{23}x_3 - \\cdots - a_{2n}x_n}{ a_{22}}\\\\\n\\vdots \\\\\nx_j = \\frac{b_j - a_{j1}x_1 \\cdots - a_{j(j-1)}x_{j-1} - a_{j(j+1)}x_{j+1} - \\cdots - a_{jn}x_n}{ a_{jj}}\\\\\n\\vdots \\\\\nx_n = \\frac{b_n - a_{n1}x_1 - \\cdots - a_{n(n-1)}x_{n-1}}{a_{nn}}\\\\\n\\end{cases}\n\\]\n\nA partir de estas expresiones, se plantea el proceso iterativo que arranca con valores iniciales \\((x_1^{(0)}, ..., x_n^{(0)})\\):\n\n\\[\n\\begin{cases}\nx_1^{(k+1)} = \\frac{b_1 - a_{12}x_2^{(k)} - \\cdots - a_{1n}x_n^{(k)}}{ a_{11}}\\\\\nx_2^{(k+1)} = \\frac{b_2 - a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - \\cdots - a_{2n}x_n^{(k)}}{ a_{22}}\\\\\n\\vdots \\\\\nx_j^{(k+1)} = \\frac{b_j - a_{j1}x_1^{(k+1)} \\cdots - a_{j(j-1)}x_{j-1}^{(k+1)} - a_{j(j+1)}x_{j+1}^{(k)} - \\cdots - a_{jn}x_n^{(k)}}{ a_{jj}}\\\\\n\\vdots \\\\\nx_n^{(k+1)} = \\frac{b_n - a_{n1}x_1^{(k+1)} - \\cdots - a_{n(n-1)}x_{n-1}^{(k+1)}}{a_{nn}}\\\\\n\\end{cases}\n\\]\n\\[\n\\implies\nx_j^{(k+1)} = \\frac{1}{a_{jj}} \\Big[ b_j - \\sum_{i&lt;j} a_{ji} x_i^{(k+1)} - \\sum_{i&gt;j} a_{ji} x_i^{(k)} \\Big] \\qquad j = 1,...,n \\quad k = 0, 1, 2, ...\n\\]\n\nPara encontrar una expresión matricial, descomponemos a la matriz \\(\\mathbf{A}\\) como \\(\\mathbf{A=L+U}\\), donde \\(\\mathbf{L}\\) es una matriz triangular inferior (incluyendo la diagonal de \\(\\mathbf{A}\\)) y \\(\\mathbf{U}\\) es una matriz triangular superior (con ceros en la diagonal):\n\\[\\begin{align*}\n  \\mathbf{Ax} &= \\mathbf{b} \\\\\n  \\mathbf{(L+U)x} &= \\mathbf{b} \\\\\n  \\mathbf{Lx} &= \\mathbf{b} - \\mathbf{Ux}  \\\\\n  \\mathbf{x} &= \\mathbf{L}^{-1} (\\mathbf{b} - \\mathbf{Ux})  \\\\\n  \\end{align*}\\]\nEsto da lugar a la siguiente fórmula de recurrencia, que es equivalente a la vista anteriormente:\n\\[\n  \\mathbf{x}^{(k+1)} = \\mathbf{L}^{-1} (\\mathbf{b} - \\mathbf{Ux}^{(k)})\n  \\]\nRequisito: ningún elemento en la diagonal de \\(\\mathbf{A}\\) es cero.\n\n\n\n1.3.3 Convergencia\n\nCondiciones para la convergencia\n\nPara establecer una condición de convergencia de estos métodos, necesitamos la siguiente definición:\n\n\nDefinición:\n\nSe dice que una matriz \\(\\mathbf{A}\\) de orden \\(n \\times n\\) es diagonal dominante cuando cada elemento diagonal es mayor o igual a la suma del resto de los elementos de su fila en valor absoluto:\n\\[\n  |a_{kk}| \\geq \\sum\\limits_{\\substack{j=1 \\\\ j\\neq k}}^n |a_{kj}| \\quad \\forall \\,k=1, 2, \\cdots, n\n  \\]\nSe dice que una matriz \\(\\mathbf{A}\\) de orden \\(n \\times n\\) es estrictamente diagonal dominante cuando cada elemento diagonal es mayor a la suma del resto de los elementos de su fila en valor absoluto:\n\\[\n  |a_{kk}| &gt; \\sum\\limits_{\\substack{j=1 \\\\ j\\neq k}}^n |a_{kj}| \\quad \\forall \\,k=1, 2, \\cdots, n\n  \\]\n\n\n\nLos sistemas de ecuaciones cuya matriz de coeficiente es estrictamente diagonal dominante poseen algunas ventajas.\nUna matriz estrictamente diagonal dominante es no singular (el sistema es compatible determinado).\nAdemás, la eliminación gaussiana se puede realizar sin intercambios de fila o columna y los cálculos serán estables respecto al crecimiento de los errores de redondeo.\nY en particular, para estas matrices está asegurada la convergencia de los métodos iterativos, como lo indica el siguiente teorema:\n\n\nTeorema: si la matriz \\(\\mathbf{A}\\) es estrictamente diagonal dominante, entonces el sistema lineal \\(\\mathbf{Ax=b}\\) tiene solución única y los procesos iterativos de Jacobi y de Gauss-Seidel convergen hacia la misma cualquiera sea el vector de partida \\(\\mathbf{x}^{(0)}\\).\n\n\nEs una condición suficiente pero no necesaria.\nEn la práctica, ante un sistema buscamos reordenar las columnas y filas para intentar obtener una matriz con estas características.\nSi no es posible, igualmente buscamos colocar en la diagonal los elementos de mayor valor absoluto, puesto que esto puede favorecer la convergencia.\n\n\n\n1.3.3.1 Criterios para detener el proceso iterativo\n\nPara establecer si los métodos iterativos están convergiendo y así detener el proceso, necesitamos una forma para medir la distancia entre vectores columna \\(n\\)-dimensionales (es decir, entre dos vectores \\(\\mathbf{x}^{(k)}\\) consecutivos en la iteración).\nPara definir la distancia en \\(\\mathbb{R}^n\\) usamos la noción de norma, que es la generalización del valor absoluto en \\(\\mathbb{R}\\).\n\n\nDefinición: Una norma vectorial en \\(\\mathbb{R}^n\\) es una función \\(||\\cdot||\\) de \\(\\mathbb{R}^n\\) a \\(\\mathbb{R}\\), con las siguientes propiedades:\n\n\\(||\\mathbf{x}|| \\geq 0 \\quad \\forall \\, \\mathbf{x} \\in \\mathbb{R}^n\\)\n\\(||\\mathbf{x}|| = 0 \\Leftrightarrow \\mathbf{x} = \\mathbf{0}\\)\n\\(||\\alpha \\mathbf{x}|| = |\\alpha| ||\\mathbf{x}|| \\quad \\forall \\, \\alpha \\in \\mathbb{R}, \\mathbf{x} \\in \\mathbb{R}^n\\)\n\\(|| \\mathbf{x} + \\mathbf{y}|| \\leq ||\\mathbf{x}|| + ||\\mathbf{y}|| \\quad \\forall \\, \\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n\\)\n\n\n\nLas siguientes son tres normas ampliamente utilizadas:\n\nNorma \\(l_1\\): \\(||\\mathbf{x}||_1 = \\sum_{j=1}^n |x_j|\\)\nNorma \\(l_2\\) o euclideana: \\(||\\mathbf{x}||_2 = \\sqrt{\\mathbf{x}^t\\mathbf{x}} = \\sqrt{\\sum_{j=1}^n x_j^2}\\)\nNorma \\(l_{\\infty}\\): \\(||\\mathbf{x}||_{\\infty} = \\max_{1 \\leq j \\leq n} |x_j|\\)\n\nLa norma de un vector proporciona una medida para la distancia entre un vector arbitrario y el vector cero, de la misma forma en la que el valor absoluto de un nùmero real describe su distancia desde 0.\nDe igual forma, la distancia entre dos vectores (que necesitamos para juzgar la convergencia del proceso) está definida como la norma de la diferencia de los vectores, al igual que la distancia entre dos números reales es el valor absoluto de la diferencia.\nLuego, para “comparar” dos vectores sucesivos que aproximan a la solución del sistema, podemos usar las siguientes definiciones de distancias:\n\nDistancia \\(l_1\\) o de Manhattan:\n\\[\n||\\mathbf{x}^{(k+1)}-\\mathbf{x}^{(k)}||_1 =\\sum_{j=1}^n |x_j^{(k+1)} - x_j^{(k)}|\n\\]\nDistancia \\(l_2\\) o euclídea:\n\\[||\\mathbf{x}^{(k+1)}-\\mathbf{x}^{(k)}||_2 = \\sqrt{(\\mathbf{x}^{(k+1)}-\\mathbf{x}^{(k)})^t(\\mathbf{x}^{(k+1)}-\\mathbf{x}^{(k+1)})} = \\sqrt{\\sum_{j=1}^n (x_j^{(k+1)} - x_j^{(k)})^2}\\]\nDistancia \\(l_\\infty\\) o máxima diferencia:\n\n\\[\n  ||\\mathbf{x}^{(k+1)}-\\mathbf{x}^{(k)}||_\\infty = \\max_{j} |x_j^{(k+1)} - x_j^{(k)}|\n  \\]\nEl proceso iterativo se detiene cuando la distancia elegida entre dos vectores solución consecutivos sea menor a algún valor tan pequeño como se desee, \\(\\epsilon\\).\nLo más común es emplear la norma \\(l_\\infty\\).\nSiendo semejante a la definición de error relativo, también es usual iterar hasta que:\n\\[\\frac{||\\mathbf{x}^{(k+1)}-\\mathbf{x}^{(k)}||}{||\\mathbf{x}^{(k+1)}||} &lt; \\epsilon\\]\nPor último, se debe establecer un número máximo de iteraciones, para detener el proceso cuando el mismo no converge.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Unidad 3: Resolución de sistemas de ecuaciones lineales</span>"
    ]
  }
]