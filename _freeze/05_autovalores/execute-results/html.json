{
  "hash": "cc9b1e6ca7c525f0dc822c7c2253cc2a",
  "result": {
    "engine": "knitr",
    "markdown": "# Valores y vectores propios\n\n## Introducci贸n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n<!-- \n\nLa explicacion que usaba antes estaba basada en https://math.stackexchange.com/questions/36815/a-simple-explanation-of-eigenvectors-and-eigenvalues-with-big-picture-ideas-of \n\nMUY BUENA EXPLICACION DE LO QUE ES UN AUTOVALOR Y AUTOVECTOR ACA:\nhttps://math.stackexchange.com/questions/36815/a-simple-explanation-of-eigenvectors-and-eigenvalues-with-big-picture-ideas-of\nhacer algo como lo de la mona lisa de wikipedia pero con vectores en un plano y una matriz, todo de orden 2? \ncomo aca: http://www.visiondummy.com/2014/03/eigenvalues-eigenvectors/\nhttps://medium.com/@isomorphisms/what-do-eigenvectors-mean-intuitively-524b036a777d\n\nhttps://github.com/joanby/curso-numerico-1 \n\nhttps://es.khanacademy.org/math/linear-algebra/matrix-transformations/linear-transformations/a/visualizing-linear-transformations\n\nhttps://www.youtube.com/watch?v=YJfS4_m_0Z8\n\nhttps://aga.frba.utn.edu.ar/matriz-asociada-a-una-transformacion-lineal/\n\nhttps://aga.frba.utn.edu.ar/definicion-y-propiedades-de-las-transformaciones-lineales/\n\nhttps://www.uv.mx/personal/aherrera/files/2014/08/21a.-TRANSFORMACIONES-LINEALES-1.pdf\n\nhttp://mate.dm.uba.ar/~jeronimo/algebra_lineal/Capitulo3.pdf\n-->\n\n\n- Una matriz $m \\times n$ se puede considerar como una funci贸n que utiliza multiplicaci贸n de matrices para transformar vectores columna $n$-dimensionales en vectores columna $m$-dimensional.\n- Por eso, toda matriz $\\mathbf A$ de dimensi贸n $m \\times n$ puede ser pensada como una transformaci贸n lineal de $\\mathbb R^n$ a $\\mathbb R^m$:\n\n$$\nT: \\mathbb R^n \\rightarrow \\mathbb R^m \\quad | \\quad T(\\mathbf x) = \\mathbf{Ax}\n$$\n\n- Nos va a interesar de manera particular los casos donde esta funci贸n est谩 definida por una matriz $\\mathbf A$ cuadrada (de dimensi贸n $n \\times n$), con lo cual la transformaci贸n $\\mathbf{Ax}$ toma un vector en $\\mathbb R^n$ y devuelve otro en $\\mathbb R^n$.\n- Ahora bien, en general no es muy intuitivo saber qu茅 tipo de cambios va a sufrir un vector $\\mathbf x$ si lo premultiplicamos por $\\mathbf A$.\n- Pero hay ciertos vectores que se modifican de una manera muy sencilla: lo 煤nico que hace la matriz $\\mathbf A$ es \"estirarlos\" o \"comprimirlos\". Es decir, puede cambiar su m贸dulo o sentido, pero no su direcci贸n.\n- Expresado matem谩ticamente, para algunos vectores, la transformaci贸n $\\mathbf{Ax}$ da por resultado el mismo vector $\\mathbf{x}$, multiplicado por una constante no nula $\\lambda$: $\\mathbf{Ax} = \\lambda \\mathbf{x}$.\n- Estos vectores que \"cambian poco\" cuando se los transforma mediante la matriz $\\mathbf A$ reciben el nombre de *autovectores*, *vectores propios* o *eigenvectores* de matriz $\\mathbf A$.\n\n::: {.alert .alert-success}\n**Definici贸n**: Un **autovector** de una matriz $\\mathbf{A}$ es cualquier vector $\\mathbf{x}$ para el que s贸lo cambia su escala cuando se lo multiplica con $\\mathbf{A}$, es decir: $\\mathbf{Ax} = \\lambda \\mathbf{x}$, para alg煤n n煤mero $\\lambda$ real o complejo, que recibe el nombre de **autovalor**. En otras palabras:\n\n$$\\mathbf{x} \\text{ es un autovector y } \\lambda \\text{ es un autovalor de }\\mathbf{A} \\iff \\mathbf{Ax} = \\lambda \\mathbf{x}, \\quad \\mathbf{x} \\neq \\mathbf{0}, \\quad \\lambda \\in \\mathbb{C}$$\n:::\n\n- Los autovalores y autovectores son muy importantes en muchas disciplinas, ya que los objetos que se estudian suelen ser representados con vectores y las operaciones que se hacen sobre ellos, con matrices.\n- Entonces si una matriz $\\mathbf{A}$ describe alg煤n tipo de sistema u operaci贸n, los autovectores son aquellos vectores que, cuando pasan por el sistema, se modifican en una forma muy sencilla.\n- Por ejemplo, si la matriz $\\mathbf{A}$ representa transformaciones en $\\mathbb R^2$, en principio $\\mathbf{A}$ podr铆a estirar y rotar a los vectores. Sin embargo, a sus autovectores lo 煤nico que puede hacerles es estirarlos, no rotarlos.\n- Veamos un caso concreto:\n\n$$\\mathbf{A} = \\begin{bmatrix} 3 & 2 \\\\ 1 & 4 \\end{bmatrix} \\qquad \\mathbf{u} = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} \\qquad \\mathbf{v} = \\begin{bmatrix} 1 \\\\ -0.5 \\end{bmatrix} \\qquad \\mathbf{w} = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}$$\n\n- En este gr谩fico podemos ver los vectores antes de transformarlos (premultiplicarlos) mediante $\\mathbf{A}$:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Plots/U5/auto1.png){width=384}\n:::\n:::\n\n\n\n- Y en este gr谩fico podemos ver como quedan luego de la transformaci贸n:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Plots/U5/auto2.png){width=386}\n:::\n:::\n\n\n\n- $\\mathbf{u}$ y $\\mathbf{v}$ no cambiaron su direcci贸n, s贸lo su norma: son **autovectores** de $\\mathbf{A}$, asociados a los autovalores 5 y 2.\n\n- En cambio, la matriz $\\mathbf{A}$ modific贸 la direcci贸n de $\\mathbf{w}$, entonces $\\mathbf{w}$ no es un autovector de $\\mathbf{A}$.\n\n- Haciendo los c谩lculos:\n\n$$\n\\mathbf{Au} = \\begin{bmatrix} 5 \\\\ 5 \\end{bmatrix} = 5 \\mathbf{u} \\qquad\n\\mathbf{Av} = \\begin{bmatrix} 2 \\\\ -1 \\end{bmatrix} = 2\\mathbf{v} \\qquad\n\\mathbf{Aw} = \\begin{bmatrix} 2 \\\\ 4 \\end{bmatrix} \n$$\n\n- De forma general, si $\\mathbf x$ es un autovector asociado con el autovalor real $\\lambda$, entonces $\\mathbf{Ax} = \\lambda \\mathbf x$, por lo que la matriz $\\mathbf{A}$ transforma al vector $\\mathbf{x}$ en un m煤ltiplo escalar de s铆 mismo, con las siguientes opciones:\n\n\ta) Si $\\lambda > 1$, entonces $\\mathbf{A}$ tiene el efecto de expandir $\\mathbf{x}$ en un factor de $\\lambda$.\n\tb) Si $0 < \\lambda < 1$, entonces $\\mathbf{A}$ comprime $\\mathbf{x}$  en un factor de $\\lambda$.\n\tc) Si $\\lambda < 0$, los efectos son similares, pero el sentido de $\\mathbf{Ax}$ se invierte.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Plots/U5/auto3.png){width=420}\n:::\n:::\n\n\n\n### Propiedades\n\n- Se debe observar que si $\\mathbf{x}$ es un autovector asociado con el autovalor $\\lambda$ y $\\alpha$ es cualquier constante diferente de cero, entonces $\\alpha \\mathbf x$ tambi茅n es un autovector asociado con el mismo autovalor ya que:\n\n$$\n\\mathbf A(\\alpha \\mathbf x) = \\alpha (\\mathbf{Ax}) = \\alpha (\\lambda \\mathbf x) = \\lambda (\\alpha \\mathbf x)\n$$\n\n- En el ejemplo anterior vimos que $\\mathbf u = (1, 1)^T$ es un autovector de $\\mathbf u$ asociado al autovalor $\\lambda = 5$. Pero tambi茅n lo es, por ejemplo, $\\mathbf z = 2\\mathbf u = (2, 2)^T$, ya que $\\mathbf A \\mathbf z = (10, 10)^T = 5 (2, 2)^T = 5 \\mathbf z$.\n\n- Si bien hay infinitos autovectores asociados a un autovalor, para todos los autovalores y usando cualquier norma vectorial $||.||$, siempre existe un autovector de norma 1, el cual puede ser hallado a partir de cualquier autovector $\\mathbf x$ como $\\alpha \\mathbf x$, con $\\alpha = ||\\mathbf x||^{-1}$.\n\n- Dada una matriz $\\mathbf{A}$ cuadradada de orden $n$:\n\n    - $\\mathbf{A}$ tiene $n$ autovalores, $\\lambda_1, \\lambda_2, \\cdots, \\lambda_n$, los cuales no necesariamente son todos distintos. Si lo son, los autovectores forman un conjunto linealmente independiente.\n    - $tr(A) = \\sum_{i=1}^n a_{ii} = \\sum_{i=1}^n \\lambda_{i}$.\n    - $\\det(A) = \\prod_{i=1}^n \\lambda_{i}$.\n    - Los autovalores de $\\mathbf{A}^k$ son $\\lambda_1^k, \\lambda_2^k, \\cdots, \\lambda_n^k$.\n    - Si $\\mathbf{A}$ es real y sim茅trica todos sus autovalores son reales y los autovectores correspondientes a distintos autovalores son ortogonales.\n    - Si $\\mathbf{A}$ es triangular los valores propios son los elementos diagonales.\n    - Los autovalores de una matriz y su transpuesta son los mismos.\n    - Si $\\mathbf{A}$ tiene inversa, los autovalores de $\\mathbf{A}^{-1}$ son $1/\\lambda_1, 1/\\lambda_2, \\cdots, 1/\\lambda_n$.\n    - Los autovalores de $\\alpha \\mathbf{A}$ son $\\alpha \\lambda_1, \\alpha \\lambda_2, \\cdots, \\alpha \\lambda_n, \\, \\alpha \\in \\mathbb{R}$.\n    - Dos matrices cuadradas $\\mathbf{A}$ y $\\mathbf{B}$ son *semejantes* o *similares* si existe una matriz invertible $\\mathbf{Q}$ tal que $\\mathbf{B} = \\mathbf{Q}^{-1}\\mathbf{A}\\mathbf{Q}$. Las matrices semejantes tienen los mismos autovalores. \n\n### Obtenci贸n de autovalores y autovectores\n\n- Como estudiar谩n en lgebra Lineal, para hallar autovalores y autovectores se deben seguir los siguientes dos pasos:\n\n\t1. Se determinan los autovalores encontrando las soluciones de la ecuaci贸n algebraica de grado $n$: $det(\\mathbf A - \\lambda \\mathbf I) = 0$ (la inc贸gnita es $\\lambda$).\n\t2. Para cada autovalor $\\lambda$, se determina un autovector al resolver el sistema lineal $n \\times n$: $(\\mathbf A - \\lambda \\mathbf I)\\mathbf x = \\mathbf 0$.\n\n- Estos pasos son el resultado de las siguientes consideraciones:\n\n\ta. A partir de la definici贸n tenemos: $\\mathbf{Ax} = \\lambda \\mathbf{x} \\implies \\mathbf{Ax} - \\lambda \\mathbf{x} = \\mathbf{0} \\implies (\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{x} = \\mathbf{0}$.\n\tb. Esto es un sistema de ecuaciones lineales con matriz de coeficientes $\\mathbf{A} - \\lambda \\mathbf{I}$, vector de inc贸gnitas $\\mathbf x$ (el autovector) y vector de t茅rminos independientes $\\mathbf{0}$. Es decir, es un **sistema homog茅neo**.\n\tc. Un sistema homog茅neo es siempre compatible, ya que al menos tiene la soluci贸n trivial $\\mathbf x = (0, \\cdots, 0)^T$. Esta soluci贸n no nos interesa, puesto que buscamos autovectores y los mismos deben ser no nulos.\n\td. Como sabemos, para que el sistema tenga otra soluci贸n adem谩s de la trivial, se tiene que tratar de un sistema indeterminado, con infinitas soluciones, ya que los sistemas compatibles o bien tienen una sola soluci贸n o infinitas. Esto tiene sentido, porque cada autovalor $\\lambda$ tiene asociados infinitos autovectores. Entonces, para hallar autovectores necesitamos que el sistema $(\\mathbf{A} - \\lambda \\mathbf{I}) \\mathbf{x} = \\mathbf{0}$ sea compatible indeterminado.\n\te. Para que un sistema sea indeterminado, su matriz de coeficientes debe tener determinante igual a 0, es decir: $det(\\mathbf A - \\lambda \\mathbf I) = 0$.\n\tf. Por eso sabemos que los autovalores de $\\mathbf A$ tienen que ser aquellos valores $\\lambda$ que satisfagan la igualdad anterior, que es una ecuaci贸n algebraica en $\\lambda$ de grado $n$. $det(\\mathbf A - \\lambda \\mathbf I)$ recibe el nombre de **polinomio caracter铆stico** de $\\mathbf A$.\n\t\n- **Ejemplo**:\n\n\\begin{gather*}\n\\mathbf{A} = \n\\begin{bmatrix} \n    5 & -2 & 0 \\\\ \n    -2 & 3 & -1 \\\\\n    0 & -1 & 1    \n\\end{bmatrix} \n\\implies \\\\ \\\\\n det(\\mathbf{A} - \\lambda \\mathbf{I}) = \n\\begin{vmatrix}\n    5 - \\lambda & -2 & 0 \\\\ \n    -2 & 3 - \\lambda & -1 \\\\\n    0 & -1 & 1-\\lambda\n\\end{vmatrix}  = \n\\cdots  = -\\lambda^3 + 9 \\lambda^2 - 18 \\lambda + 6 = 0\n\\end{gather*}\n\n- Como pueden verificar ustedes (opcionalmente aplicando los m茅todos de la Unidad 2), las soluciones de la ecuaci贸n caracter铆stica son $\\lambda_1 = 6.2899, \\lambda_2 = 2.2943$ y $\\lambda_3 = 0.4158$, los cuales son los autovalores de $\\mathbf{A}$.\n- Para hallar un autovector asociado a $\\lambda_1 = 6.2899$, resolvemos el sistema de ecuaciones $(\\mathbf{A} - 6.2899 \\, \\mathbf{I}) \\mathbf{x} = \\mathbf{0}$:\n\n\\begin{gather*}\n(\\mathbf{A} - 6.2899 \\, \\mathbf{I}) \\mathbf{x} = \\mathbf{0} \\implies \n\\begin{bmatrix}\n    -1.2899 & -2 & 0 \\\\ \n    -2 & -3.2899 & -1 \\\\\n    0 & -1 & -5.2899\n\\end{bmatrix}\n\\begin{bmatrix}\n    x_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix} \n=\n\\begin{bmatrix}\n    0 \\\\ 0 \\\\ 0\n\\end{bmatrix}\n\\\\ \\\\\n\\implies\n\\begin{cases}\n-1.2899 x_1 -2 x_2 &= 0 \\\\\n-2 x_1 - 3.2899 x_2 - x_3 &= 0\\\\\n-x_2 - 5.2899 x_3 &= 0\n\\end{cases} \\implies\n\\begin{cases}\n    x_1 = 8.2018 x_3\\\\\n    x_2 = -5.2899 x_2\\\\\n    x_3 \\in \\mathbb{R} \n\\end{cases}\n\\end{gather*}\n\n- Como se puede ver la soluci贸n de este sistema homog茅neo no es 煤nica, representando los infinitos autovectores asociados a $\\lambda_1 = 6.2899$. Por ejemplo, si elegimos $x_3 = 1$, obtenemos el autovector:\n\n$$\n\\mathbf{x}_1 = \n\\begin{bmatrix}\n    8.2018 \\\\ -5.2899 \\\\ 1\n\\end{bmatrix} \n$$\n\n- En general, se acostumbra a informar el autovector de norma 1 (que s铆 es 煤nico).\n- De la misma forma se procede con los restantes autovalores $\\lambda_2$ y $\\lambda_3$.\n\n- Hallar la ecuaci贸n caracter铆stica ya es demasiado trabajoso para $n=3$, y mucho m谩s ser谩 para mayor $n$. Ni hablar de resolver el sistema para encontrar los autovectores.\n- Por eso en esta unidad veremos m茅todos que directamente nos dan como resultados los autovectores y autovalores de una matriz.\n\n- Por supuesto, Python trae una funci贸n para esto. Podemos usarla para chequear los resultados, pero no porque sea f谩cil emplearla nos libraremos de estudiar los algoritmos encargados de producir nuestros queridos autovectores y autovalores:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nimport numpy as np\nfrom scipy.linalg import eig\n\nA = np.array([[ 5, -2, 0],\n              [-2, 3, -1],\n              [ 0, -1, 1]])\n\nautovalores, autovectores = eig(A)\nprint(\"Autovalores:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAutovalores:\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(autovalores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[6.2899+0.j 2.2943+0.j 0.4158+0.j]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nAutovectores:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAutovectores:\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(autovectores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 0.836   0.5049  0.2149]\n [-0.5392  0.6831  0.4927]\n [ 0.1019 -0.5277  0.8433]]\n```\n\n\n:::\n:::\n\n\n\n## El M茅todo de Potencia\n\n\n- El **m茅todo de potencia** (tambi茅n conocido como *de las potencias* o *de aproximaciones sucesivas*) es una t茅cnica iterativa que se usa para determinar el autovalor dominante de una matriz y un autovector asociado.\n\n::: {.alert .alert-success}\n**Definici贸n**: sean $\\lambda_1, \\lambda_2, \\cdots, \\lambda_n$ los autovalores de una matriz $n \\times n$,  $\\mathbf{A}$. $\\lambda_1$ es llamado **autovalor dominante** de $\\mathbf A$ si:\n\n$$\n|\\lambda_1| > |\\lambda_i|, \\quad i=2, \\cdots,n\n$$\n\nLos autovectores correspondientes a $\\lambda_1$ se llaman **autovectores dominantes** de $\\mathbf A$.\n:::\n\n- En primer lugar, se debe tomar un vector inicial $\\mathbf x^{(0)}$ con norma $||\\mathbf x^{(0)}||_{\\infty}  =1$. \n- Por ejemplo, para $n=3$ puede ser $\\mathbf x^{(0)} = (1, 1, 1)^T$ o $\\mathbf x^{(0)} = (1, 0, 0)^T$, entre otros.\n- Luego, para cada $k = 1, 2, \\cdots$ se da lugar al siguiente proceso iterativo:\n\n\t1. Calcular $\\mathbf y^{(k)} = \\mathbf A \\mathbf x^{(k-1)}$.\n\t2. Determinar $\\mu^{(k)}$ como la coordenada de mayor valor absoluto en $\\mathbf y^{(k)}$. \n\t\n\t\tEs decir, tomar $\\mu^{(k)} / \\, |\\mu^{(k)}| = ||\\mathbf y^{(k)}||_{\\infty}$. Si hay varias coordenadas que cumplen con esta caracter铆stica, tomar la primera.\n\t\t\n\t3. Calcular: $\\mathbf x^{(k)} = \\frac{\\mathbf y^{(k)}}{\\mu^{(k)}}$\n\n- De esta forma, la sucesi贸n $\\{\\mu^{(k)}\\}^{\\infty}_{k=0}$ converge al autovalor dominante de $\\mathbf A$, mientras que la sucesi贸n $\\{\\mathbf x^{(k)}\\}^{\\infty}_{k=0}$ converge a un autovector asociado de norma $L_{\\infty} = 1$.\n\n- La deducci贸n y justificaci贸n de este m茅todo puede leerse opcionalmente en las p谩ginas 432-433 del libro.\n\n- Retomando el ejemplo de la secci贸n anterior, vamos a aplicar este proceso con:\n\n$$\n\\mathbf{A} = \n\\begin{bmatrix} \n    5 & -2 & 0 \\\\ \n    -2 & 3 & -1 \\\\\n    0 & -1 & 1    \n\\end{bmatrix} \\qquad\n\\mathbf{x}^{(0)} = \n\\begin{bmatrix} \n    1 \\\\ \n    1 \\\\\n    1    \n\\end{bmatrix}\n$$\n\n| $k$   | $\\mathbf{x}^{(k)}$                  | $\\mathbf{y}^{(k)} = \\mathbf{Ax}^{(k)}$   | $\\mu^{(k)}$  | $\\mathbf{x}^{(k+1)}$ = $\\mathbf{y}^{(k)} / \\mu^{(k)}$ | Error ($L_2$) |\n|-----|---------------------------------|---------------------------------|--------|----------------------------------------------|---------------|\n| 0   | [1 1 1]$^T$                 | [3 0 0]$^T$                 | 3      | [1 0 0]$^T$                              | 1.4142        |\n| 1   | [1 0 0]$^T$                 | [5 -2 0]$^T$                | 5      | [1 -0.4 0]$^T$                           | 0.4           |\n| 2   | [1 -0.4 0]$^T$              | [5.8 -3.2 0.4]$^T$          | 5.8    | [1 -0.5517 0.0690]$^T$                   | 0.1667        |\n| 3   | [1 -0.5517 0.0690]$^T$      | [6.1034 -3.7241 0.6207]$^T$ | 6.1034 | [1 -0.6102 0.1017]$^T$                   | 0.0690        |\n| 4   | [1 -0.6102 0.1017]$^T$      | [6.2203 -3.9322 0.7119]$^T$ | 6.2203 | [1 -0.6322 0.1144]$^T$                   | 0.0254        |\n| ... | ...                             | ...                             | ...    | ...                                          | ...           |\n| 16  | [1 -0.644972 0.1219239]$^T$ | [6.2899 -4.0568 0.7669]$^T$ | 6.2899 | [1 -0.644972 0.1219241]$^T$              | 3.956E-7      |\n\n### Convergencia\n\n- Para que la convergencia est茅 garantizada, se deben cumplir las siguientes condiciones:\n\n\ta. Los autovalores de $\\mathbf A$, $\\lambda_1, \\lambda_2, \\cdots, \\lambda_n$ est谩n asociados a un conjunto de autovectores linealmente independientes^[Esta condici贸n es equivalente a decir que la matriz $\\mathbf A$ es [\"diagonalizable\"](https://es.wikipedia.org/wiki/Matriz_diagonalizable).].\n\tb. $\\mathbf A$ tiene un autovalor dominante, es decir, se verifica: $|\\lambda_1| > |\\lambda_2|  \\geq \\cdots \\geq |\\lambda_n| \\geq 0$.\n\n- Si se cumplen estas condiciones, en general el m茅todo converge con cualquier vector inicial $\\mathbf x^{(0)}$^[En realidad $\\mathbf x^{(0)}$ debe verificar una condici贸n te贸rica que se puede leer al final de la p谩gina 433, pero en la pr谩ctica no lo podemos verificar y el m茅todo sencillamente funciona.].\n\n- Si no se cumplen estas condiciones, el m茅todo puede converger o fallar.\n\n- Como en la pr谩ctica no podemos verificar el cumplimiento de las mismas, sencillamente corremos el m茅todo y observamos el resultado.\n\n- Para detener el proceso, podemos usar los mismos criterios vistos en la Unidad 3.\n\n- Con el m茅todo as铆 presentado, la convergencia ser谩 m谩s r谩pida cuanto mayor sea el valor absoluto del autovalor dominante $|\\lambda_1|$ comparado con el que le sigue, $|\\lambda_2|$. \n\n- Tambi茅n es m谩s r谩pida cuando se aplica en matrices sim茅tricas que en matrices asim茅tricas.\n\n### Otras caracter铆sticas\n\n- La divisi贸n por la coordenada de mayor valor absoluto, $\\mu^{(k)}$, produce como resultado en cada paso un vector de norma $L_{\\infty} = 1$. Si no se incluyera esta normalizaci贸n, el proceso iterativo resultar铆a igual a: \n\n\t$$\n\t\\begin{aligned}\n\t\\mathbf{x}^{(1)} &= \\mathbf{Ax}^{(0)} \\\\\n\t\\mathbf{x}^{(2)} &= \\mathbf{Ax}^{(1)} = \\mathbf{A}^2 \\mathbf{x}^{(0)}\\\\\n\t\\mathbf{x}^{(3)} &= \\mathbf{Ax}^{(2)} = \\mathbf{A}^3 \\mathbf{x}^{(0)}\\\\\n\t&\\vdots \\\\\n\t\\mathbf{x}^{(k)} &= \\mathbf{Ax}^{(k-1)} = \\mathbf{A}^k \\mathbf{x}^{(0)}\\\\\n\t&\\vdots \\\\\n\t\\end{aligned}\n\t$$\n\n- Esta sucesi贸n tambi茅n converge a un autovector dominante, pero no normalizado y no nos entrega el autovalor correspondiente, el cual puede ser calculado mediante el **cociente de Rayleigh** luego de detener el proceso: si $\\mathbf{x}$ es un autovector de $\\mathbf{A}$, entonces su correspondiente autovalor es:\n\t\n\t$$\n\t\\lambda = \\frac{(\\mathbf{Ax})^t\\mathbf{x}}{\\mathbf{x}^t\\mathbf{x}}\n\t$$\n\n- Sin embargo, las sucesivas potencias de $\\mathbf A$ tienden a terminar en errores de desbordamiento o subdesbordamiento. Por eso resulta necesaria la introducci贸n de la constante normalizadora, como se indic贸 inicialmente.\n\n### Variantes para acelerar la convergencia\n\n- Se han desarrollado modificaciones del m茅todo de potencia que logran una convergencia m谩s r谩pida y que son importantes en problemas con matrices de gran dimensi贸n.\n- En el caso de matrices generales, se pueden aplicar el *m茅todo de potencia trasladada* o el *procedimiento de Aitkens*.\n- Para matrices sim茅tricas, se puede mejorar significativamente la convergencia con algunas modificaciones en los c谩lculos, en lo que se conoce como *m茅todo de potencia sim茅trica*.\n- No nos detendremos en estas variantes.\n\n### Variantes para hallar el autovalor m谩s peque帽o\n\n- *Recordatorio*: los autovalores de $\\mathbf{A}^{-1}$ son los rec铆procos de los de $\\mathbf{A}$.\n- Si aplicamos el m茅todo a $\\mathbf{A}^{-1}$, obtenemos su autovalor dominante. \n- Y, por la observaci贸n anterior, si tomamos el rec铆proco del autovalor as铆 hallado, obtenemos el autovalor de $\\mathbf{A}$ de menor valor absoluto.\n\n\n### Variantes para hallar otros autovalores\n\n**M茅todo de potencia inversa**\n\n- Es una modificaci贸n que se usa para encontrar el autovalor de $\\mathbf A$ que est谩 m谩s cerca de un n煤mero espec铆fico que hay que establecer de antemano, $q$.\n- Esto se utiliza en aplicaciones donde $q$ es una aproximaci贸n a alg煤n autovalor que se tiene disponible y que se desea mejorar.\n- Tampoco profundizaremos en este m茅todo, pero se lo puede consultar en las p谩ginas 439-440.\n\n**T茅cnicas de deflaci贸n**\n\n- Las t茅cnicas de deflaci贸n permiten obtener los otros autovalores de la matriz, luego de haber obtenido el dominante con el m茅todo de potencia.\n- Consisten en formar una nueva matriz $\\mathbf A_2$ cuyos autovalores sean iguales a los de la matriz original $\\mathbf A$, excepto por el autovalor dominante de $\\mathbf A$, que es reemplazado por un autovalor igual a cero en $\\mathbf A_2$.\n- Entre estos algoritmos encontramos a la **deflaci贸n de Wielandt** y la **deflaci贸n de Hotelling**.\n\t\n\t- La **deflaci贸n de Wielandt** se puede utilizar de manera general para cualquier tipo de matriz. Si bien no reviste de demasiada complejidad, involucra numerosos c谩lculos y no nos detendremos en ello, pero puede ser consultada en la p谩gina 443 del libro.\n\t- La **deflaci贸n de Hotelling** se aplica para matrices sim茅tricas. Una vez hallada una aproximaci贸n para el autovalor dominante $\\lambda_1$ con un autovector asociado $\\mathbf x_1$, se debe calcular la siguiente matriz:\n\t\t\n\t\t$$\n\t\t\\mathbf{A}_2 = \\mathbf{A} - \\lambda_1 \\mathbf{u}_1 \\mathbf{u}_1^T\n\t\t$$\n\t\n\t\tdonde $\\mathbf{u}_1 = \\mathbf{x}_1 / ||\\mathbf{x}_1||_2$ (es decir, $\\mathbf{u}_1$ es el autovector asociado a $\\lambda_1$ de norma euclidiana igual a 1).\n\t\t\n\t\tLos autovalores de $\\mathbf A_2$ son $\\{0, \\lambda_2, \\cdots, \\lambda_n\\}$, de modo que al aplicar nuevamente el m茅todo de potencia sobre $\\mathbf A_2$ para hallar su autovalor dominante, encontraremos el segundo autovalor de $\\mathbf A$, $\\lambda_2$.\n\t\t\n\t\tRepitiendo este proceso se pueden encontrar los restantes autovalores (por ejemplo, $\\mathbf{A}_3 = \\mathbf{A}_{2} - \\lambda_{2} \\mathbf{u}_{2} \\mathbf{u}_{2}^T$).\n\t\n<!-- En todos lados veo que Hotelling es s贸lo para matrices sim茅tricas, pero cuando lo hago a mano en ejemplos veo que tambi茅n funciona para casos donde A no es sim茅trica. Me queda esta duda. Por otro lado, ver en mi archivo de ejemplo de R, si aplico la deflaci贸n usando los valores sacados con eigen(), funciona, pero si lo hago a partir de los resultados de la funci贸n que program茅 con el m茅todo de potencia, el tercer autovalor ya no es igual en algunos casos, por los errores -->\n\n- No obstante, se debe tener en cuenta que las t茅cnicas de deflaci贸n en general no se aplican para calcular todos los autovalores de una matriz, sino s贸lo algunos, ya que presentan un grave inconveniente ligado al deterioro de las aproximaciones de los autovalores restantes.\n- Dado que el valor obtenido en la primera etapa es una aproximaci贸n del verdadero autovalor $\\lambda_1$, los autovalores de $\\mathbf A_2$ no son exactamente los restantes autovalores de $\\mathbf A$ sino una aproximaci贸n a los mismos. \n- Al aplicar el m茅todo otra vez, se obtiene una aproximaci贸n al autovalor dominante de $\\mathbf A_2$, que es a su vez aproximado pero no igual al verdadero valor $\\lambda_2$ que buscamos.\n- Entonces, tras un cierto n煤mero de etapas de deflaci贸n, la acumulaci贸n de errores de redondeo y de truncamiento pueden deteriorar notablemente la aproximaci贸n.\n- Por esta raz贸n, si es necesario encontrar todos los autovalores de una matriz, es conveniente emplear otras t茅cnicas, como la del algoritmo QR que veremos en la siguiente secci贸n.\n\n### Importancia del m茅todo\n\n- Si hay otras t茅cnicas que hallan todos los autovalores, 驴por qu茅 nos preocupamos por el m茅todo de potencia que nos da s贸lo uno?\n\t\n\t- Porque hallar todos los autovalores en matriz de gran dimensi贸n es computacionalmente costoso.\n\t- Porque es utilizado en muchas aplicaciones donde s贸lo se necesita obtener el autovalor dominante.\n\t- Porque es eficiente cuando la matriz es dispersa (matriz de gran dimensi贸n con la gran mayor铆a de sus entradas iguales a cero).\n\n- De hecho, Google utiliza el m茅todo de potencia en su algoritmo *PageRank* para buscar rankear los resultados de b煤squedas de p谩ginas web, desarrollado en Stanford University en 1996 por Larry Page y Sergey Brin. El exito de este algoritmo deriv贸 en la creaci贸n de esta mega empresa que empez贸 siendo s贸lo un motor de b煤squeda (pueden buscar en Wikipedia o leer el art铆culo [The \\$25.000.000.000 eigenvector: the linear algebra behind Google](https://www.math.arizona.edu/~glickenstein/math443f12/bryanleise.pdf)).\n- Twitter tambi茅n lo usa para generar las recomendaciones acerca de a qui茅n seguir.\n\n## El algoritmo QR\n\n- En esta secci贸n consideramos el **algoritmo QR**, una t茅cnica que se utiliza para determinar en forma sistem谩tica todos los autovalores de una matriz cuadrada.\n- Primero vamos a ver de qu茅 se trata la **factorizaci贸n QR** y luego veremos el **algoritmo QR** para hallar los autovalores.\n\n### Factorizaci贸n QR\n\n- Ya hemos mencionado un tipo especial de factorizaci贸n de matrices, la **LU**. \n- Ahora vamos a ver otra factorizaci贸n, que tambi茅n tiene numerosas aplicaciones:\n\n\t- Resolver sistemas de ecuaciones lineales.\n\t- Calcular determinantes e inversas.\n\t- Encontrar otras factorizacones (como la de Cholesky y la de Schur).\n\t- Otras.\n\n::: {.alert .alert-info}\n**Teorema:**: \n\n- Toda matriz real cuadrada no singular $\\mathbf A$ de dimensi贸n $n \\times n$ puede factorizarse en la forma $\\mathbf A = \\mathbf{QR}$, donde $\\mathbf Q$ es una matriz ortogonal $n \\times n$ y $\\mathbf R$ es una matriz triangular superior $n \\times n$. La factorizaci贸n es 煤nica si se pide que los elementos diagonales de $\\mathbf R$ sean positivos.\n\n- Toda matriz real rectangular $\\mathbf A$ de dimensi贸n $m \\times n$ ($m > n$), puede factorizarse en la forma $\\mathbf A = \\mathbf{QR}$, donde $\\mathbf Q$ es una matriz ortogonal $m \\times m$ y $\\mathbf R$ es una matriz triangular superior $m \\times n$, en la cual sus 煤ltimas $m-n$ filas son todos ceros. Dado que las 煤ltimas filas son nulas, las 煤ltimas columnas de $\\mathbf Q$ no aportan al producto $\\mathbf Q\\mathbf R$ y por lo tanto otras definiciones y algunos algoritmos presentan a $\\mathbf Q$ como una matriz $m \\times n$ con columnas ortonormales y $\\mathbf R$ como una matriz triangular $n \\times n$.\n:::\n\n- Recordamos que una matriz ortogonal es una matriz cuadrada cuya matriz inversa coincide con su matriz traspuesta: $\\mathbf Q^T = \\mathbf Q^{-1}$. Sus columnas son vectores ortogonales de norma 1.\n\n<!-- Recordatorio para m铆: Una matriz ortogonal es necesariamente unitaria. Ortogonal es el equivalente a unitaria en los reales. Unitaria es cuando la traspuesta conjugada (con complejos) es igual a la inversa. -->\n\n- Para obtener $\\mathbf Q$ se puede aplicar el proceso de Gram-Schmidt a las columnas de $\\mathbf A$ (las columnas de $\\mathbf Q$ son las de $\\mathbf A$ luego de la ortonormalizaci贸n).\n- Una vez obtenida $\\mathbf Q$, $\\mathbf R$ se puede obtener como $\\mathbf R = \\mathbf Q^T \\mathbf A$.\n- Hay otros m茅todos que tambi茅n permiten hacer esto, pero en este curso no nos vamos a preocupar por el c谩lculo de la factorizaci贸n y directamente emplearemos la funci贸n de R con la que se obtiene.\n- Ejemplo con una matriz cuadrada. Sea:\n\n\t$$\n\t\\mathbf A =\n\t\\begin{bmatrix}\n\t1 & -2 & 1 \\\\\n\t-1 & 3 & 2 \\\\\n\t1 & -1 & -4 \n\t\\end{bmatrix}\n\t$$\n\n\tSu factorizaci贸n QR es:\n\n\t$$\n\t\\mathbf Q =\n\t\\begin{bmatrix}\n\t-\\frac{1}{\\sqrt 3} & 0                  & -\\frac{2}{\\sqrt 6} \\\\\n\t\\frac{1}{\\sqrt 3} & -\\frac{1}{\\sqrt 2}  & -\\frac{1}{\\sqrt 6} \\\\\n\t-\\frac{1}{\\sqrt 3} & -\\frac{1}{\\sqrt 2} & \\frac{1}{\\sqrt 6} \n\t\\end{bmatrix}\n\t\\qquad\n\t\\mathbf R =\n\t\t\\begin{bmatrix}\n\t-\\sqrt 3 & 2\\sqrt 3  & \\frac{5\\sqrt 3}{3} \\\\\n\t0        & -\\sqrt 2  &  \\sqrt 2 \\\\\n\t0        & 0         & -\\frac{\\sqrt{96}}{3} \n\t\\end{bmatrix}\n\t$$\n\n\tde modo que se verifica: $\\mathbf A = \\mathbf{QR}$.\n\t\n- Lo comprobamos en Python. \n- **Ejemplo con una matriz cuadrada.**\n\t\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom scipy.linalg import qr\n\nA = np.array([[1, -2, 1],\n              [-1, 3, 2],\n              [1, -1, -4]])\n\nQ, R = qr(A)\n\nprint(\"Matriz Q (ortogonal):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatriz Q (ortogonal):\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(Q)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[-0.5774  0.     -0.8165]\n [ 0.5774 -0.7071 -0.4082]\n [-0.5774 -0.7071  0.4082]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nMatriz R (triangular superior):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatriz R (triangular superior):\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(R)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[-1.7321  3.4641  2.8868]\n [ 0.     -1.4142  1.4142]\n [ 0.      0.     -3.266 ]]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Verificamos que Q es ortogonal\nQ_traspuesta = np.transpose(Q)\nQ_inversa = np.linalg.inv(Q)\nprint(\"\\n驴Q es ortogonal? (t(Q) == inv(Q)):\", np.allclose(Q_traspuesta, Q_inversa))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n驴Q es ortogonal? (t(Q) == inv(Q)): True\n```\n\n\n:::\n\n```{.python .cell-code}\n# Verificamos A = QR\nresultado = np.dot(Q, R)\nprint(resultado)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 1. -2.  1.]\n [-1.  3.  2.]\n [ 1. -1. -4.]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\n驴A = QR?\", np.allclose(A, resultado))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n驴A = QR? True\n```\n\n\n:::\n:::\n\n\n\n- **Ejemplo con una matriz rectangular:**\n\t\n\n\n::: {.cell}\n\n```{.python .cell-code}\nA = np.array([[1, -2],\n              [-1, 3],\n              [1, -1]])\n              \n# Forma 1\nQ, R = qr(A)\nprint(\"Matriz Q (ortogonal):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatriz Q (ortogonal):\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(Q)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[-0.5774  0.     -0.8165]\n [ 0.5774 -0.7071 -0.4082]\n [-0.5774 -0.7071  0.4082]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nMatriz R (triangular superior):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatriz R (triangular superior):\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(R)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[-1.7321  3.4641]\n [ 0.     -1.4142]\n [ 0.      0.    ]]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Verificamos que Q es ortogonal\nQ_traspuesta = np.transpose(Q)\nQ_inversa = np.linalg.inv(Q)\nprint(\"\\n驴Q es ortogonal? (t(Q) == inv(Q)):\", np.allclose(Q_traspuesta, Q_inversa))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n驴Q es ortogonal? (t(Q) == inv(Q)): True\n```\n\n\n:::\n\n```{.python .cell-code}\n# Verificamos A = QR\nresultado = np.dot(Q, R)\nprint(resultado)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 1. -2.]\n [-1.  3.]\n [ 1. -1.]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\n驴A = QR?\", np.allclose(A, resultado))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n驴A = QR? True\n```\n\n\n:::\n\n```{.python .cell-code}\n# Forma 2 (omite filas nulas de R)\nQ, R = qr(A, mode=\"economic\")\nprint(\"Matriz Q (ortogonal):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatriz Q (ortogonal):\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(Q)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[-0.5774  0.    ]\n [ 0.5774 -0.7071]\n [-0.5774 -0.7071]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nMatriz R (triangular superior):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatriz R (triangular superior):\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(R)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[-1.7321  3.4641]\n [ 0.     -1.4142]]\n```\n\n\n:::\n:::\n\n\n\n### El algoritmo QR\n\n- Ahora estamos en condiciones de usar la factorizaci贸n QR para obtener todos los autovalores de una matriz cuadrada $n \\times n$, $\\mathbf A$.\n- Es un algoritmo tan sencillo, que sorprende que sea tan efectivo.\n- Primero se toma $\\mathbf A = \\mathbf A^{(0)}$ como matriz inicial.\n- Luego, para cada $k = 1, 2, \\cdots$:\n\n\t1. Realizar la factorizaci贸n QR de $\\mathbf A^{(k-1)}$ para obtener $\\mathbf Q^{(k-1)}$ y $\\mathbf R^{(k-1)}$ (es decir: $\\mathbf A^{(k-1)}=\\mathbf Q^{(k-1)}\\mathbf R^{(k-1)}$).\n\t2. Calcular la siguiente matriz del proceso iterativo como: $\\mathbf A^{(k)} = \\mathbf R^{(k-1)}\\mathbf Q^{(k-1)}$\n\t\n- La sucesi贸n $\\mathbf A^{(k)}$ converge a una matriz triangular cuyos elementos diagonales son los autovalores de $\\mathbf A$.\n\n- La idea detr谩s de este m茅todo es la siguiente: las sucesivas matrices $\\mathbf A^{(k)}$ son semejantes (revisar secci贸n de propiedades) y, por lo tanto, tienen los mismos autovalores. Adem谩s, estas operaciones van transformando de a poco a las matrices $\\mathbf A^{(k)}$ en triangulares superiores y sabemos que en tales matrices los autovalores son los elementos diagonales (repasar las propiedades enunciadas al inicio del apunte).\n\n- Para darnos cuenta de que las matrices  $\\mathbf A^{(k)}$ son semejantes debemos notar:\n\n$$\n\\mathbf A^{(k)} = \\mathbf R^{(k-1)}\\mathbf Q^{(k-1)} = \n\\underbrace{{\\mathbf Q^{(k-1)}}^{-1} \\mathbf Q^{(k-1)}}_{\\mathbf I}\\mathbf R^{(k-1)}\\mathbf Q^{(k-1)} =\n{\\mathbf Q^{(k-1)}}^{-1}  \\mathbf A^{(k-1)}\\mathbf Q^{(k-1)} \n麓\\implies A^{(k)} \\text{ y } A^{(k-1)} \\text{ son semejantes}\n$$\n\n- 驴Y los autovectores?\n\n\t- Si la matriz es sim茅trica, los autovectores son las columnas de $\\prod_{k=0} \\mathbf Q^{(k)}$.\n\t- Si la matriz no es sim茅trica, esta forma presentada del algoritmo, que es la m谩s sencilla posible y por eso a veces es llamado \"el algoritmo QR puro\" no entrega los autovectores, pero hay otras variantes que s铆 lo hacen.\n\n<!-- Esto tomado de:  -->\n<!-- https://stats.stackexchange.com/questions/20643/finding-matrix-eigenvectors-using-qr-decomposition \nhttps://www.youtube.com/watch?v=1kw8bpA9QmQ\n-->\n\n- El proceso iterativo debe detenerse cuando se haya llegado a una matriz triangular superior (las entradas del tri谩ngulo inferior sin la diagonal deber铆an ser cero o muy cercanas). Para implementar un criterio m谩s sencillo, podemos detenernos cuando la distancia entre los vectores formados por los elementos diagonales de la matriz sea tan peque帽a como se desee.\n\t\n<!-- One may prove convergence to an upper triangular matrix, if |位||位| for all eigenvalues 位,位 of . \nhttps://math.stackexchange.com/questions/3464295/what-is-explicit-and-implicit-qr-algorithms-for-symmetric-and-non-symmetric-matr\n-->\n\n- Ejemplo:\n\n\t$$\n\t\\mathbf A =\n\t\\begin{bmatrix}\n\t1 & -2 & 1 \\\\\n\t-1 & 3 & 2 \\\\\n\t1 & -1 & -4 \n\t\\end{bmatrix}\n\t$$\n\n\tLlamamos a esta matriz con $\\mathbf A^{(0)}$ y ya vimos que su factorizaci贸n QR es:\n\n\t$$\n\t\\mathbf Q^{(0)} =\n\t\\begin{bmatrix}\n\t-\\frac{1}{\\sqrt 3} & 0                  & -\\frac{2}{\\sqrt 6} \\\\\n\t\\frac{1}{\\sqrt 3} & -\\frac{1}{\\sqrt 2}  & -\\frac{1}{\\sqrt 6} \\\\\n\t-\\frac{1}{\\sqrt 3} & -\\frac{1}{\\sqrt 2} & \\frac{1}{\\sqrt 6} \n\t\\end{bmatrix}\n\t\\qquad\n\t\\mathbf R^{(0)} =\n\t\t\\begin{bmatrix}\n\t-\\sqrt 3 & 2\\sqrt 3  & \\frac{5\\sqrt 3}{3} \\\\\n\t0        & -\\sqrt 2  &  \\sqrt 2 \\\\\n\t0        & 0         & -\\frac{\\sqrt{96}}{3} \n\t\\end{bmatrix}\n\t$$\n\n\tCon lo cual, la siguiente matriz de la sucesi贸n es: \n\t\n\t$$\n\t\\mathbf A^{(1)} = \\mathbf R^{(0)}\\mathbf Q^{(0)}=\n\t\\begin{bmatrix}\n\t-\\frac{4}{3} & -\\frac{11}{6} \\sqrt 6                  & -\\frac{5}{6}\\sqrt2 \\\\\n\t-\\frac{2}{3}\\sqrt 6 & 0  & \\frac{2}{3} \\sqrt 3 \\\\\n\t\\frac{4}{3} \\sqrt 2 & \\frac{4}{3} \\sqrt 3 & -\\frac{4}{3} \n\t\\end{bmatrix}\n\t$$\n\n\tVerificamos en Python este y los siguientes pasos:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nA = np.array([[1, -2, 1],\n              [-1, 3, 2],\n              [1, -1, -4]])\n\n# Iteraci贸n 1\nQ0, R0 = qr(A)\nA1 = np.dot(R0, Q0)\nprint(A1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 1.3333 -4.4907  1.1785]\n [-1.633  -0.      1.1547]\n [ 1.8856  2.3094 -1.3333]]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Iteraci贸n 2\nQ1, R1 = qr(A1)\nA2 = np.dot(R1, Q1)\nprint(A2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 1.      2.8772  0.2349]\n [ 4.0856 -1.2914  3.1605]\n [-0.3759  0.3028  0.2914]]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Iteraci贸n 3\nQ2, R2 = qr(A2)\nA3 = np.dot(R2, Q2)\nprint(A3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 0.1495 -4.4805 -2.7609]\n [-3.0529 -0.7539 -0.1445]\n [ 0.0542  0.0489  0.6044]]\n```\n\n\n:::\n:::\n\n\n\n- Si seguimos iterando vamos a ver que la matriz converge y en su diagonal tendremos a los autovalores.\n- Usando la funci贸n provista que implementa este algoritmo vemos el resultado:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nrtdo = algoritmo_qr(A)\n[print(keys, \"\\n\", value) for keys, value in rtdo.items()]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nconvergencia \n True\niteraciones \n 115\nautovalores \n [-4.      3.4142  0.5858]\npasos \n        i                                      autovalores_i       error_i\n0      0                                         [1, 3, -4]           NaN\n1      1  [1.3333333333333333, -1.300816253288535e-15, -...  4.027682e+00\n2      2  [0.999999999999999, -1.2913907284768211, 0.291...  2.102030e+00\n3      3  [0.14953271028037157, -0.7539198862276028, 0.6...  1.053630e+00\n4      4  [-0.3840000000000031, -0.19584605115074222, 0....  7.724674e-01\n..   ...                                                ...           ...\n111  111  [-3.999999941718327, 3.4142135040914274, 0.585...  1.789869e-07\n112  112  [-4.000000049746487, 3.4142136121195867, 0.585...  1.527749e-07\n113  113  [-3.9999999575386904, 3.4142135199117907, 0.58...  1.304015e-07\n114  114  [-4.000000036242971, 3.4142135986160715, 0.585...  1.113047e-07\n115  115  [-3.9999999690646675, 3.4142135314377686, 0.58...  9.500447e-08\n\n[116 rows x 3 columns]\n[None, None, None, None]\n```\n\n\n:::\n:::\n\n\n\n- Lo comparamos con el resultado de la funci贸n `eig()` de Python:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nautovalores, autovectores = eig(A)\nprint(\"Autovalores:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAutovalores:\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(autovalores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[-4.    +0.j  0.5858+0.j  3.4142+0.j]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nAutovectores:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nAutovectores:\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(autovectores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 0.3015  0.9511 -0.6715]\n [ 0.3015  0.2711  0.7169]\n [-0.9045  0.1483 -0.1873]]\n```\n\n\n:::\n:::\n\n\n\n- Al algoritmo QR \"puro\" definido en esta secci贸n tambi茅n se lo conoce como \"impr谩ctico\" porque tiene algunas desventajas:\n\n\t- La factorizaci贸n QR en cada paso es costosa computacionalmente.\n\t- La convergencia de las entradas subdiagonales a cero es lineal (convergencia lenta).\n\t\n- Por eso se han propuesto algunas modificaciones que mejoran notablemente el desempe帽o del m茅todo:\n\n\t- En primer lugar, se debe  transformar a la matriz original $\\mathbf A$ en otra similar (mismos autovalores) pero que sea [tridiagonal](https://es.wikipedia.org/wiki/Matriz_tridiagonal) (se logra con el m茅todo de Householder) o que sea una [matriz de Hessenberg](https://es.wikipedia.org/wiki/Matriz_de_Hessenberg).\n\t- Luego, en el proceso iteratvio, se debe usar un procedimiento de deflaci贸n cada vez que un elemento subdiagonal se hace 0 para disminuir la cantidad de c谩lculos.\n\t- Y tambi茅n se debe implementar una estrategia de cambios de filas y columnas (*shifted QR*) que acelera la convergencia.\n\t\n- En este curso, no veremos estas variantes (est谩n en el libro, que de hecho no presenta la forma simple que vimos ac谩).\n\n## Descomposici贸n en valores singulares (DVS)\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nfrom matplotlib.image import imread\nimport matplotlib.pyplot as plt\n```\n:::\n\n\n\n- Una matriz rectangular $\\mathbf A$ no puede tener un autovalor porque $\\mathbf {Ax}$ y $\\mathbf x$ son vectores de diferentes tama帽os.\n- Sin embargo, existen n煤meros que desempe帽an un rol an谩logo al de los autovalores para las matrices no cuadradas.\n- Se trata de los **valores singulares** de una matriz.\n\n- La **Descomposici贸n en Valores Singulares** (DVS, tambi茅n llamada *SVD*, por las siglas de *Singular Value Decomposition*) es una factorizaci贸n para matrices rectangulares que tiene numerosas aplicaciones, por ejemplo en compresi贸n de im谩genes y an谩lisis de se帽ales.\n- En Estad铆stica tiene gran importancia para tareas relacionadas con la reducci贸n de dimensionalidad de grandes conjuntos de datos (tiene una vinculaci贸n directa con el An谩lisis de Componentes Principales, t茅cnica que estudiar谩n en An谩lisis de Datos Multivariados). Tambi茅n se la puede utilizar para realizar ajustes por M铆nimos Cuadrados.\n\n::: {.alert .alert-info}\n**Teorema de Descomposici贸n en Valores Singulares**: una matriz rectangular $\\mathbf A$ de dimensi贸n $m \\times n$ puede ser factorizada como:\n\n$$\n\\mathbf A = \\mathbf U \\mathbf S \\mathbf V^T\n$$\n\ndonde:\n\n- $\\mathbf U$ es una matriz ortogonal $m \\times m$\n- $\\mathbf S$ es una matriz diagonal $m \\times n$ con elementos $\\sigma_i$ ($\\mathbf s_{ij} = 0 \\,\\forall i \\neq j$).\n- $\\mathbf V$ es una matriz ortogonal $n \\times n$\n\nAdem谩s:\n\n- Los elementos diagonales de $\\mathbf S$, $\\sigma_i$, son llamados **valores singulares** de $\\mathbf A$. Son tales que $\\sigma_1 \\geq \\sigma_2 \\geq \\cdots \\sigma_k \\geq 0$, con $k=min\\{m,n\\}$ y son iguales a las ra铆ces cuadradas positivas de los autovalores no nulos de $\\mathbf A^T\\mathbf A$.\n- Las columnas de $\\mathbf V$ son los autovectores ortonormales de $\\mathbf A^T\\mathbf A$ y se llaman *vectores singulares derechos* porque $\\mathbf {AV} = \\mathbf U \\mathbf S$.\n- Las columnas de $\\mathbf U$ son los autovectores ortonormales de $\\mathbf A\\mathbf A^T$ y se llaman *vectores singulares izquierdos* porque $\\mathbf {U}^T\\mathbf {A} =  \\mathbf S \\mathbf V^T$.\n:::\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Plots/U5/svd1.png){width=381}\n:::\n:::\n\n\n\n- Esas tres 煤ltimas observaciones proporcionan una forma de obtener la DVS.\n\n### Ejemplo\n\n- Vamos a buscar la DVS de la siguiente matriz haciendo los c谩lculos en Python:\n\n\n<!-- este ejemplo lo descarto porque por el ordenamiento y signo no se puede reproducir con UxSxVT -->\n<!-- \t$$ -->\n<!-- \t\\mathbf A = -->\n<!-- \t\\begin{bmatrix} -->\n<!-- \t1 & 0 & 1 \\\\ -->\n<!-- \t0 & 1 & 0 \\\\ -->\n<!-- \t1 & 1 & 0  \\\\ -->\n<!-- \t0 & 1 & 0 \\\\ -->\n<!-- \t0 & 1 & 1  -->\n<!-- \t\\end{bmatrix} -->\n<!-- \t$$ -->\n\t\n$$\n\\mathbf A =\n\\begin{bmatrix}\n4 & 2 & 0 \\\\\n1 & 5 & 6 \n\\end{bmatrix}\n$$\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nA = np.array([[4,2,0],\n              [1,5,6]])\n```\n:::\n\n\n\n- Para generar la matriz diagonal $\\mathbf S$, buscamos los valores singulares que son las ra铆ces positivas de los autovalores no nulos de $\\mathbf A^T\\mathbf A$.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nATA = A.T @ A\nprint(ATA)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[17 13  6]\n [13 29 30]\n [ 6 30 36]]\n```\n\n\n:::\n\n```{.python .cell-code}\nautovalores, autovectores = np.linalg.eig(ATA)\n\nval_sing = np.sqrt(autovalores)\nprint(val_sing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[8.1387 3.97   0.    ]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Ponemos a los valores singulares en la matriz S\nS = np.zeros((A.shape[0], A.shape[1]))\nfor i in range(min(A.shape)):\n    S[i, i] = val_sing[i]\nprint(S)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[8.1387 0.     0.    ]\n [0.     3.97   0.    ]]\n```\n\n\n:::\n:::\n\n\n\n- Las columnas de $\\mathbf V$ son los autovectores ortonormales de $\\mathbf A^T\\mathbf A$:\n\t\n\n\n::: {.cell}\n\n```{.python .cell-code}\nV = autovectores\nprint(V.T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[-0.26   -0.6592 -0.7056]\n [-0.8913 -0.1172  0.438 ]\n [ 0.3714 -0.7428  0.5571]]\n```\n\n\n:::\n:::\n\n\n\n- Las columnas de $\\mathbf U$ son los autovectores ortonormales de $\\mathbf A\\mathbf A^T$:\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nAAT = A @ A.T\nautovalores, autovectores = np.linalg.eig(AAT)\nprint(autovalores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[15.7611 66.2389]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Los necesitamos ordenados de mayor a menor\nindices = np.argsort(autovalores)[::-1]\nautovalores = autovalores[indices]\nprint(autovalores)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[66.2389 15.7611]\n```\n\n\n:::\n\n```{.python .cell-code}\nU = autovectores[:, indices] # reordenamos de la misma forma los autovectores\nprint(U)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[-0.2898 -0.9571]\n [-0.9571  0.2898]]\n```\n\n\n:::\n:::\n\n\n\n- Con los resultados obtenidos, podemos verificar que $\\mathbf A = \\mathbf U \\mathbf S \\mathbf V^T$:\n\t\n\n\n::: {.cell}\n\n```{.python .cell-code}\nprint(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[4 2 0]\n [1 5 6]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(U @ S @ V.T)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 4.  2. -0.]\n [ 1.  5.  6.]]\n```\n\n\n:::\n:::\n\n\n\n- Esta no es la forma m谩s eficiente ni robusta de obtener la descomposici贸n. \n- Es sensible al ordenamiento de los autovalores y a los signos de los autovectores de $\\mathbf A^T\\mathbf A$: y de $\\mathbf A\\mathbf A^T$, que se obtienen de forma independiente entre s铆.\n- Por eso, en debemos utilizar programas creados espec铆ficametne para este fin.\n- Python tiene una funci贸n que se encarga de aplicar esto: `np.linalg.svd()`.\n- Debemos notar que devuelve directamente la transpuesta de $V$.\n\n<!-- , sin embargo, que como en este ejemplo la matriz es de $5 \\times 3$, R no reporta las 煤ltimas dos columnas de $\\mathbf U$. Las mismas no son 煤tiles porque en la multiplicaci贸n matricial sus elementos se multiplican con los de las 煤ltimas dos filas de $\\mathbf S$ que son nulas: -->\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nU, val_sing, VT = np.linalg.svd(A, full_matrices=True)\n\nprint(\"Matriz U:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMatriz U:\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(U)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 0.2898  0.9571]\n [ 0.9571 -0.2898]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nValores singulares:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nValores singulares:\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(val_sing)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[8.1387 3.97  ]\n```\n\n\n:::\n\n```{.python .cell-code}\nS = np.zeros((A.shape[0], A.shape[1]))\nfor i in range(min(A.shape)):\n    S[i, i] = val_sing[i]\nprint(\"\\nMatriz S (valores singulares):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatriz S (valores singulares):\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(S)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[8.1387 0.     0.    ]\n [0.     3.97   0.    ]]\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(\"\\nMatriz VT (transpuesta de V):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nMatriz VT (transpuesta de V):\n```\n\n\n:::\n\n```{.python .cell-code}\nprint(VT)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[[ 0.26    0.6592  0.7056]\n [ 0.8913  0.1172 -0.438 ]\n [ 0.3714 -0.7428  0.5571]]\n```\n\n\n:::\n\n```{.python .cell-code}\n# Comprobamos que se reconstruye la matriz A\nU @ S @ VT\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[ 4.,  2., -0.],\n       [ 1.,  5.,  6.]])\n```\n\n\n:::\n:::\n\n\n\n### Aplicaciones\n\n- La raz贸n de la importancia de la DVS en muchas aplicaciones es que nos permite captar las caracter铆sticas m谩s importantes de una matriz $m \\times n$ (en muchos casos, con $m$ mucho mayor que $n$) usando una matriz que, a menudo, es de tama帽o significativamente m谩s peque帽o.\n\n- El hecho de que los valores singulares est谩n en la diagonal de $\\mathbf S$ en orden decreciente implica que al hacer el producto $\\mathbf U \\mathbf S \\mathbf V^T$ para reconstruir a $\\mathbf A$, quienes aportan la mayor parte de la informaci贸n son las primeras columnas de cada una de estas matrices.\n\n- Entonces para reconstruir $\\mathbf A$ de manera exacta necesitamos estas tres matrices completas, pero para construir una muy buena aproximaci贸n a  $\\mathbf A$ nos alcanza con hacer el mismo producto usando s贸lo sus primeras $k$ columnas:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](Plots/U5/svd2.png){width=408}\n:::\n:::\n\n\n\n- 隆Esto es un resultado impresionante! Significa que a un gran conjunto de datos lo podemos almacenar con mucho menos espacio mediante esas matrices reducidas, con muy poca p茅rdida de informaci贸n. \n- No hay una forma anticipada de saber con cu谩ntos valores singulares ($k$) alcanza para tener una buena aproximaci贸n, eso depende de cada caso^[Para cuando vean An谩lisis de Componentes Principales, esto equivale a tener que elegir el n煤mero de componentes a utilizar.].\n- La matriz $\\mathbf A$ de dimensi贸n $m \\times n$ requiere $mn$ registros para su almacenamiento.\n- Sin embargo, la matriz $\\mathbf A_k$, que aproxima a $\\mathbf A$ y tambi茅n es dimensi贸n $m \\times n$, s贸lo requiere de $k(m+n+1)$ registros para su almacenamiento ($mk$ para $\\mathbf U_k$, $k$ para $\\mathbf S_k$ y $nk$ para $\\mathbf V_k$).\n- Hacer las cuentas para ver cu谩nto se gana de \"espacio\" si $m=100$, $n=10$ y $k=4$...\n- Esto se conoce como **compresi贸n de datos** y de aqu铆 que la DVS est谩 tan relacionada con el An谩lisis de Componentes Principales, una t茅cnica de reducci贸n de la dimensionalidad.\n\n- Para ponernos un poco m谩s rigurosos, vale comentar que la matriz $\\mathbf A_k = \\mathbf U_k \\mathbf S_k \\mathbf V_k^T$ es de rango $k$ y se demuestra que es la mejor aproximaci贸n mediante una matriz de rango $k < n$ de la matriz de datos $\\mathbf A$ (posiblemente de rango $n$), en el sentido que es la que minimiza el error cuadr谩tico de la predicci贸n^[Para m谩s detalles se puede consultar el libro sobre An谩lisis Multivariado de Pe帽a, p谩gina 168.].\n\n- Para finalizar vamos a ver un ejemplo de DVS aplicado al procesamiento de im谩genes.\n\n- 驴Qu茅 tienen que ver las im谩genes con nuestros conocimientos de matrices? Toda imagen digital se representa en la computadora como una matriz de [p铆xeles](https://es.wikipedia.org/wiki/P%C3%ADxel), es decir, como un gran conjunto de puntitos ordenados en forma de matriz con filas y columnas, cada uno de un color en particular, que visualizados juntos dan lugar a la figura. Por lo tanto, una imagen se puede representar por una matriz donde cada celda tiene informaci贸n acerca del color del p铆xel correspondiente:\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Plots/U5/pixeles.png){fig-align='center' width=60%}\n:::\n:::\n\n\n\n- Existen c贸digos para representar a los distintos colores, por ejemplo, en el sistema hexadecimal, el c贸digo para el rojo es *FF0000*. Entonces, en la matriz que representa a una imagen digital est谩 el valor *FF0000* por cada p铆xel rojo que la misma tenga. En ese caso, la matriz es de tipo caracter. Hay otros tipos de representaci贸n de colores que usan arreglos tridimensionales num茅ricos para indicar *cu谩nto* de rojo, de az煤l y de verde tiene un p铆xel, ya que combinando esos tres se pueden formar el resto de los colores.\n\n- Cuando se trabaja con im谩genes en escala de grises, la cuesti贸n es m谩s sencilla. En cada celda de la matriz hay un n煤mero que var铆a entre 0 y 1. Una celda con un valor de 0 indica un p铆xel negro, mientras que una celda con un valor de 1 indica un pixel blanco. Es decir, un valor cercano a 0 es un gris bien oscuro, mientras que un valor cercano a 1 es un gris bien clarito. Por ejemplo:\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](Plots/U5/representa.png){fig-align='center' width=40%}\n:::\n:::\n\n\n\n- Con el siguiente c贸digo leemos la imagen del Monumento Nacional a la Bandera mostrada anteriormente y la convertimos en escala de grises:\n\n  1. Cargar una imagen desde el archivo \"monu.png\" utilizando la funci贸n imread y representarla con la matriz `A`.\n  \n  2. Convertirla a escala de grises. Para esto, se calcular la media a lo largo del 煤ltimo eje de la matriz A utilizando la funci贸n `np.mean` de NumPy con el argumento -1. Esto es equivalente a tomar el promedio a trav茅s de los canales de color en una imagen. Cuando se trabaja con im谩genes, el 煤ltimo eje suele representar los canales de color (por ejemplo, Rojo, Verde, Azul en una imagen RGB). Al tomar el promedio a lo largo del 煤ltimo eje, se obtiene una imagen en escala de grises en la que cada p铆xel representa el valor promedio de los canales de color en el p铆xel correspondiente. Por lo tanto, `A` queda como la matriz de valores entre 0 y 1 que que representa a la imagen en escala de grises.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Lectura de la imagen\nA = imread(\"Plots/U5/monu.png\")\n\n# Convertir a grises\nA = np.mean(A, -1) \n\n# Explorarla un poco\nnp.max(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.97647065\n```\n\n\n:::\n\n```{.python .cell-code}\nnp.min(A)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n0.023529412\n```\n\n\n:::\n\n```{.python .cell-code}\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\narray([[0.1085, 0.1085, 0.1085, ..., 0.1203, 0.1203, 0.119 ],\n       [0.1124, 0.1124, 0.1085, ..., 0.1203, 0.1203, 0.119 ],\n       [0.1163, 0.1085, 0.1085, ..., 0.1203, 0.1203, 0.119 ],\n       ...,\n       [0.102 , 0.1137, 0.1242, ..., 0.4314, 0.4235, 0.4052],\n       [0.1033, 0.1124, 0.132 , ..., 0.4235, 0.4157, 0.4   ],\n       [0.1072, 0.1111, 0.1373, ..., 0.4549, 0.4353, 0.4196]], dtype=float32)\n```\n\n\n:::\n\n```{.python .cell-code}\n# Graficarla\nplt.imshow(A, cmap='gray')\nplt.axis('off')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(-0.5, 519.5, 768.5, -0.5)\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](05_autovalores_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\n\n- Siendo la imagen de dimensi贸n $m=769 \\times n=560$, se requiere de $769 \\times 520 = 399880$ valores para su registro.\n- 驴Ser谩 posible aplicarle una DVS para poder almacenarla con muchos menos valores, pero elegidos de forma tal que los mismos sirvan para reconstruir una buena aproximaci贸n de la imagen? Probemos...\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Aplicar DVS\nU, val_sing, VT = np.linalg.svd(A, full_matrices = False)\nS = np.diag(val_sing)\n\n# Reconstruir la imagen de manera exacta (salvo errores de redondeo)\nA2 = U @ S @ VT\nplt.imshow(A2, cmap='gray')\nplt.axis('off')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(-0.5, 519.5, 768.5, -0.5)\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](05_autovalores_files/figure-html/unnamed-chunk-24-3.png){width=100%}\n:::\n:::\n\n\n\n<!-- - Nota: con `full_matrices = False`, devuelve $U$ con menos columnas, las que realmente se usan (tantos como valores singulares no nulos haya).  -->\n\n- Ahora vamos qu茅 sucede si empleamos $k=20$ valores singulares. En lugar de necesitar $399880$ valores para almacenar la imagen, esto nos permitir谩 emplear s贸lo $k(m+n+1) = 20(520+769+1)=25800$ (un 6.45% del original).\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Reconstruir la imagen usando menos informaci贸n\nk = 20\nimg = U[:, :k] @ S[:k, :k] @ VT[:k, :]\nplt.imshow(img, cmap='gray')\nplt.axis('off')\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(-0.5, 519.5, 768.5, -0.5)\n```\n\n\n:::\n\n```{.python .cell-code}\nplt.show()\n```\n\n::: {.cell-output-display}\n![](05_autovalores_files/figure-html/unnamed-chunk-25-5.png){width=100%}\n:::\n:::\n\n\n\n- Vemos que con una cantidad de registros que representa tan s贸lo un 6.45% de la cantidad original, se logra reconstruir una imagen que conserva todos los rasgos principales.\n- A continuaci贸n se presenta el resultado empleando distintos valores de $k$. Calcular en cada caso cu谩ntos registros se necesitan.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\nvalores_k = [2, 5, 10, 50, 100, 200]\n\nfor i in range(len(valores_k)):\n\n    # Establecer k\n    k = valores_k[i]\n  \n    # Calcular la aproximaci贸n de la imagen\n    img = U[:, :k] @ S[:k, :k] @ VT[:k, :]\n    \n    # Mostrar la aproximaci贸n de la imagen en escala de grises\n    plt.subplot(3, 2, i+1)\n    plt.imshow(img, cmap='gray')\n    plt.axis('off')\n    plt.title(f'k = {k}')\n\nplt.show()\n```\n\n::: {.cell-output-display}\n![](05_autovalores_files/figure-html/unnamed-chunk-26-7.png){width=100%}\n:::\n:::\n\n\n\n\n- Para determinar un buen valor de $k$ es 煤til graficar los valores singulares en orden decreciente. Se puede elegir el valor para el cual se forma una especie de \"codo\", a partir del cual los valores singulares son similares entre s铆 y tienen un valor peque帽o con respecto a los primeros. En el caso de la imagen del monumento, parece que $k=20$ estar铆a bien.\n\n\n\n::: {.cell}\n\n```{.python .cell-code}\n# Trazar los valores singulares en funci贸n de k\nplt.plot(range(520) , val_sing, marker='o', color='b')\nplt.xlabel(\"k\")\nplt.ylabel(\"Valores singulares\")\nplt.grid(True)\nplt.show()\n```\n\n::: {.cell-output-display}\n![](05_autovalores_files/figure-html/unnamed-chunk-27-9.png){width=672}\n:::\n:::\n\n\n\n- En otras aplicaciones, aplicar una DVS a una imagen puede ser necesario para poder borrar \"ruido\". Por ejemplo, si se trata de una fotograf铆a que tal vez se tom贸 a gran distancia, como una imagen satelital, es probable que la misma incluya ruido, es decir, datos que no representan verdaderamente la imagen, sino el deterioro de 茅sta mediante part铆culas atmosf茅ricas, la calidad de las lentes, procesos de reproducci贸n, etc. Los datos de ruido se incorporan en los datos de la matriz $\\mathbf A$, pero con suerte este ruido es mucho menos significativo que la verdadera imagen. Se espera que los valores singulares m谩s grandes representen a la verdadera imagen y que los m谩s peque帽os, los m谩s cercanos a cero, sean las contribuciones del ruido. Al realizar la DVS que solamente retiene esos valores singulares por encima de cierto umbral, podr铆amos ser capaces de eliminar la mayor parte del ruido y, en realidad, obtener una imagen que no s贸lo sea de menor tama帽o sino tambi茅n una representaci贸n m谩s clara de la superficie.\n\n### No est谩 de m谩s saber que...\n\n- Ya qued贸 claro que DVS ser谩 importante a la hora de estudiar An谩lisis de Datos Multivariados.\n- En esta secci贸n vamos a mencionar un par de detalles adicionales que son un nexo entre lo que han estudiado de lgebra Lineal, estamos aplicando ahora mediante M茅todos Num茅ricos y ver谩n su utilidad en An谩lisis de Datos Multivariados:\n\n\t1. Un poquito m谩s arriba mencionamos al rango de una matriz. Recordamos que el rango de una matriz es el n煤mero m谩ximo de vectores fila o columna que linealmente independientes. Si $\\mathbf A_{m\\times n}$, $rg(\\mathbf A) \\leq min(m, n)$. Si $rg(\\mathbf A) = min(m, n)$, se dice que la matriz es de rango completo. En Estad铆stica, el rango de una matriz de datos nos indica la dimensi贸n real necesaria para representar el conjunto de datos, o el n煤mero real de variables distintas que disponemos. Analizar el rango de una matriz de datos es la clave para reducir el n煤mero de variables sin p茅rdida de informaci贸n.\n\t2. Tambi茅n ha aparecido en la DVS la matriz $\\mathbf A^TA$. Pas贸 por ah铆 casi desapercibida, pero esta matriz es fundamental en Estad铆stica. Si $\\mathbf A$ es nuestra matriz de datos, que generalmente denotamos con $\\mathbf X$, entonces $\\mathbf X^T \\mathbf X$ es proporcional a la **matriz de variancias y covariancias**. Su **determinante** es una medida global de la independencia entre las variables. A mayor determinante, mayor independencia. Para que la DVS pueda hacer una buena compresi贸n con pocos valores singulares $k$, se necesita que las variables est茅n correlacionadas.\n\t3. La **traza** de una matriz es una medida global de su tama帽o que se obtiene sumando sus elementos diagonales. Por ejemplo, la traza de una matriz de variancias y covariancias es la suma de todas las variancias de las variables. Entonces, la suma de los elementos diagonales es una medida de variabilidad que, a diferencia del determinante, no tiene en cuenta las relaciones entre las variables.\n",
    "supporting": [
      "05_autovalores_files/figure-html"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}